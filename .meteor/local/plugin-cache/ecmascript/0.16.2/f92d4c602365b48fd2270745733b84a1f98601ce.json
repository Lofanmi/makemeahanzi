{"metadata":{},"options":{"assumptions":{},"compact":false,"sourceMaps":true,"ast":true,"babelrc":false,"configFile":false,"parserOpts":{"sourceType":"module","sourceFileName":"/Users/a37/code/github/makemeahanzi-tool/lib/external/convnet/1.1.0/convnet.js","plugins":["*","flow","jsx","asyncGenerators","bigInt","classPrivateMethods","classPrivateProperties","classProperties","doExpressions","dynamicImport","exportDefaultFrom","exportExtensions","exportNamespaceFrom","functionBind","functionSent","importMeta","nullishCoalescingOperator","numericSeparator","objectRestSpread","optionalCatchBinding","optionalChaining",["pipelineOperator",{"proposal":"minimal"}],"throwExpressions","objectRestSpread","asyncGenerators","classProperties","classPrivateProperties","jsx","nullishCoalescingOperator","nullishCoalescingOperator","optionalChaining","optionalCatchBinding","optionalCatchBinding","classProperties","classPrivateProperties","classPrivateMethods","classProperties","classPrivateProperties","asyncGenerators","asyncGenerators","objectRestSpread","logicalAssignment"],"allowImportExportEverywhere":true,"allowReturnOutsideFunction":true,"allowUndeclaredExports":true,"strictMode":false},"caller":{"name":"meteor","arch":"os.osx.arm64"},"sourceFileName":"lib/external/convnet/1.1.0/convnet.js","filename":"/Users/a37/code/github/makemeahanzi-tool/lib/external/convnet/1.1.0/convnet.js","targets":{},"cloneInputAst":true,"browserslistConfigFile":false,"passPerPreset":false,"envName":"development","cwd":"/Users/a37/code/github/makemeahanzi-tool","root":"/Users/a37/code/github/makemeahanzi-tool","rootMode":"root","plugins":[{"key":"base$0","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"base$0$0","visitor":{"Program":{"enter":[null],"exit":[null]},"_exploded":true,"_verified":true},"options":{"avoidModernSyntax":false,"enforceStrictMode":false,"dynamicImport":true,"generateLetDeclarations":true},"externalDependencies":[]},{"key":"transform-runtime","visitor":{"MemberExpression":{"enter":[null]},"ObjectPattern":{"enter":[null]},"BinaryExpression":{"enter":[null]},"_exploded":{},"_verified":{},"Identifier":{"enter":[null]},"JSXIdentifier":{"enter":[null]}},"options":{"version":"7.17.2","helpers":true,"useESModules":false,"corejs":false},"externalDependencies":[]},{"key":"syntax-object-rest-spread","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"proposal-object-rest-spread","visitor":{"VariableDeclarator":{"enter":[null]},"ExportNamedDeclaration":{"enter":[null]},"CatchClause":{"enter":[null]},"AssignmentExpression":{"enter":[null]},"ArrayPattern":{"enter":[null]},"ObjectExpression":{"enter":[null]},"_exploded":true,"_verified":true,"FunctionDeclaration":{"enter":[null]},"FunctionExpression":{"enter":[null]},"ObjectMethod":{"enter":[null]},"ArrowFunctionExpression":{"enter":[null]},"ClassMethod":{"enter":[null]},"ClassPrivateMethod":{"enter":[null]},"ForInStatement":{"enter":[null]},"ForOfStatement":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"transform-meteor-async-await","visitor":{"AwaitExpression":{"enter":[null]},"_exploded":true,"_verified":true,"FunctionDeclaration":{"exit":[null]},"FunctionExpression":{"exit":[null]},"ObjectMethod":{"exit":[null]},"ArrowFunctionExpression":{"exit":[null]},"ClassMethod":{"exit":[null]},"ClassPrivateMethod":{"exit":[null]}},"options":{"useNativeAsyncAwait":false},"externalDependencies":[]},{"key":"proposal-async-generator-functions","visitor":{"_exploded":{},"_verified":{},"Program":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"proposal-class-properties","visitor":{"ExportDefaultDeclaration":{"enter":[null]},"_exploded":true,"_verified":true,"ClassExpression":{"enter":[null]},"ClassDeclaration":{"enter":[null]}},"options":{"loose":true},"externalDependencies":[]},{"key":"transform-react-jsx","visitor":{"_exploded":{},"_verified":{},"JSXNamespacedName":{"enter":[null]},"JSXSpreadChild":{"enter":[null]},"Program":{"enter":[null]},"JSXElement":{"exit":[null]},"JSXFragment":{"exit":[null]},"JSXAttribute":{"enter":[null]}},"options":{"pragma":"React.createElement","pragmaFrag":"React.Fragment","runtime":"classic","throwIfNamespace":true,"useBuiltIns":false},"externalDependencies":[]},{"key":"transform-react-display-name","visitor":{"ExportDefaultDeclaration":{"enter":[null]},"CallExpression":{"enter":[null]},"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"transform-react-pure-annotations","visitor":{"CallExpression":{"enter":[null]},"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"syntax-nullish-coalescing-operator","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"proposal-nullish-coalescing-operator","visitor":{"_exploded":{},"_verified":{},"LogicalExpression":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"syntax-optional-chaining","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"proposal-optional-chaining","visitor":{"_exploded":true,"OptionalCallExpression":{"enter":[null]},"OptionalMemberExpression":{"enter":[null]},"_verified":true},"options":{},"externalDependencies":[]},{"key":"syntax-optional-catch-binding","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"proposal-optional-catch-binding","visitor":{"_exploded":{},"_verified":{},"CatchClause":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"syntax-class-properties","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"proposal-class-properties","visitor":{"ExportDefaultDeclaration":{"enter":[null]},"_exploded":true,"_verified":true,"ClassExpression":{"enter":[null]},"ClassDeclaration":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"syntax-async-generators","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"proposal-async-generator-functions","visitor":{"_exploded":{},"_verified":{},"Program":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"syntax-object-rest-spread","visitor":{"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"proposal-object-rest-spread","visitor":{"VariableDeclarator":{"enter":[null]},"ExportNamedDeclaration":{"enter":[null]},"CatchClause":{"enter":[null]},"AssignmentExpression":{"enter":[null]},"ArrayPattern":{"enter":[null]},"ObjectExpression":{"enter":[null]},"_exploded":true,"_verified":true,"FunctionDeclaration":{"enter":[null]},"FunctionExpression":{"enter":[null]},"ObjectMethod":{"enter":[null]},"ArrowFunctionExpression":{"enter":[null]},"ClassMethod":{"enter":[null]},"ClassPrivateMethod":{"enter":[null]},"ForInStatement":{"enter":[null]},"ForOfStatement":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"proposal-logical-assignment-operators","visitor":{"_exploded":{},"_verified":{},"AssignmentExpression":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"transform-literals","visitor":{"NumericLiteral":{"enter":[null]},"StringLiteral":{"enter":[null]},"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"transform-template-literals","visitor":{"TaggedTemplateExpression":{"enter":[null]},"TemplateLiteral":{"enter":[null]},"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]},{"key":"transform-parameters","visitor":{"_exploded":true,"_verified":true,"FunctionDeclaration":{"enter":[null]},"FunctionExpression":{"enter":[null]},"ObjectMethod":{"enter":[null]},"ArrowFunctionExpression":{"enter":[null]},"ClassMethod":{"enter":[null]},"ClassPrivateMethod":{"enter":[null]}},"options":{},"externalDependencies":[]},{"key":"transform-exponentiation-operator","visitor":{"AssignmentExpression":{"enter":[null]},"BinaryExpression":{"enter":[null]},"_exploded":true,"_verified":true},"options":{},"externalDependencies":[]}],"presets":[],"generatorOpts":{"filename":"/Users/a37/code/github/makemeahanzi-tool/lib/external/convnet/1.1.0/convnet.js","comments":true,"compact":false,"sourceMaps":true,"sourceFileName":"lib/external/convnet/1.1.0/convnet.js"}},"code":"var convnetjs = convnetjs || {\n  REVISION: 'ALPHA'\n};\n\n(function (global) {\n  \"use strict\"; // Random number utilities\n\n  var return_v = false;\n  var v_val = 0.0;\n\n  var gaussRandom = function () {\n    if (return_v) {\n      return_v = false;\n      return v_val;\n    }\n\n    var u = 2 * Math.random() - 1;\n    var v = 2 * Math.random() - 1;\n    var r = u * u + v * v;\n    if (r == 0 || r > 1) return gaussRandom();\n    var c = Math.sqrt(-2 * Math.log(r) / r);\n    v_val = v * c; // cache this\n\n    return_v = true;\n    return u * c;\n  };\n\n  var randf = function (a, b) {\n    return Math.random() * (b - a) + a;\n  };\n\n  var randi = function (a, b) {\n    return Math.floor(Math.random() * (b - a) + a);\n  };\n\n  var randn = function (mu, std) {\n    return mu + gaussRandom() * std;\n  }; // Array utilities\n\n\n  var zeros = function (n) {\n    if (typeof n === 'undefined' || isNaN(n)) {\n      return [];\n    }\n\n    if (typeof ArrayBuffer === 'undefined') {\n      // lacking browser support\n      var arr = new Array(n);\n\n      for (var i = 0; i < n; i++) {\n        arr[i] = 0;\n      }\n\n      return arr;\n    } else {\n      return new Float64Array(n);\n    }\n  };\n\n  var arrContains = function (arr, elt) {\n    for (var i = 0, n = arr.length; i < n; i++) {\n      if (arr[i] === elt) return true;\n    }\n\n    return false;\n  };\n\n  var arrUnique = function (arr) {\n    var b = [];\n\n    for (var i = 0, n = arr.length; i < n; i++) {\n      if (!arrContains(b, arr[i])) {\n        b.push(arr[i]);\n      }\n    }\n\n    return b;\n  }; // return max and min of a given non-empty array.\n\n\n  var maxmin = function (w) {\n    if (w.length === 0) {\n      return {};\n    } // ... ;s\n\n\n    var maxv = w[0];\n    var minv = w[0];\n    var maxi = 0;\n    var mini = 0;\n    var n = w.length;\n\n    for (var i = 1; i < n; i++) {\n      if (w[i] > maxv) {\n        maxv = w[i];\n        maxi = i;\n      }\n\n      if (w[i] < minv) {\n        minv = w[i];\n        mini = i;\n      }\n    }\n\n    return {\n      maxi: maxi,\n      maxv: maxv,\n      mini: mini,\n      minv: minv,\n      dv: maxv - minv\n    };\n  }; // create random permutation of numbers, in range [0...n-1]\n\n\n  var randperm = function (n) {\n    var i = n,\n        j = 0,\n        temp;\n    var array = [];\n\n    for (var q = 0; q < n; q++) array[q] = q;\n\n    while (i--) {\n      j = Math.floor(Math.random() * (i + 1));\n      temp = array[i];\n      array[i] = array[j];\n      array[j] = temp;\n    }\n\n    return array;\n  }; // sample from list lst according to probabilities in list probs\n  // the two lists are of same size, and probs adds up to 1\n\n\n  var weightedSample = function (lst, probs) {\n    var p = randf(0, 1.0);\n    var cumprob = 0.0;\n\n    for (var k = 0, n = lst.length; k < n; k++) {\n      cumprob += probs[k];\n\n      if (p < cumprob) {\n        return lst[k];\n      }\n    }\n  }; // syntactic sugar function for getting default parameter values\n\n\n  var getopt = function (opt, field_name, default_value) {\n    return typeof opt[field_name] !== 'undefined' ? opt[field_name] : default_value;\n  };\n\n  global.randf = randf;\n  global.randi = randi;\n  global.randn = randn;\n  global.zeros = zeros;\n  global.maxmin = maxmin;\n  global.randperm = randperm;\n  global.weightedSample = weightedSample;\n  global.arrUnique = arrUnique;\n  global.arrContains = arrContains;\n  global.getopt = getopt;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\"; // Vol is the basic building block of all data in a net.\n  // it is essentially just a 3D volume of numbers, with a\n  // width (sx), height (sy), and depth (depth).\n  // it is used to hold data for all filters, all volumes,\n  // all weights, and also stores all gradients w.r.t. \n  // the data. c is optionally a value to initialize the volume\n  // with. If c is missing, fills the Vol with random numbers.\n\n  var Vol = function (sx, sy, depth, c) {\n    // this is how you check if a variable is an array. Oh, Javascript :)\n    if (Object.prototype.toString.call(sx) === '[object Array]') {\n      // we were given a list in sx, assume 1D volume and fill it up\n      this.sx = 1;\n      this.sy = 1;\n      this.depth = sx.length; // we have to do the following copy because we want to use\n      // fast typed arrays, not an ordinary javascript array\n\n      this.w = global.zeros(this.depth);\n      this.dw = global.zeros(this.depth);\n\n      for (var i = 0; i < this.depth; i++) {\n        this.w[i] = sx[i];\n      }\n    } else {\n      // we were given dimensions of the vol\n      this.sx = sx;\n      this.sy = sy;\n      this.depth = depth;\n      var n = sx * sy * depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n);\n\n      if (typeof c === 'undefined') {\n        // weight normalization is done to equalize the output\n        // variance of every neuron, otherwise neurons with a lot\n        // of incoming connections have outputs of larger variance\n        var scale = Math.sqrt(1.0 / (sx * sy * depth));\n\n        for (var i = 0; i < n; i++) {\n          this.w[i] = global.randn(0.0, scale);\n        }\n      } else {\n        for (var i = 0; i < n; i++) {\n          this.w[i] = c;\n        }\n      }\n    }\n  };\n\n  Vol.prototype = {\n    get: function (x, y, d) {\n      var ix = (this.sx * y + x) * this.depth + d;\n      return this.w[ix];\n    },\n    set: function (x, y, d, v) {\n      var ix = (this.sx * y + x) * this.depth + d;\n      this.w[ix] = v;\n    },\n    add: function (x, y, d, v) {\n      var ix = (this.sx * y + x) * this.depth + d;\n      this.w[ix] += v;\n    },\n    get_grad: function (x, y, d) {\n      var ix = (this.sx * y + x) * this.depth + d;\n      return this.dw[ix];\n    },\n    set_grad: function (x, y, d, v) {\n      var ix = (this.sx * y + x) * this.depth + d;\n      this.dw[ix] = v;\n    },\n    add_grad: function (x, y, d, v) {\n      var ix = (this.sx * y + x) * this.depth + d;\n      this.dw[ix] += v;\n    },\n    cloneAndZero: function () {\n      return new Vol(this.sx, this.sy, this.depth, 0.0);\n    },\n    clone: function () {\n      var V = new Vol(this.sx, this.sy, this.depth, 0.0);\n      var n = this.w.length;\n\n      for (var i = 0; i < n; i++) {\n        V.w[i] = this.w[i];\n      }\n\n      return V;\n    },\n    addFrom: function (V) {\n      for (var k = 0; k < this.w.length; k++) {\n        this.w[k] += V.w[k];\n      }\n    },\n    addFromScaled: function (V, a) {\n      for (var k = 0; k < this.w.length; k++) {\n        this.w[k] += a * V.w[k];\n      }\n    },\n    setConst: function (a) {\n      for (var k = 0; k < this.w.length; k++) {\n        this.w[k] = a;\n      }\n    },\n    toJSON: function () {\n      // todo: we may want to only save d most significant digits to save space\n      var json = {};\n      json.sx = this.sx;\n      json.sy = this.sy;\n      json.depth = this.depth;\n      json.w = this.w;\n      return json; // we wont back up gradients to save space\n    },\n    fromJSON: function (json) {\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.depth = json.depth;\n      var n = this.sx * this.sy * this.depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n); // copy over the elements.\n\n      for (var i = 0; i < n; i++) {\n        this.w[i] = json.w[i];\n      }\n    }\n  };\n  global.Vol = Vol;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n  // Volume utilities\n  // intended for use with data augmentation\n  // crop is the size of output\n  // dx,dy are offset wrt incoming volume, of the shift\n  // fliplr is boolean on whether we also want to flip left<->right\n\n  var augment = function (V, crop, dx, dy, fliplr) {\n    // note assumes square outputs of size crop x crop\n    if (typeof fliplr === 'undefined') var fliplr = false;\n    if (typeof dx === 'undefined') var dx = global.randi(0, V.sx - crop);\n    if (typeof dy === 'undefined') var dy = global.randi(0, V.sy - crop); // randomly sample a crop in the input volume\n\n    var W;\n\n    if (crop !== V.sx || dx !== 0 || dy !== 0) {\n      W = new Vol(crop, crop, V.depth, 0.0);\n\n      for (var x = 0; x < crop; x++) {\n        for (var y = 0; y < crop; y++) {\n          if (x + dx < 0 || x + dx >= V.sx || y + dy < 0 || y + dy >= V.sy) continue; // oob\n\n          for (var d = 0; d < V.depth; d++) {\n            W.set(x, y, d, V.get(x + dx, y + dy, d)); // copy data over\n          }\n        }\n      }\n    } else {\n      W = V;\n    }\n\n    if (fliplr) {\n      // flip volume horziontally\n      var W2 = W.cloneAndZero();\n\n      for (var x = 0; x < W.sx; x++) {\n        for (var y = 0; y < W.sy; y++) {\n          for (var d = 0; d < W.depth; d++) {\n            W2.set(x, y, d, W.get(W.sx - x - 1, y, d)); // copy data over\n          }\n        }\n      }\n\n      W = W2; //swap\n    }\n\n    return W;\n  }; // img is a DOM element that contains a loaded image\n  // returns a Vol of size (W, H, 4). 4 is for RGBA\n\n\n  var img_to_vol = function (img, convert_grayscale) {\n    if (typeof convert_grayscale === 'undefined') var convert_grayscale = false;\n    var canvas = document.createElement('canvas');\n    canvas.width = img.width;\n    canvas.height = img.height;\n    var ctx = canvas.getContext(\"2d\"); // due to a Firefox bug\n\n    try {\n      ctx.drawImage(img, 0, 0);\n    } catch (e) {\n      if (e.name === \"NS_ERROR_NOT_AVAILABLE\") {\n        // sometimes happens, lets just abort\n        return false;\n      } else {\n        throw e;\n      }\n    }\n\n    try {\n      var img_data = ctx.getImageData(0, 0, canvas.width, canvas.height);\n    } catch (e) {\n      if (e.name === 'IndexSizeError') {\n        return false; // not sure what causes this sometimes but okay abort\n      } else {\n        throw e;\n      }\n    } // prepare the input: get pixels and normalize them\n\n\n    var p = img_data.data;\n    var W = img.width;\n    var H = img.height;\n    var pv = [];\n\n    for (var i = 0; i < p.length; i++) {\n      pv.push(p[i] / 255.0 - 0.5); // normalize image pixels to [-0.5, 0.5]\n    }\n\n    var x = new Vol(W, H, 4, 0.0); //input volume (image)\n\n    x.w = pv;\n\n    if (convert_grayscale) {\n      // flatten into depth=1 array\n      var x1 = new Vol(W, H, 1, 0.0);\n\n      for (var i = 0; i < W; i++) {\n        for (var j = 0; j < H; j++) {\n          x1.set(i, j, 0, x.get(i, j, 0));\n        }\n      }\n\n      x = x1;\n    }\n\n    return x;\n  };\n\n  global.augment = augment;\n  global.img_to_vol = img_to_vol;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n  // This file contains all layers that do dot products with input,\n  // but usually in a different connectivity pattern and weight sharing\n  // schemes: \n  // - FullyConn is fully connected dot products \n  // - ConvLayer does convolutions (so weight sharing spatially)\n  // putting them together in one file because they are very similar\n\n  var ConvLayer = function (opt) {\n    var opt = opt || {}; // required\n\n    this.out_depth = opt.filters;\n    this.sx = opt.sx; // filter size. Should be odd if possible, it's cleaner.\n\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy; // optional\n\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 1; // stride at which we apply filters to input volume\n\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n\n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0; // computed\n    // note we are doing floor, so if the strided convolution of the filter doesnt fit into the input\n    // volume exactly, the output volume will be trimmed and not contain the (incomplete) computed\n    // final application.\n\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'conv'; // initializations\n\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n\n    for (var i = 0; i < this.out_depth; i++) {\n      this.filters.push(new Vol(this.sx, this.sy, this.in_depth));\n    }\n\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  };\n\n  ConvLayer.prototype = {\n    forward: function (V, is_training) {\n      // optimized code by @mdda that achieves 2x speedup over previous version\n      this.in_act = V;\n      var A = new Vol(this.out_sx | 0, this.out_sy | 0, this.out_depth | 0, 0.0);\n      var V_sx = V.sx | 0;\n      var V_sy = V.sy | 0;\n      var xy_stride = this.stride | 0;\n\n      for (var d = 0; d < this.out_depth; d++) {\n        var f = this.filters[d];\n        var x = -this.pad | 0;\n        var y = -this.pad | 0;\n\n        for (var ay = 0; ay < this.out_sy; y += xy_stride, ay++) {\n          // xy_stride\n          x = -this.pad | 0;\n\n          for (var ax = 0; ax < this.out_sx; x += xy_stride, ax++) {\n            // xy_stride\n            // convolve centered at this particular location\n            var a = 0.0;\n\n            for (var fy = 0; fy < f.sy; fy++) {\n              var oy = y + fy; // coordinates in the original input array coordinates\n\n              for (var fx = 0; fx < f.sx; fx++) {\n                var ox = x + fx;\n\n                if (oy >= 0 && oy < V_sy && ox >= 0 && ox < V_sx) {\n                  for (var fd = 0; fd < f.depth; fd++) {\n                    // avoid function call overhead (x2) for efficiency, compromise modularity :(\n                    a += f.w[(f.sx * fy + fx) * f.depth + fd] * V.w[(V_sx * oy + ox) * V.depth + fd];\n                  }\n                }\n              }\n            }\n\n            a += this.biases.w[d];\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function () {\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt bottom data, we're about to fill it\n\n      var V_sx = V.sx | 0;\n      var V_sy = V.sy | 0;\n      var xy_stride = this.stride | 0;\n\n      for (var d = 0; d < this.out_depth; d++) {\n        var f = this.filters[d];\n        var x = -this.pad | 0;\n        var y = -this.pad | 0;\n\n        for (var ay = 0; ay < this.out_sy; y += xy_stride, ay++) {\n          // xy_stride\n          x = -this.pad | 0;\n\n          for (var ax = 0; ax < this.out_sx; x += xy_stride, ax++) {\n            // xy_stride\n            // convolve centered at this particular location\n            var chain_grad = this.out_act.get_grad(ax, ay, d); // gradient from above, from chain rule\n\n            for (var fy = 0; fy < f.sy; fy++) {\n              var oy = y + fy; // coordinates in the original input array coordinates\n\n              for (var fx = 0; fx < f.sx; fx++) {\n                var ox = x + fx;\n\n                if (oy >= 0 && oy < V_sy && ox >= 0 && ox < V_sx) {\n                  for (var fd = 0; fd < f.depth; fd++) {\n                    // avoid function call overhead (x2) for efficiency, compromise modularity :(\n                    var ix1 = (V_sx * oy + ox) * V.depth + fd;\n                    var ix2 = (f.sx * fy + fx) * f.depth + fd;\n                    f.dw[ix2] += V.w[ix1] * chain_grad;\n                    V.dw[ix1] += f.w[ix2] * chain_grad;\n                  }\n                }\n              }\n            }\n\n            this.biases.dw[d] += chain_grad;\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function () {\n      var response = [];\n\n      for (var i = 0; i < this.out_depth; i++) {\n        response.push({\n          params: this.filters[i].w,\n          grads: this.filters[i].dw,\n          l2_decay_mul: this.l2_decay_mul,\n          l1_decay_mul: this.l1_decay_mul\n        });\n      }\n\n      response.push({\n        params: this.biases.w,\n        grads: this.biases.dw,\n        l1_decay_mul: 0.0,\n        l2_decay_mul: 0.0\n      });\n      return response;\n    },\n    toJSON: function () {\n      var json = {};\n      json.sx = this.sx; // filter size in x, y dims\n\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.pad = this.pad;\n      json.filters = [];\n\n      for (var i = 0; i < this.filters.length; i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx; // filter size in x, y dims\n\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth; // depth of input volume\n\n      this.filters = [];\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0;\n\n      for (var i = 0; i < json.filters.length; i++) {\n        var v = new Vol(0, 0, 0, 0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n\n      this.biases = new Vol(0, 0, 0, 0);\n      this.biases.fromJSON(json.biases);\n    }\n  };\n\n  var FullyConnLayer = function (opt) {\n    var opt = opt || {}; // required\n    // ok fine we will allow 'filters' as the word as well\n\n    this.out_depth = typeof opt.num_neurons !== 'undefined' ? opt.num_neurons : opt.filters; // optional \n\n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0; // computed\n\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'fc'; // initializations\n\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n\n    for (var i = 0; i < this.out_depth; i++) {\n      this.filters.push(new Vol(1, 1, this.num_inputs));\n    }\n\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  };\n\n  FullyConnLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var A = new Vol(1, 1, this.out_depth, 0.0);\n      var Vw = V.w;\n\n      for (var i = 0; i < this.out_depth; i++) {\n        var a = 0.0;\n        var wi = this.filters[i].w;\n\n        for (var d = 0; d < this.num_inputs; d++) {\n          a += Vw[d] * wi[d]; // for efficiency use Vols directly for now\n        }\n\n        a += this.biases.w[i];\n        A.w[i] = a;\n      }\n\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function () {\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out the gradient in input Vol\n      // compute gradient wrt weights and data\n\n      for (var i = 0; i < this.out_depth; i++) {\n        var tfi = this.filters[i];\n        var chain_grad = this.out_act.dw[i];\n\n        for (var d = 0; d < this.num_inputs; d++) {\n          V.dw[d] += tfi.w[d] * chain_grad; // grad wrt input data\n\n          tfi.dw[d] += V.w[d] * chain_grad; // grad wrt params\n        }\n\n        this.biases.dw[i] += chain_grad;\n      }\n    },\n    getParamsAndGrads: function () {\n      var response = [];\n\n      for (var i = 0; i < this.out_depth; i++) {\n        response.push({\n          params: this.filters[i].w,\n          grads: this.filters[i].dw,\n          l1_decay_mul: this.l1_decay_mul,\n          l2_decay_mul: this.l2_decay_mul\n        });\n      }\n\n      response.push({\n        params: this.biases.w,\n        grads: this.biases.dw,\n        l1_decay_mul: 0.0,\n        l2_decay_mul: 0.0\n      });\n      return response;\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.filters = [];\n\n      for (var i = 0; i < this.filters.length; i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.filters = [];\n\n      for (var i = 0; i < json.filters.length; i++) {\n        var v = new Vol(0, 0, 0, 0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n\n      this.biases = new Vol(0, 0, 0, 0);\n      this.biases.fromJSON(json.biases);\n    }\n  };\n  global.ConvLayer = ConvLayer;\n  global.FullyConnLayer = FullyConnLayer;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n\n  var PoolLayer = function (opt) {\n    var opt = opt || {}; // required\n\n    this.sx = opt.sx; // filter size\n\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy; // optional\n\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 2;\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n    // computed\n\n    this.out_depth = this.in_depth;\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'pool'; // store switches for x,y coordinates for where the max comes from, for each output neuron\n\n    this.switchx = global.zeros(this.out_sx * this.out_sy * this.out_depth);\n    this.switchy = global.zeros(this.out_sx * this.out_sy * this.out_depth);\n  };\n\n  PoolLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      var n = 0; // a counter for switches\n\n      for (var d = 0; d < this.out_depth; d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n\n        for (var ax = 0; ax < this.out_sx; x += this.stride, ax++) {\n          y = -this.pad;\n\n          for (var ay = 0; ay < this.out_sy; y += this.stride, ay++) {\n            // convolve centered at this particular location\n            var a = -99999; // hopefully small enough ;\\\n\n            var winx = -1,\n                winy = -1;\n\n            for (var fx = 0; fx < this.sx; fx++) {\n              for (var fy = 0; fy < this.sy; fy++) {\n                var oy = y + fy;\n                var ox = x + fx;\n\n                if (oy >= 0 && oy < V.sy && ox >= 0 && ox < V.sx) {\n                  var v = V.get(ox, oy, d); // perform max pooling and store pointers to where\n                  // the max came from. This will speed up backprop \n                  // and can help make nice visualizations in future\n\n                  if (v > a) {\n                    a = v;\n                    winx = ox;\n                    winy = oy;\n                  }\n                }\n              }\n            }\n\n            this.switchx[n] = winx;\n            this.switchy[n] = winy;\n            n++;\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function () {\n      // pooling layers have no parameters, so simply compute \n      // gradient wrt data here\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n\n      var A = this.out_act; // computed in forward pass \n\n      var n = 0;\n\n      for (var d = 0; d < this.out_depth; d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n\n        for (var ax = 0; ax < this.out_sx; x += this.stride, ax++) {\n          y = -this.pad;\n\n          for (var ay = 0; ay < this.out_sy; y += this.stride, ay++) {\n            var chain_grad = this.out_act.get_grad(ax, ay, d);\n            V.add_grad(this.switchx[n], this.switchy[n], d, chain_grad);\n            n++;\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.sx = this.sx;\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.pad = this.pad;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0; // backwards compatibility\n\n      this.switchx = global.zeros(this.out_sx * this.out_sy * this.out_depth); // need to re-init these appropriately\n\n      this.switchy = global.zeros(this.out_sx * this.out_sy * this.out_depth);\n    }\n  };\n  global.PoolLayer = PoolLayer;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n\n  var InputLayer = function (opt) {\n    var opt = opt || {}; // this is a bit silly but lets allow people to specify either ins or outs\n\n    this.out_sx = typeof opt.out_sx !== 'undefined' ? opt.out_sx : opt.in_sx;\n    this.out_sy = typeof opt.out_sy !== 'undefined' ? opt.out_sy : opt.in_sy;\n    this.out_depth = typeof opt.out_depth !== 'undefined' ? opt.out_depth : opt.in_depth;\n    this.layer_type = 'input';\n  };\n\n  InputLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function () {},\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n    }\n  };\n  global.InputLayer = InputLayer;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n  // Layers that implement a loss. Currently these are the layers that \n  // can initiate a backward() pass. In future we probably want a more \n  // flexible system that can accomodate multiple losses to do multi-task\n  // learning, and stuff like that. But for now, one of the layers in this\n  // file must be the final layer in a Net.\n  // This is a classifier, with N discrete classes from 0 to N-1\n  // it gets a stream of N incoming numbers and computes the softmax\n  // function (exponentiate and normalize to sum to 1 as probabilities should)\n\n  var SoftmaxLayer = function (opt) {\n    var opt = opt || {}; // computed\n\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'softmax';\n  };\n\n  SoftmaxLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var A = new Vol(1, 1, this.out_depth, 0.0); // compute max activation\n\n      var as = V.w;\n      var amax = V.w[0];\n\n      for (var i = 1; i < this.out_depth; i++) {\n        if (as[i] > amax) amax = as[i];\n      } // compute exponentials (carefully to not blow up)\n\n\n      var es = global.zeros(this.out_depth);\n      var esum = 0.0;\n\n      for (var i = 0; i < this.out_depth; i++) {\n        var e = Math.exp(as[i] - amax);\n        esum += e;\n        es[i] = e;\n      } // normalize and output to sum to one\n\n\n      for (var i = 0; i < this.out_depth; i++) {\n        es[i] /= esum;\n        A.w[i] = es[i];\n      }\n\n      this.es = es; // save these for backprop\n\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function (y) {\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      for (var i = 0; i < this.out_depth; i++) {\n        var indicator = i === y ? 1.0 : 0.0;\n        var mul = -(indicator - this.es[i]);\n        x.dw[i] = mul;\n      } // loss is the class negative log likelihood\n\n\n      return -Math.log(this.es[y]);\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }; // implements an L2 regression cost layer,\n  // so penalizes \\sum_i(||x_i - y_i||^2), where x is its input\n  // and y is the user-provided array of \"correct\" values.\n\n  var RegressionLayer = function (opt) {\n    var opt = opt || {}; // computed\n\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'regression';\n  };\n\n  RegressionLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return V; // identity function\n    },\n    // y is a list here of size num_inputs\n    backward: function (y) {\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      var loss = 0.0;\n\n      if (y instanceof Array || y instanceof Float64Array) {\n        for (var i = 0; i < this.out_depth; i++) {\n          var dy = x.w[i] - y[i];\n          x.dw[i] = dy;\n          loss += 2 * dy * dy;\n        }\n      } else {\n        // assume it is a struct with entries .dim and .val\n        // and we pass gradient only along dimension dim to be equal to val\n        var i = y.dim;\n        var yi = y.val;\n        var dy = x.w[i] - yi;\n        x.dw[i] = dy;\n        loss += 2 * dy * dy;\n      }\n\n      return loss;\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  };\n\n  var SVMLayer = function (opt) {\n    var opt = opt || {}; // computed\n\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'svm';\n  };\n\n  SVMLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      this.out_act = V; // nothing to do, output raw scores\n\n      return V;\n    },\n    backward: function (y) {\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      var yscore = x.w[y]; // score of ground truth\n\n      var margin = 1.0;\n      var loss = 0.0;\n\n      for (var i = 0; i < this.out_depth; i++) {\n        if (-yscore + x.w[i] + margin > 0) {\n          // violating example, apply loss\n          // I love hinge loss, by the way. Truly.\n          // Seriously, compare this SVM code with Softmax forward AND backprop code above\n          // it's clear which one is superior, not only in code, simplicity\n          // and beauty, but also in practice.\n          x.dw[i] += 1;\n          x.dw[y] -= 1;\n          loss += -yscore + x.w[i] + margin;\n        }\n      }\n\n      return loss;\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  };\n  global.RegressionLayer = RegressionLayer;\n  global.SoftmaxLayer = SoftmaxLayer;\n  global.SVMLayer = SVMLayer;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n  // Implements ReLU nonlinearity elementwise\n  // x -> max(0, x)\n  // the output is in [0, inf)\n\n  var ReluLayer = function (opt) {\n    var opt = opt || {}; // computed\n\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'relu';\n  };\n\n  ReluLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var V2 = V.clone();\n      var N = V.w.length;\n      var V2w = V2.w;\n\n      for (var i = 0; i < N; i++) {\n        if (V2w[i] < 0) V2w[i] = 0; // threshold at 0\n      }\n\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function () {\n      var V = this.in_act; // we need to set dw of this\n\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n\n      for (var i = 0; i < N; i++) {\n        if (V2.w[i] <= 0) V.dw[i] = 0; // threshold\n        else V.dw[i] = V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n    }\n  }; // Implements Sigmoid nnonlinearity elementwise\n  // x -> 1/(1+e^(-x))\n  // so the output is between 0 and 1.\n\n  var SigmoidLayer = function (opt) {\n    var opt = opt || {}; // computed\n\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'sigmoid';\n  };\n\n  SigmoidLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n      var V2w = V2.w;\n      var Vw = V.w;\n\n      for (var i = 0; i < N; i++) {\n        V2w[i] = 1.0 / (1.0 + Math.exp(-Vw[i]));\n      }\n\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function () {\n      var V = this.in_act; // we need to set dw of this\n\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n\n      for (var i = 0; i < N; i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] = v2wi * (1.0 - v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n    }\n  }; // Implements Maxout nnonlinearity that computes\n  // x -> max(x)\n  // where x is a vector of size group_size. Ideally of course,\n  // the input size should be exactly divisible by group_size\n\n  var MaxoutLayer = function (opt) {\n    var opt = opt || {}; // required\n\n    this.group_size = typeof opt.group_size !== 'undefined' ? opt.group_size : 2; // computed\n\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = Math.floor(opt.in_depth / this.group_size);\n    this.layer_type = 'maxout';\n    this.switches = global.zeros(this.out_sx * this.out_sy * this.out_depth); // useful for backprop\n  };\n\n  MaxoutLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var N = this.out_depth;\n      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0); // optimization branch. If we're operating on 1D arrays we dont have\n      // to worry about keeping track of x,y,d coordinates inside\n      // input volumes. In convnets we do :(\n\n      if (this.out_sx === 1 && this.out_sy === 1) {\n        for (var i = 0; i < N; i++) {\n          var ix = i * this.group_size; // base index offset\n\n          var a = V.w[ix];\n          var ai = 0;\n\n          for (var j = 1; j < this.group_size; j++) {\n            var a2 = V.w[ix + j];\n\n            if (a2 > a) {\n              a = a2;\n              ai = j;\n            }\n          }\n\n          V2.w[i] = a;\n          this.switches[i] = ix + ai;\n        }\n      } else {\n        var n = 0; // counter for switches\n\n        for (var x = 0; x < V.sx; x++) {\n          for (var y = 0; y < V.sy; y++) {\n            for (var i = 0; i < N; i++) {\n              var ix = i * this.group_size;\n              var a = V.get(x, y, ix);\n              var ai = 0;\n\n              for (var j = 1; j < this.group_size; j++) {\n                var a2 = V.get(x, y, ix + j);\n\n                if (a2 > a) {\n                  a = a2;\n                  ai = j;\n                }\n              }\n\n              V2.set(x, y, i, a);\n              this.switches[n] = ix + ai;\n              n++;\n            }\n          }\n        }\n      }\n\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function () {\n      var V = this.in_act; // we need to set dw of this\n\n      var V2 = this.out_act;\n      var N = this.out_depth;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      // pass the gradient through the appropriate switch\n\n      if (this.out_sx === 1 && this.out_sy === 1) {\n        for (var i = 0; i < N; i++) {\n          var chain_grad = V2.dw[i];\n          V.dw[this.switches[i]] = chain_grad;\n        }\n      } else {\n        // bleh okay, lets do this the hard way\n        var n = 0; // counter for switches\n\n        for (var x = 0; x < V2.sx; x++) {\n          for (var y = 0; y < V2.sy; y++) {\n            for (var i = 0; i < N; i++) {\n              var chain_grad = V2.get_grad(x, y, i);\n              V.set_grad(x, y, this.switches[n], chain_grad);\n              n++;\n            }\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.group_size = this.group_size;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.group_size = json.group_size;\n      this.switches = global.zeros(this.group_size);\n    }\n  }; // a helper function, since tanh is not yet part of ECMAScript. Will be in v6.\n\n  function tanh(x) {\n    var y = Math.exp(2 * x);\n    return (y - 1) / (y + 1);\n  } // Implements Tanh nnonlinearity elementwise\n  // x -> tanh(x) \n  // so the output is between -1 and 1.\n\n\n  var TanhLayer = function (opt) {\n    var opt = opt || {}; // computed\n\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'tanh';\n  };\n\n  TanhLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n\n      for (var i = 0; i < N; i++) {\n        V2.w[i] = tanh(V.w[i]);\n      }\n\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function () {\n      var V = this.in_act; // we need to set dw of this\n\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n\n      for (var i = 0; i < N; i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] = (1.0 - v2wi * v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n    }\n  };\n  global.TanhLayer = TanhLayer;\n  global.MaxoutLayer = MaxoutLayer;\n  global.ReluLayer = ReluLayer;\n  global.SigmoidLayer = SigmoidLayer;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n  // An inefficient dropout layer\n  // Note this is not most efficient implementation since the layer before\n  // computed all these activations and now we're just going to drop them :(\n  // same goes for backward pass. Also, if we wanted to be efficient at test time\n  // we could equivalently be clever and upscale during train and copy pointers during test\n  // todo: make more efficient.\n\n  var DropoutLayer = function (opt) {\n    var opt = opt || {}; // computed\n\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'dropout';\n    this.drop_prob = typeof opt.drop_prob !== 'undefined' ? opt.drop_prob : 0.5;\n    this.dropped = global.zeros(this.out_sx * this.out_sy * this.out_depth);\n  };\n\n  DropoutLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n\n      if (typeof is_training === 'undefined') {\n        is_training = false;\n      } // default is prediction mode\n\n\n      var V2 = V.clone();\n      var N = V.w.length;\n\n      if (is_training) {\n        // do dropout\n        for (var i = 0; i < N; i++) {\n          if (Math.random() < this.drop_prob) {\n            V2.w[i] = 0;\n            this.dropped[i] = true;\n          } // drop!\n          else {\n            this.dropped[i] = false;\n          }\n        }\n      } else {\n        // scale the activations during prediction\n        for (var i = 0; i < N; i++) {\n          V2.w[i] *= this.drop_prob;\n        }\n      }\n\n      this.out_act = V2;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function () {\n      var V = this.in_act; // we need to set dw of this\n\n      var chain_grad = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n\n      for (var i = 0; i < N; i++) {\n        if (!this.dropped[i]) {\n          V.dw[i] = chain_grad.dw[i]; // copy over the gradient\n        }\n      }\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.drop_prob = this.drop_prob;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.drop_prob = json.drop_prob;\n    }\n  };\n  global.DropoutLayer = DropoutLayer;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n  // a bit experimental layer for now. I think it works but I'm not 100%\n  // the gradient check is a bit funky. I'll look into this a bit later.\n  // Local Response Normalization in window, along depths of volumes\n\n  var LocalResponseNormalizationLayer = function (opt) {\n    var opt = opt || {}; // required\n\n    this.k = opt.k;\n    this.n = opt.n;\n    this.alpha = opt.alpha;\n    this.beta = opt.beta; // computed\n\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'lrn'; // checks\n\n    if (this.n % 2 === 0) {\n      console.log('WARNING n should be odd for LRN layer');\n    }\n  };\n\n  LocalResponseNormalizationLayer.prototype = {\n    forward: function (V, is_training) {\n      this.in_act = V;\n      var A = V.cloneAndZero();\n      this.S_cache_ = V.cloneAndZero();\n      var n2 = Math.floor(this.n / 2);\n\n      for (var x = 0; x < V.sx; x++) {\n        for (var y = 0; y < V.sy; y++) {\n          for (var i = 0; i < V.depth; i++) {\n            var ai = V.get(x, y, i); // normalize in a window of size n\n\n            var den = 0.0;\n\n            for (var j = Math.max(0, i - n2); j <= Math.min(i + n2, V.depth - 1); j++) {\n              var aa = V.get(x, y, j);\n              den += aa * aa;\n            }\n\n            den *= this.alpha / this.n;\n            den += this.k;\n            this.S_cache_.set(x, y, i, den); // will be useful for backprop\n\n            den = Math.pow(den, this.beta);\n            A.set(x, y, i, ai / den);\n          }\n        }\n      }\n\n      this.out_act = A;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function () {\n      // evaluate gradient wrt data\n      var V = this.in_act; // we need to set dw of this\n\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n\n      var A = this.out_act; // computed in forward pass \n\n      var n2 = Math.floor(this.n / 2);\n\n      for (var x = 0; x < V.sx; x++) {\n        for (var y = 0; y < V.sy; y++) {\n          for (var i = 0; i < V.depth; i++) {\n            var chain_grad = this.out_act.get_grad(x, y, i);\n            var S = this.S_cache_.get(x, y, i);\n            var SB = Math.pow(S, this.beta);\n            var SB2 = SB * SB; // normalize in a window of size n\n\n            for (var j = Math.max(0, i - n2); j <= Math.min(i + n2, V.depth - 1); j++) {\n              var aj = V.get(x, y, j);\n              var g = -aj * this.beta * Math.pow(S, this.beta - 1) * this.alpha / this.n * 2 * aj;\n              if (j === i) g += SB;\n              g /= SB2;\n              g *= chain_grad;\n              V.add_grad(x, y, j, g);\n            }\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function () {\n      return [];\n    },\n    toJSON: function () {\n      var json = {};\n      json.k = this.k;\n      json.n = this.n;\n      json.alpha = this.alpha; // normalize by size\n\n      json.beta = this.beta;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.out_depth = this.out_depth;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function (json) {\n      this.k = json.k;\n      this.n = json.n;\n      this.alpha = json.alpha; // normalize by size\n\n      this.beta = json.beta;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.out_depth = json.out_depth;\n      this.layer_type = json.layer_type;\n    }\n  };\n  global.LocalResponseNormalizationLayer = LocalResponseNormalizationLayer;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n  // Net manages a set of layers\n  // For now constraints: Simple linear order of layers, first layer input last layer a cost layer\n\n  var Net = function (options) {\n    this.layers = [];\n  };\n\n  Net.prototype = {\n    // takes a list of layer definitions and creates the network layer objects\n    makeLayers: function (defs) {\n      // few checks for now\n      if (defs.length < 2) {\n        console.log('ERROR! For now at least have input and softmax layers.');\n      }\n\n      if (defs[0].type !== 'input') {\n        console.log('ERROR! For now first layer should be input.');\n      } // desugar syntactic for adding activations and dropouts\n\n\n      var desugar = function () {\n        var new_defs = [];\n\n        for (var i = 0; i < defs.length; i++) {\n          var def = defs[i];\n\n          if (def.type === 'softmax' || def.type === 'svm') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({\n              type: 'fc',\n              num_neurons: def.num_classes\n            });\n          }\n\n          if (def.type === 'regression') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({\n              type: 'fc',\n              num_neurons: def.num_neurons\n            });\n          }\n\n          if ((def.type === 'fc' || def.type === 'conv') && typeof def.bias_pref === 'undefined') {\n            def.bias_pref = 0.0;\n\n            if (typeof def.activation !== 'undefined' && def.activation === 'relu') {\n              def.bias_pref = 0.1; // relus like a bit of positive bias to get gradients early\n              // otherwise it's technically possible that a relu unit will never turn on (by chance)\n              // and will never get any gradient and never contribute any computation. Dead relu.\n            }\n          }\n\n          if (typeof def.tensor !== 'undefined') {\n            // apply quadratic transform so that the upcoming multiply will include\n            // quadratic terms, equivalent to doing a tensor product\n            if (def.tensor) {\n              new_defs.push({\n                type: 'quadtransform'\n              });\n            }\n          }\n\n          new_defs.push(def);\n\n          if (typeof def.activation !== 'undefined') {\n            if (def.activation === 'relu') {\n              new_defs.push({\n                type: 'relu'\n              });\n            } else if (def.activation === 'sigmoid') {\n              new_defs.push({\n                type: 'sigmoid'\n              });\n            } else if (def.activation === 'tanh') {\n              new_defs.push({\n                type: 'tanh'\n              });\n            } else if (def.activation === 'maxout') {\n              // create maxout activation, and pass along group size, if provided\n              var gs = def.group_size !== 'undefined' ? def.group_size : 2;\n              new_defs.push({\n                type: 'maxout',\n                group_size: gs\n              });\n            } else {\n              console.log('ERROR unsupported activation ' + def.activation);\n            }\n          }\n\n          if (typeof def.drop_prob !== 'undefined' && def.type !== 'dropout') {\n            new_defs.push({\n              type: 'dropout',\n              drop_prob: def.drop_prob\n            });\n          }\n        }\n\n        return new_defs;\n      };\n\n      defs = desugar(defs); // create the layers\n\n      this.layers = [];\n\n      for (var i = 0; i < defs.length; i++) {\n        var def = defs[i];\n\n        if (i > 0) {\n          var prev = this.layers[i - 1];\n          def.in_sx = prev.out_sx;\n          def.in_sy = prev.out_sy;\n          def.in_depth = prev.out_depth;\n        }\n\n        switch (def.type) {\n          case 'fc':\n            this.layers.push(new global.FullyConnLayer(def));\n            break;\n\n          case 'lrn':\n            this.layers.push(new global.LocalResponseNormalizationLayer(def));\n            break;\n\n          case 'dropout':\n            this.layers.push(new global.DropoutLayer(def));\n            break;\n\n          case 'input':\n            this.layers.push(new global.InputLayer(def));\n            break;\n\n          case 'softmax':\n            this.layers.push(new global.SoftmaxLayer(def));\n            break;\n\n          case 'regression':\n            this.layers.push(new global.RegressionLayer(def));\n            break;\n\n          case 'conv':\n            this.layers.push(new global.ConvLayer(def));\n            break;\n\n          case 'pool':\n            this.layers.push(new global.PoolLayer(def));\n            break;\n\n          case 'relu':\n            this.layers.push(new global.ReluLayer(def));\n            break;\n\n          case 'sigmoid':\n            this.layers.push(new global.SigmoidLayer(def));\n            break;\n\n          case 'tanh':\n            this.layers.push(new global.TanhLayer(def));\n            break;\n\n          case 'maxout':\n            this.layers.push(new global.MaxoutLayer(def));\n            break;\n\n          case 'quadtransform':\n            this.layers.push(new global.QuadTransformLayer(def));\n            break;\n\n          case 'svm':\n            this.layers.push(new global.SVMLayer(def));\n            break;\n\n          default:\n            console.log('ERROR: UNRECOGNIZED LAYER TYPE!');\n        }\n      }\n    },\n    // forward prop the network. A trainer will pass in is_training = true\n    forward: function (V, is_training) {\n      if (typeof is_training === 'undefined') is_training = false;\n      var act = this.layers[0].forward(V, is_training);\n\n      for (var i = 1; i < this.layers.length; i++) {\n        act = this.layers[i].forward(act, is_training);\n      }\n\n      return act;\n    },\n    getCostLoss: function (V, y) {\n      this.forward(V, false);\n      var N = this.layers.length;\n      var loss = this.layers[N - 1].backward(y);\n      return loss;\n    },\n    // backprop: compute gradients wrt all parameters\n    backward: function (y) {\n      var N = this.layers.length;\n      var loss = this.layers[N - 1].backward(y); // last layer assumed softmax\n\n      for (var i = N - 2; i >= 0; i--) {\n        // first layer assumed input\n        this.layers[i].backward();\n      }\n\n      return loss;\n    },\n    getParamsAndGrads: function () {\n      // accumulate parameters and gradients for the entire network\n      var response = [];\n\n      for (var i = 0; i < this.layers.length; i++) {\n        var layer_reponse = this.layers[i].getParamsAndGrads();\n\n        for (var j = 0; j < layer_reponse.length; j++) {\n          response.push(layer_reponse[j]);\n        }\n      }\n\n      return response;\n    },\n    getPrediction: function () {\n      var S = this.layers[this.layers.length - 1]; // softmax layer\n\n      var p = S.out_act.w;\n      var maxv = p[0];\n      var maxi = 0;\n\n      for (var i = 1; i < p.length; i++) {\n        if (p[i] > maxv) {\n          maxv = p[i];\n          maxi = i;\n        }\n      }\n\n      return maxi;\n    },\n    toJSON: function () {\n      var json = {};\n      json.layers = [];\n\n      for (var i = 0; i < this.layers.length; i++) {\n        json.layers.push(this.layers[i].toJSON());\n      }\n\n      return json;\n    },\n    fromJSON: function (json) {\n      this.layers = [];\n\n      for (var i = 0; i < json.layers.length; i++) {\n        var Lj = json.layers[i];\n        var t = Lj.layer_type;\n        var L;\n\n        if (t === 'input') {\n          L = new global.InputLayer();\n        }\n\n        if (t === 'relu') {\n          L = new global.ReluLayer();\n        }\n\n        if (t === 'sigmoid') {\n          L = new global.SigmoidLayer();\n        }\n\n        if (t === 'tanh') {\n          L = new global.TanhLayer();\n        }\n\n        if (t === 'dropout') {\n          L = new global.DropoutLayer();\n        }\n\n        if (t === 'conv') {\n          L = new global.ConvLayer();\n        }\n\n        if (t === 'pool') {\n          L = new global.PoolLayer();\n        }\n\n        if (t === 'lrn') {\n          L = new global.LocalResponseNormalizationLayer();\n        }\n\n        if (t === 'softmax') {\n          L = new global.SoftmaxLayer();\n        }\n\n        if (t === 'regression') {\n          L = new global.RegressionLayer();\n        }\n\n        if (t === 'fc') {\n          L = new global.FullyConnLayer();\n        }\n\n        if (t === 'maxout') {\n          L = new global.MaxoutLayer();\n        }\n\n        if (t === 'quadtransform') {\n          L = new global.QuadTransformLayer();\n        }\n\n        if (t === 'svm') {\n          L = new global.SVMLayer();\n        }\n\n        L.fromJSON(Lj);\n        this.layers.push(L);\n      }\n    }\n  };\n  global.Net = Net;\n})(convnetjs);\n\n(function (global) {\n  \"use strict\";\n\n  var Vol = global.Vol; // convenience\n\n  var Trainer = function (net, options) {\n    this.net = net;\n    var options = options || {};\n    this.learning_rate = typeof options.learning_rate !== 'undefined' ? options.learning_rate : 0.01;\n    this.l1_decay = typeof options.l1_decay !== 'undefined' ? options.l1_decay : 0.0;\n    this.l2_decay = typeof options.l2_decay !== 'undefined' ? options.l2_decay : 0.0;\n    this.batch_size = typeof options.batch_size !== 'undefined' ? options.batch_size : 1;\n    this.method = typeof options.method !== 'undefined' ? options.method : 'sgd'; // sgd/adagrad/adadelta/windowgrad\n\n    this.momentum = typeof options.momentum !== 'undefined' ? options.momentum : 0.9;\n    this.ro = typeof options.ro !== 'undefined' ? options.ro : 0.95; // used in adadelta\n\n    this.eps = typeof options.eps !== 'undefined' ? options.eps : 1e-6; // used in adadelta\n\n    this.k = 0; // iteration counter\n\n    this.gsum = []; // last iteration gradients (used for momentum calculations)\n\n    this.xsum = []; // used in adadelta\n  };\n\n  Trainer.prototype = {\n    train: function (x, y) {\n      var start = new Date().getTime();\n      this.net.forward(x, true); // also set the flag that lets the net know we're just training\n\n      var end = new Date().getTime();\n      var fwd_time = end - start;\n      var start = new Date().getTime();\n      var cost_loss = this.net.backward(y);\n      var l2_decay_loss = 0.0;\n      var l1_decay_loss = 0.0;\n      var end = new Date().getTime();\n      var bwd_time = end - start;\n      this.k++;\n\n      if (this.k % this.batch_size === 0) {\n        var pglist = this.net.getParamsAndGrads(); // initialize lists for accumulators. Will only be done once on first iteration\n\n        if (this.gsum.length === 0 && (this.method !== 'sgd' || this.momentum > 0.0)) {\n          // only vanilla sgd doesnt need either lists\n          // momentum needs gsum\n          // adagrad needs gsum\n          // adadelta needs gsum and xsum\n          for (var i = 0; i < pglist.length; i++) {\n            this.gsum.push(global.zeros(pglist[i].params.length));\n\n            if (this.method === 'adadelta') {\n              this.xsum.push(global.zeros(pglist[i].params.length));\n            } else {\n              this.xsum.push([]); // conserve memory\n            }\n          }\n        } // perform an update for all sets of weights\n\n\n        for (var i = 0; i < pglist.length; i++) {\n          var pg = pglist[i]; // param, gradient, other options in future (custom learning rate etc)\n\n          var p = pg.params;\n          var g = pg.grads; // learning rate for some parameters.\n\n          var l2_decay_mul = typeof pg.l2_decay_mul !== 'undefined' ? pg.l2_decay_mul : 1.0;\n          var l1_decay_mul = typeof pg.l1_decay_mul !== 'undefined' ? pg.l1_decay_mul : 1.0;\n          var l2_decay = this.l2_decay * l2_decay_mul;\n          var l1_decay = this.l1_decay * l1_decay_mul;\n          var plen = p.length;\n\n          for (var j = 0; j < plen; j++) {\n            l2_decay_loss += l2_decay * p[j] * p[j] / 2; // accumulate weight decay loss\n\n            l1_decay_loss += l1_decay * Math.abs(p[j]);\n            var l1grad = l1_decay * (p[j] > 0 ? 1 : -1);\n            var l2grad = l2_decay * p[j];\n            var gij = (l2grad + l1grad + g[j]) / this.batch_size; // raw batch gradient\n\n            var gsumi = this.gsum[i];\n            var xsumi = this.xsum[i];\n\n            if (this.method === 'adagrad') {\n              // adagrad update\n              gsumi[j] = gsumi[j] + gij * gij;\n              var dx = -this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij;\n              p[j] += dx;\n            } else if (this.method === 'windowgrad') {\n              // this is adagrad but with a moving window weighted average\n              // so the gradient is not accumulated over the entire history of the run. \n              // it's also referred to as Idea #1 in Zeiler paper on Adadelta. Seems reasonable to me!\n              gsumi[j] = this.ro * gsumi[j] + (1 - this.ro) * gij * gij;\n              var dx = -this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij; // eps added for better conditioning\n\n              p[j] += dx;\n            } else if (this.method === 'adadelta') {\n              // assume adadelta if not sgd or adagrad\n              gsumi[j] = this.ro * gsumi[j] + (1 - this.ro) * gij * gij;\n              var dx = -Math.sqrt((xsumi[j] + this.eps) / (gsumi[j] + this.eps)) * gij;\n              xsumi[j] = this.ro * xsumi[j] + (1 - this.ro) * dx * dx; // yes, xsum lags behind gsum by 1.\n\n              p[j] += dx;\n            } else {\n              // assume SGD\n              if (this.momentum > 0.0) {\n                // momentum update\n                var dx = this.momentum * gsumi[j] - this.learning_rate * gij; // step\n\n                gsumi[j] = dx; // back this up for next iteration of momentum\n\n                p[j] += dx; // apply corrected gradient\n              } else {\n                // vanilla sgd\n                p[j] += -this.learning_rate * gij;\n              }\n            }\n\n            g[j] = 0.0; // zero out gradient so that we can begin accumulating anew\n          }\n        }\n      } // appending softmax_loss for backwards compatibility, but from now on we will always use cost_loss\n      // in future, TODO: have to completely redo the way loss is done around the network as currently \n      // loss is a bit of a hack. Ideally, user should specify arbitrary number of loss functions on any layer\n      // and it should all be computed correctly and automatically. \n\n\n      return {\n        fwd_time: fwd_time,\n        bwd_time: bwd_time,\n        l2_decay_loss: l2_decay_loss,\n        l1_decay_loss: l1_decay_loss,\n        cost_loss: cost_loss,\n        softmax_loss: cost_loss,\n        loss: cost_loss + l1_decay_loss + l2_decay_loss\n      };\n    }\n  };\n  global.Trainer = Trainer;\n  global.SGDTrainer = Trainer; // backwards compatibility\n})(convnetjs);\n\n(function (global) {\n  \"use strict\"; // used utilities, make explicit local references\n\n  var randf = global.randf;\n  var randi = global.randi;\n  var Net = global.Net;\n  var Trainer = global.Trainer;\n  var maxmin = global.maxmin;\n  var randperm = global.randperm;\n  var weightedSample = global.weightedSample;\n  var getopt = global.getopt;\n  var arrUnique = global.arrUnique;\n  /*\n  A MagicNet takes data: a list of convnetjs.Vol(), and labels\n  which for now are assumed to be class indeces 0..K. MagicNet then:\n  - creates data folds for cross-validation\n  - samples candidate networks\n  - evaluates candidate networks on all data folds\n  - produces predictions by model-averaging the best networks\n  */\n\n  var MagicNet = function (data, labels, opt) {\n    var opt = opt || {};\n\n    if (typeof data === 'undefined') {\n      data = [];\n    }\n\n    if (typeof labels === 'undefined') {\n      labels = [];\n    } // required inputs\n\n\n    this.data = data; // store these pointers to data\n\n    this.labels = labels; // optional inputs\n\n    this.train_ratio = getopt(opt, 'train_ratio', 0.7);\n    this.num_folds = getopt(opt, 'num_folds', 10);\n    this.num_candidates = getopt(opt, 'num_candidates', 50); // we evaluate several in parallel\n    // how many epochs of data to train every network? for every fold?\n    // higher values mean higher accuracy in final results, but more expensive\n\n    this.num_epochs = getopt(opt, 'num_epochs', 50); // number of best models to average during prediction. Usually higher = better\n\n    this.ensemble_size = getopt(opt, 'ensemble_size', 10); // candidate parameters\n\n    this.batch_size_min = getopt(opt, 'batch_size_min', 10);\n    this.batch_size_max = getopt(opt, 'batch_size_max', 300);\n    this.l2_decay_min = getopt(opt, 'l2_decay_min', -4);\n    this.l2_decay_max = getopt(opt, 'l2_decay_max', 2);\n    this.learning_rate_min = getopt(opt, 'learning_rate_min', -4);\n    this.learning_rate_max = getopt(opt, 'learning_rate_max', 0);\n    this.momentum_min = getopt(opt, 'momentum_min', 0.9);\n    this.momentum_max = getopt(opt, 'momentum_max', 0.9);\n    this.neurons_min = getopt(opt, 'neurons_min', 5);\n    this.neurons_max = getopt(opt, 'neurons_max', 30); // computed\n\n    this.folds = []; // data fold indices, gets filled by sampleFolds()\n\n    this.candidates = []; // candidate networks that are being currently evaluated\n\n    this.evaluated_candidates = []; // history of all candidates that were fully evaluated on all folds\n\n    this.unique_labels = arrUnique(labels);\n    this.iter = 0; // iteration counter, goes from 0 -> num_epochs * num_training_data\n\n    this.foldix = 0; // index of active fold\n    // callbacks\n\n    this.finish_fold_callback = null;\n    this.finish_batch_callback = null; // initializations\n\n    if (this.data.length > 0) {\n      this.sampleFolds();\n      this.sampleCandidates();\n    }\n  };\n\n  MagicNet.prototype = {\n    // sets this.folds to a sampling of this.num_folds folds\n    sampleFolds: function () {\n      var N = this.data.length;\n      var num_train = Math.floor(this.train_ratio * N);\n      this.folds = []; // flush folds, if any\n\n      for (var i = 0; i < this.num_folds; i++) {\n        var p = randperm(N);\n        this.folds.push({\n          train_ix: p.slice(0, num_train),\n          test_ix: p.slice(num_train, N)\n        });\n      }\n    },\n    // returns a random candidate network\n    sampleCandidate: function () {\n      var input_depth = this.data[0].w.length;\n      var num_classes = this.unique_labels.length; // sample network topology and hyperparameters\n\n      var layer_defs = [];\n      layer_defs.push({\n        type: 'input',\n        out_sx: 1,\n        out_sy: 1,\n        out_depth: input_depth\n      });\n      var nl = weightedSample([0, 1, 2, 3], [0.2, 0.3, 0.3, 0.2]); // prefer nets with 1,2 hidden layers\n\n      for (var q = 0; q < nl; q++) {\n        var ni = randi(this.neurons_min, this.neurons_max);\n        var act = ['tanh', 'maxout', 'relu'][randi(0, 3)];\n\n        if (randf(0, 1) < 0.5) {\n          var dp = Math.random();\n          layer_defs.push({\n            type: 'fc',\n            num_neurons: ni,\n            activation: act,\n            drop_prob: dp\n          });\n        } else {\n          layer_defs.push({\n            type: 'fc',\n            num_neurons: ni,\n            activation: act\n          });\n        }\n      }\n\n      layer_defs.push({\n        type: 'softmax',\n        num_classes: num_classes\n      });\n      var net = new Net();\n      net.makeLayers(layer_defs); // sample training hyperparameters\n\n      var bs = randi(this.batch_size_min, this.batch_size_max); // batch size\n\n      var l2 = Math.pow(10, randf(this.l2_decay_min, this.l2_decay_max)); // l2 weight decay\n\n      var lr = Math.pow(10, randf(this.learning_rate_min, this.learning_rate_max)); // learning rate\n\n      var mom = randf(this.momentum_min, this.momentum_max); // momentum. Lets just use 0.9, works okay usually ;p\n\n      var tp = randf(0, 1); // trainer type\n\n      var trainer_def;\n\n      if (tp < 0.33) {\n        trainer_def = {\n          method: 'adadelta',\n          batch_size: bs,\n          l2_decay: l2\n        };\n      } else if (tp < 0.66) {\n        trainer_def = {\n          method: 'adagrad',\n          learning_rate: lr,\n          batch_size: bs,\n          l2_decay: l2\n        };\n      } else {\n        trainer_def = {\n          method: 'sgd',\n          learning_rate: lr,\n          momentum: mom,\n          batch_size: bs,\n          l2_decay: l2\n        };\n      }\n\n      var trainer = new Trainer(net, trainer_def);\n      var cand = {};\n      cand.acc = [];\n      cand.accv = 0; // this will maintained as sum(acc) for convenience\n\n      cand.layer_defs = layer_defs;\n      cand.trainer_def = trainer_def;\n      cand.net = net;\n      cand.trainer = trainer;\n      return cand;\n    },\n    // sets this.candidates with this.num_candidates candidate nets\n    sampleCandidates: function () {\n      this.candidates = []; // flush, if any\n\n      for (var i = 0; i < this.num_candidates; i++) {\n        var cand = this.sampleCandidate();\n        this.candidates.push(cand);\n      }\n    },\n    step: function () {\n      // run an example through current candidate\n      this.iter++; // step all candidates on a random data point\n\n      var fold = this.folds[this.foldix]; // active fold\n\n      var dataix = fold.train_ix[randi(0, fold.train_ix.length)];\n\n      for (var k = 0; k < this.candidates.length; k++) {\n        var x = this.data[dataix];\n        var l = this.labels[dataix];\n        this.candidates[k].trainer.train(x, l);\n      } // process consequences: sample new folds, or candidates\n\n\n      var lastiter = this.num_epochs * fold.train_ix.length;\n\n      if (this.iter >= lastiter) {\n        // finished evaluation of this fold. Get final validation\n        // accuracies, record them, and go on to next fold.\n        var val_acc = this.evalValErrors();\n\n        for (var k = 0; k < this.candidates.length; k++) {\n          var c = this.candidates[k];\n          c.acc.push(val_acc[k]);\n          c.accv += val_acc[k];\n        }\n\n        this.iter = 0; // reset step number\n\n        this.foldix++; // increment fold\n\n        if (this.finish_fold_callback !== null) {\n          this.finish_fold_callback();\n        }\n\n        if (this.foldix >= this.folds.length) {\n          // we finished all folds as well! Record these candidates\n          // and sample new ones to evaluate.\n          for (var k = 0; k < this.candidates.length; k++) {\n            this.evaluated_candidates.push(this.candidates[k]);\n          } // sort evaluated candidates according to accuracy achieved\n\n\n          this.evaluated_candidates.sort(function (a, b) {\n            return a.accv / a.acc.length > b.accv / b.acc.length ? -1 : 1;\n          }); // and clip only to the top few ones (lets place limit at 3*ensemble_size)\n          // otherwise there are concerns with keeping these all in memory \n          // if MagicNet is being evaluated for a very long time\n\n          if (this.evaluated_candidates.length > 3 * this.ensemble_size) {\n            this.evaluated_candidates = this.evaluated_candidates.slice(0, 3 * this.ensemble_size);\n          }\n\n          if (this.finish_batch_callback !== null) {\n            this.finish_batch_callback();\n          }\n\n          this.sampleCandidates(); // begin with new candidates\n\n          this.foldix = 0; // reset this\n        } else {\n          // we will go on to another fold. reset all candidates nets\n          for (var k = 0; k < this.candidates.length; k++) {\n            var c = this.candidates[k];\n            var net = new Net();\n            net.makeLayers(c.layer_defs);\n            var trainer = new Trainer(net, c.trainer_def);\n            c.net = net;\n            c.trainer = trainer;\n          }\n        }\n      }\n    },\n    evalValErrors: function () {\n      // evaluate candidates on validation data and return performance of current networks\n      // as simple list\n      var vals = [];\n      var fold = this.folds[this.foldix]; // active fold\n\n      for (var k = 0; k < this.candidates.length; k++) {\n        var net = this.candidates[k].net;\n        var v = 0.0;\n\n        for (var q = 0; q < fold.test_ix.length; q++) {\n          var x = this.data[fold.test_ix[q]];\n          var l = this.labels[fold.test_ix[q]];\n          net.forward(x);\n          var yhat = net.getPrediction();\n          v += yhat === l ? 1.0 : 0.0; // 0 1 loss\n        }\n\n        v /= fold.test_ix.length; // normalize\n\n        vals.push(v);\n      }\n\n      return vals;\n    },\n    // returns prediction scores for given test data point, as Vol\n    // uses an averaged prediction from the best ensemble_size models\n    // x is a Vol.\n    predict_soft: function (data) {\n      // forward prop the best networks\n      // and accumulate probabilities at last layer into a an output Vol\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n\n      if (nv === 0) {\n        return new convnetjs.Vol(0, 0, 0);\n      } // not sure what to do here? we're not ready yet\n\n\n      var xout, n;\n\n      for (var j = 0; j < nv; j++) {\n        var net = this.evaluated_candidates[j].net;\n        var x = net.forward(data);\n\n        if (j === 0) {\n          xout = x;\n          n = x.w.length;\n        } else {\n          // add it on\n          for (var d = 0; d < n; d++) {\n            xout.w[d] += x.w[d];\n          }\n        }\n      } // produce average\n\n\n      for (var d = 0; d < n; d++) {\n        xout.w[d] /= n;\n      }\n\n      return xout;\n    },\n    predict: function (data) {\n      var xout = this.predict_soft(data);\n\n      if (xout.w.length !== 0) {\n        var stats = maxmin(xout.w);\n        var predicted_label = stats.maxi;\n      } else {\n        var predicted_label = -1; // error out\n      }\n\n      return predicted_label;\n    },\n    toJSON: function () {\n      // dump the top ensemble_size networks as a list\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n      var json = {};\n      json.nets = [];\n\n      for (var i = 0; i < nv; i++) {\n        json.nets.push(this.evaluated_candidates[i].net.toJSON());\n      }\n\n      return json;\n    },\n    fromJSON: function (json) {\n      this.ensemble_size = json.nets.length;\n      this.evaluated_candidates = [];\n\n      for (var i = 0; i < this.ensemble_size; i++) {\n        var net = new Net();\n        net.fromJSON(json.nets[i]);\n        var dummy_candidate = {};\n        dummy_candidate.net = net;\n        this.evaluated_candidates.push(dummy_candidate);\n      }\n    },\n    // callback functions\n    // called when a fold is finished, while evaluating a batch\n    onFinishFold: function (f) {\n      this.finish_fold_callback = f;\n    },\n    // called when a batch of candidates has finished evaluating\n    onFinishBatch: function (f) {\n      this.finish_batch_callback = f;\n    }\n  };\n  global.MagicNet = MagicNet;\n})(convnetjs);\n\nthis.convnetjs = convnetjs;","map":{"version":3,"sources":["lib/external/convnet/1.1.0/convnet.js"],"names":["convnetjs","REVISION","global","return_v","v_val","gaussRandom","u","Math","random","v","r","c","sqrt","log","randf","a","b","randi","floor","randn","mu","std","zeros","n","isNaN","ArrayBuffer","arr","Array","i","Float64Array","arrContains","elt","length","arrUnique","push","maxmin","w","maxv","minv","maxi","mini","dv","randperm","j","temp","array","q","weightedSample","lst","probs","p","cumprob","k","getopt","opt","field_name","default_value","Vol","sx","sy","depth","Object","prototype","toString","call","dw","scale","get","x","y","d","ix","set","add","get_grad","set_grad","add_grad","cloneAndZero","clone","V","addFrom","addFromScaled","setConst","toJSON","json","fromJSON","augment","crop","dx","dy","fliplr","W","W2","img_to_vol","img","convert_grayscale","canvas","document","createElement","width","height","ctx","getContext","drawImage","e","name","img_data","getImageData","data","H","pv","x1","ConvLayer","out_depth","filters","in_depth","in_sx","in_sy","stride","pad","l1_decay_mul","l2_decay_mul","out_sx","out_sy","layer_type","bias","bias_pref","biases","forward","is_training","in_act","A","V_sx","V_sy","xy_stride","f","ay","ax","fy","oy","fx","ox","fd","out_act","backward","chain_grad","ix1","ix2","getParamsAndGrads","response","params","grads","FullyConnLayer","num_neurons","num_inputs","Vw","wi","tfi","PoolLayer","switchx","switchy","winx","winy","InputLayer","SoftmaxLayer","as","amax","es","esum","exp","indicator","mul","RegressionLayer","loss","dim","yi","val","SVMLayer","yscore","margin","ReluLayer","V2","N","V2w","SigmoidLayer","v2wi","MaxoutLayer","group_size","switches","ai","a2","tanh","TanhLayer","DropoutLayer","drop_prob","dropped","LocalResponseNormalizationLayer","alpha","beta","console","S_cache_","n2","den","max","min","aa","pow","S","SB","SB2","aj","g","Net","options","layers","makeLayers","defs","type","desugar","new_defs","def","num_classes","activation","tensor","gs","prev","QuadTransformLayer","act","getCostLoss","layer_reponse","getPrediction","Lj","t","L","Trainer","net","learning_rate","l1_decay","l2_decay","batch_size","method","momentum","ro","eps","gsum","xsum","train","start","Date","getTime","end","fwd_time","cost_loss","l2_decay_loss","l1_decay_loss","bwd_time","pglist","pg","plen","abs","l1grad","l2grad","gij","gsumi","xsumi","softmax_loss","SGDTrainer","MagicNet","labels","train_ratio","num_folds","num_candidates","num_epochs","ensemble_size","batch_size_min","batch_size_max","l2_decay_min","l2_decay_max","learning_rate_min","learning_rate_max","momentum_min","momentum_max","neurons_min","neurons_max","folds","candidates","evaluated_candidates","unique_labels","iter","foldix","finish_fold_callback","finish_batch_callback","sampleFolds","sampleCandidates","num_train","train_ix","slice","test_ix","sampleCandidate","input_depth","layer_defs","nl","ni","dp","bs","l2","lr","mom","tp","trainer_def","trainer","cand","acc","accv","step","fold","dataix","l","lastiter","val_acc","evalValErrors","sort","vals","yhat","predict_soft","nv","xout","predict","stats","predicted_label","nets","dummy_candidate","onFinishFold","onFinishBatch"],"mappings":"AAAA,IAAIA,SAAS,GAAGA,SAAS,IAAI;AAAEC,EAAAA,QAAQ,EAAE;AAAZ,CAA7B;;AACA,CAAC,UAASC,MAAT,EAAiB;AAChB,eADgB,CAGhB;;AACA,MAAIC,QAAQ,GAAG,KAAf;AACA,MAAIC,KAAK,GAAG,GAAZ;;AACA,MAAIC,WAAW,GAAG,YAAW;AAC3B,QAAGF,QAAH,EAAa;AACXA,MAAAA,QAAQ,GAAG,KAAX;AACA,aAAOC,KAAP;AACD;;AACD,QAAIE,CAAC,GAAG,IAAEC,IAAI,CAACC,MAAL,EAAF,GAAgB,CAAxB;AACA,QAAIC,CAAC,GAAG,IAAEF,IAAI,CAACC,MAAL,EAAF,GAAgB,CAAxB;AACA,QAAIE,CAAC,GAAGJ,CAAC,GAACA,CAAF,GAAMG,CAAC,GAACA,CAAhB;AACA,QAAGC,CAAC,IAAI,CAAL,IAAUA,CAAC,GAAG,CAAjB,EAAoB,OAAOL,WAAW,EAAlB;AACpB,QAAIM,CAAC,GAAGJ,IAAI,CAACK,IAAL,CAAU,CAAC,CAAD,GAAGL,IAAI,CAACM,GAAL,CAASH,CAAT,CAAH,GAAeA,CAAzB,CAAR;AACAN,IAAAA,KAAK,GAAGK,CAAC,GAACE,CAAV,CAV2B,CAUd;;AACbR,IAAAA,QAAQ,GAAG,IAAX;AACA,WAAOG,CAAC,GAACK,CAAT;AACD,GAbD;;AAcA,MAAIG,KAAK,GAAG,UAASC,CAAT,EAAYC,CAAZ,EAAe;AAAE,WAAOT,IAAI,CAACC,MAAL,MAAeQ,CAAC,GAACD,CAAjB,IAAoBA,CAA3B;AAA+B,GAA5D;;AACA,MAAIE,KAAK,GAAG,UAASF,CAAT,EAAYC,CAAZ,EAAe;AAAE,WAAOT,IAAI,CAACW,KAAL,CAAWX,IAAI,CAACC,MAAL,MAAeQ,CAAC,GAACD,CAAjB,IAAoBA,CAA/B,CAAP;AAA2C,GAAxE;;AACA,MAAII,KAAK,GAAG,UAASC,EAAT,EAAaC,GAAb,EAAiB;AAAE,WAAOD,EAAE,GAACf,WAAW,KAAGgB,GAAxB;AAA8B,GAA7D,CAtBgB,CAwBhB;;;AACA,MAAIC,KAAK,GAAG,UAASC,CAAT,EAAY;AACtB,QAAG,OAAOA,CAAP,KAAY,WAAZ,IAA2BC,KAAK,CAACD,CAAD,CAAnC,EAAwC;AAAE,aAAO,EAAP;AAAY;;AACtD,QAAG,OAAOE,WAAP,KAAuB,WAA1B,EAAuC;AACrC;AACA,UAAIC,GAAG,GAAG,IAAIC,KAAJ,CAAUJ,CAAV,CAAV;;AACA,WAAI,IAAIK,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACL,CAAd,EAAgBK,CAAC,EAAjB,EAAqB;AAAEF,QAAAA,GAAG,CAACE,CAAD,CAAH,GAAQ,CAAR;AAAY;;AACnC,aAAOF,GAAP;AACD,KALD,MAKO;AACL,aAAO,IAAIG,YAAJ,CAAiBN,CAAjB,CAAP;AACD;AACF,GAVD;;AAYA,MAAIO,WAAW,GAAG,UAASJ,GAAT,EAAcK,GAAd,EAAmB;AACnC,SAAI,IAAIH,CAAC,GAAC,CAAN,EAAQL,CAAC,GAACG,GAAG,CAACM,MAAlB,EAAyBJ,CAAC,GAACL,CAA3B,EAA6BK,CAAC,EAA9B,EAAkC;AAChC,UAAGF,GAAG,CAACE,CAAD,CAAH,KAASG,GAAZ,EAAiB,OAAO,IAAP;AAClB;;AACD,WAAO,KAAP;AACD,GALD;;AAOA,MAAIE,SAAS,GAAG,UAASP,GAAT,EAAc;AAC5B,QAAIV,CAAC,GAAG,EAAR;;AACA,SAAI,IAAIY,CAAC,GAAC,CAAN,EAAQL,CAAC,GAACG,GAAG,CAACM,MAAlB,EAAyBJ,CAAC,GAACL,CAA3B,EAA6BK,CAAC,EAA9B,EAAkC;AAChC,UAAG,CAACE,WAAW,CAACd,CAAD,EAAIU,GAAG,CAACE,CAAD,CAAP,CAAf,EAA4B;AAC1BZ,QAAAA,CAAC,CAACkB,IAAF,CAAOR,GAAG,CAACE,CAAD,CAAV;AACD;AACF;;AACD,WAAOZ,CAAP;AACD,GARD,CA5CgB,CAsDhB;;;AACA,MAAImB,MAAM,GAAG,UAASC,CAAT,EAAY;AACvB,QAAGA,CAAC,CAACJ,MAAF,KAAa,CAAhB,EAAmB;AAAE,aAAO,EAAP;AAAY,KADV,CACW;;;AAClC,QAAIK,IAAI,GAAGD,CAAC,CAAC,CAAD,CAAZ;AACA,QAAIE,IAAI,GAAGF,CAAC,CAAC,CAAD,CAAZ;AACA,QAAIG,IAAI,GAAG,CAAX;AACA,QAAIC,IAAI,GAAG,CAAX;AACA,QAAIjB,CAAC,GAAGa,CAAC,CAACJ,MAAV;;AACA,SAAI,IAAIJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACL,CAAd,EAAgBK,CAAC,EAAjB,EAAqB;AACnB,UAAGQ,CAAC,CAACR,CAAD,CAAD,GAAOS,IAAV,EAAgB;AAAEA,QAAAA,IAAI,GAAGD,CAAC,CAACR,CAAD,CAAR;AAAaW,QAAAA,IAAI,GAAGX,CAAP;AAAW;;AAC1C,UAAGQ,CAAC,CAACR,CAAD,CAAD,GAAOU,IAAV,EAAgB;AAAEA,QAAAA,IAAI,GAAGF,CAAC,CAACR,CAAD,CAAR;AAAaY,QAAAA,IAAI,GAAGZ,CAAP;AAAW;AAC3C;;AACD,WAAO;AAACW,MAAAA,IAAI,EAAEA,IAAP;AAAaF,MAAAA,IAAI,EAAEA,IAAnB;AAAyBG,MAAAA,IAAI,EAAEA,IAA/B;AAAqCF,MAAAA,IAAI,EAAEA,IAA3C;AAAiDG,MAAAA,EAAE,EAACJ,IAAI,GAACC;AAAzD,KAAP;AACD,GAZD,CAvDgB,CAqEhB;;;AACA,MAAII,QAAQ,GAAG,UAASnB,CAAT,EAAY;AACzB,QAAIK,CAAC,GAAGL,CAAR;AAAA,QACIoB,CAAC,GAAG,CADR;AAAA,QAEIC,IAFJ;AAGA,QAAIC,KAAK,GAAG,EAAZ;;AACA,SAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACvB,CAAd,EAAgBuB,CAAC,EAAjB,EAAoBD,KAAK,CAACC,CAAD,CAAL,GAASA,CAAT;;AACpB,WAAOlB,CAAC,EAAR,EAAY;AACRe,MAAAA,CAAC,GAAGpC,IAAI,CAACW,KAAL,CAAWX,IAAI,CAACC,MAAL,MAAiBoB,CAAC,GAAC,CAAnB,CAAX,CAAJ;AACAgB,MAAAA,IAAI,GAAGC,KAAK,CAACjB,CAAD,CAAZ;AACAiB,MAAAA,KAAK,CAACjB,CAAD,CAAL,GAAWiB,KAAK,CAACF,CAAD,CAAhB;AACAE,MAAAA,KAAK,CAACF,CAAD,CAAL,GAAWC,IAAX;AACH;;AACD,WAAOC,KAAP;AACD,GAbD,CAtEgB,CAqFhB;AACA;;;AACA,MAAIE,cAAc,GAAG,UAASC,GAAT,EAAcC,KAAd,EAAqB;AACxC,QAAIC,CAAC,GAAGpC,KAAK,CAAC,CAAD,EAAI,GAAJ,CAAb;AACA,QAAIqC,OAAO,GAAG,GAAd;;AACA,SAAI,IAAIC,CAAC,GAAC,CAAN,EAAQ7B,CAAC,GAACyB,GAAG,CAAChB,MAAlB,EAAyBoB,CAAC,GAAC7B,CAA3B,EAA6B6B,CAAC,EAA9B,EAAkC;AAChCD,MAAAA,OAAO,IAAIF,KAAK,CAACG,CAAD,CAAhB;;AACA,UAAGF,CAAC,GAAGC,OAAP,EAAgB;AAAE,eAAOH,GAAG,CAACI,CAAD,CAAV;AAAgB;AACnC;AACF,GAPD,CAvFgB,CAgGhB;;;AACA,MAAIC,MAAM,GAAG,UAASC,GAAT,EAAcC,UAAd,EAA0BC,aAA1B,EAAyC;AACpD,WAAO,OAAOF,GAAG,CAACC,UAAD,CAAV,KAA2B,WAA3B,GAAyCD,GAAG,CAACC,UAAD,CAA5C,GAA2DC,aAAlE;AACD,GAFD;;AAIAtD,EAAAA,MAAM,CAACY,KAAP,GAAeA,KAAf;AACAZ,EAAAA,MAAM,CAACe,KAAP,GAAeA,KAAf;AACAf,EAAAA,MAAM,CAACiB,KAAP,GAAeA,KAAf;AACAjB,EAAAA,MAAM,CAACoB,KAAP,GAAeA,KAAf;AACApB,EAAAA,MAAM,CAACiC,MAAP,GAAgBA,MAAhB;AACAjC,EAAAA,MAAM,CAACwC,QAAP,GAAkBA,QAAlB;AACAxC,EAAAA,MAAM,CAAC6C,cAAP,GAAwBA,cAAxB;AACA7C,EAAAA,MAAM,CAAC+B,SAAP,GAAmBA,SAAnB;AACA/B,EAAAA,MAAM,CAAC4B,WAAP,GAAqBA,WAArB;AACA5B,EAAAA,MAAM,CAACmD,MAAP,GAAgBA,MAAhB;AAED,CAhHD,EAgHGrD,SAhHH;;AAiHA,CAAC,UAASE,MAAT,EAAiB;AAChB,eADgB,CAGhB;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,MAAIuD,GAAG,GAAG,UAASC,EAAT,EAAaC,EAAb,EAAiBC,KAAjB,EAAwBjD,CAAxB,EAA2B;AACnC;AACA,QAAGkD,MAAM,CAACC,SAAP,CAAiBC,QAAjB,CAA0BC,IAA1B,CAA+BN,EAA/B,MAAuC,gBAA1C,EAA4D;AAC1D;AACA,WAAKA,EAAL,GAAU,CAAV;AACA,WAAKC,EAAL,GAAU,CAAV;AACA,WAAKC,KAAL,GAAaF,EAAE,CAAC1B,MAAhB,CAJ0D,CAK1D;AACA;;AACA,WAAKI,CAAL,GAASlC,MAAM,CAACoB,KAAP,CAAa,KAAKsC,KAAlB,CAAT;AACA,WAAKK,EAAL,GAAU/D,MAAM,CAACoB,KAAP,CAAa,KAAKsC,KAAlB,CAAV;;AACA,WAAI,IAAIhC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKgC,KAAnB,EAAyBhC,CAAC,EAA1B,EAA8B;AAC5B,aAAKQ,CAAL,CAAOR,CAAP,IAAY8B,EAAE,CAAC9B,CAAD,CAAd;AACD;AACF,KAZD,MAYO;AACL;AACA,WAAK8B,EAAL,GAAUA,EAAV;AACA,WAAKC,EAAL,GAAUA,EAAV;AACA,WAAKC,KAAL,GAAaA,KAAb;AACA,UAAIrC,CAAC,GAAGmC,EAAE,GAACC,EAAH,GAAMC,KAAd;AACA,WAAKxB,CAAL,GAASlC,MAAM,CAACoB,KAAP,CAAaC,CAAb,CAAT;AACA,WAAK0C,EAAL,GAAU/D,MAAM,CAACoB,KAAP,CAAaC,CAAb,CAAV;;AACA,UAAG,OAAOZ,CAAP,KAAa,WAAhB,EAA6B;AAC3B;AACA;AACA;AACA,YAAIuD,KAAK,GAAG3D,IAAI,CAACK,IAAL,CAAU,OAAK8C,EAAE,GAACC,EAAH,GAAMC,KAAX,CAAV,CAAZ;;AACA,aAAI,IAAIhC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACL,CAAd,EAAgBK,CAAC,EAAjB,EAAqB;AACnB,eAAKQ,CAAL,CAAOR,CAAP,IAAY1B,MAAM,CAACiB,KAAP,CAAa,GAAb,EAAkB+C,KAAlB,CAAZ;AACD;AACF,OARD,MAQO;AACL,aAAI,IAAItC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACL,CAAd,EAAgBK,CAAC,EAAjB,EAAqB;AACnB,eAAKQ,CAAL,CAAOR,CAAP,IAAYjB,CAAZ;AACD;AACF;AACF;AACF,GApCD;;AAsCA8C,EAAAA,GAAG,CAACK,SAAJ,GAAgB;AACdK,IAAAA,GAAG,EAAE,UAASC,CAAT,EAAYC,CAAZ,EAAeC,CAAf,EAAkB;AACrB,UAAIC,EAAE,GAAC,CAAE,KAAKb,EAAL,GAAUW,CAAX,GAAcD,CAAf,IAAkB,KAAKR,KAAvB,GAA6BU,CAApC;AACA,aAAO,KAAKlC,CAAL,CAAOmC,EAAP,CAAP;AACD,KAJa;AAKdC,IAAAA,GAAG,EAAE,UAASJ,CAAT,EAAYC,CAAZ,EAAeC,CAAf,EAAkB7D,CAAlB,EAAqB;AACxB,UAAI8D,EAAE,GAAC,CAAE,KAAKb,EAAL,GAAUW,CAAX,GAAcD,CAAf,IAAkB,KAAKR,KAAvB,GAA6BU,CAApC;AACA,WAAKlC,CAAL,CAAOmC,EAAP,IAAa9D,CAAb;AACD,KARa;AASdgE,IAAAA,GAAG,EAAE,UAASL,CAAT,EAAYC,CAAZ,EAAeC,CAAf,EAAkB7D,CAAlB,EAAqB;AACxB,UAAI8D,EAAE,GAAC,CAAE,KAAKb,EAAL,GAAUW,CAAX,GAAcD,CAAf,IAAkB,KAAKR,KAAvB,GAA6BU,CAApC;AACA,WAAKlC,CAAL,CAAOmC,EAAP,KAAc9D,CAAd;AACD,KAZa;AAadiE,IAAAA,QAAQ,EAAE,UAASN,CAAT,EAAYC,CAAZ,EAAeC,CAAf,EAAkB;AAC1B,UAAIC,EAAE,GAAG,CAAE,KAAKb,EAAL,GAAUW,CAAX,GAAcD,CAAf,IAAkB,KAAKR,KAAvB,GAA6BU,CAAtC;AACA,aAAO,KAAKL,EAAL,CAAQM,EAAR,CAAP;AACD,KAhBa;AAiBdI,IAAAA,QAAQ,EAAE,UAASP,CAAT,EAAYC,CAAZ,EAAeC,CAAf,EAAkB7D,CAAlB,EAAqB;AAC7B,UAAI8D,EAAE,GAAG,CAAE,KAAKb,EAAL,GAAUW,CAAX,GAAcD,CAAf,IAAkB,KAAKR,KAAvB,GAA6BU,CAAtC;AACA,WAAKL,EAAL,CAAQM,EAAR,IAAc9D,CAAd;AACD,KApBa;AAqBdmE,IAAAA,QAAQ,EAAE,UAASR,CAAT,EAAYC,CAAZ,EAAeC,CAAf,EAAkB7D,CAAlB,EAAqB;AAC7B,UAAI8D,EAAE,GAAG,CAAE,KAAKb,EAAL,GAAUW,CAAX,GAAcD,CAAf,IAAkB,KAAKR,KAAvB,GAA6BU,CAAtC;AACA,WAAKL,EAAL,CAAQM,EAAR,KAAe9D,CAAf;AACD,KAxBa;AAyBdoE,IAAAA,YAAY,EAAE,YAAW;AAAE,aAAO,IAAIpB,GAAJ,CAAQ,KAAKC,EAAb,EAAiB,KAAKC,EAAtB,EAA0B,KAAKC,KAA/B,EAAsC,GAAtC,CAAP;AAAkD,KAzB/D;AA0BdkB,IAAAA,KAAK,EAAE,YAAW;AAChB,UAAIC,CAAC,GAAG,IAAItB,GAAJ,CAAQ,KAAKC,EAAb,EAAiB,KAAKC,EAAtB,EAA0B,KAAKC,KAA/B,EAAsC,GAAtC,CAAR;AACA,UAAIrC,CAAC,GAAG,KAAKa,CAAL,CAAOJ,MAAf;;AACA,WAAI,IAAIJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACL,CAAd,EAAgBK,CAAC,EAAjB,EAAqB;AAAEmD,QAAAA,CAAC,CAAC3C,CAAF,CAAIR,CAAJ,IAAS,KAAKQ,CAAL,CAAOR,CAAP,CAAT;AAAqB;;AAC5C,aAAOmD,CAAP;AACD,KA/Ba;AAgCdC,IAAAA,OAAO,EAAE,UAASD,CAAT,EAAY;AAAE,WAAI,IAAI3B,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKhB,CAAL,CAAOJ,MAArB,EAA4BoB,CAAC,EAA7B,EAAiC;AAAE,aAAKhB,CAAL,CAAOgB,CAAP,KAAa2B,CAAC,CAAC3C,CAAF,CAAIgB,CAAJ,CAAb;AAAsB;AAAC,KAhCnE;AAiCd6B,IAAAA,aAAa,EAAE,UAASF,CAAT,EAAYhE,CAAZ,EAAe;AAAE,WAAI,IAAIqC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKhB,CAAL,CAAOJ,MAArB,EAA4BoB,CAAC,EAA7B,EAAiC;AAAE,aAAKhB,CAAL,CAAOgB,CAAP,KAAarC,CAAC,GAACgE,CAAC,CAAC3C,CAAF,CAAIgB,CAAJ,CAAf;AAAwB;AAAC,KAjC9E;AAkCd8B,IAAAA,QAAQ,EAAE,UAASnE,CAAT,EAAY;AAAE,WAAI,IAAIqC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKhB,CAAL,CAAOJ,MAArB,EAA4BoB,CAAC,EAA7B,EAAiC;AAAE,aAAKhB,CAAL,CAAOgB,CAAP,IAAYrC,CAAZ;AAAgB;AAAC,KAlC9D;AAoCdoE,IAAAA,MAAM,EAAE,YAAW;AACjB;AACA,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC1B,EAAL,GAAU,KAAKA,EAAf;AACA0B,MAAAA,IAAI,CAACzB,EAAL,GAAU,KAAKA,EAAf;AACAyB,MAAAA,IAAI,CAACxB,KAAL,GAAa,KAAKA,KAAlB;AACAwB,MAAAA,IAAI,CAAChD,CAAL,GAAS,KAAKA,CAAd;AACA,aAAOgD,IAAP,CAPiB,CAQjB;AACD,KA7Ca;AA8CdC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK1B,EAAL,GAAU0B,IAAI,CAAC1B,EAAf;AACA,WAAKC,EAAL,GAAUyB,IAAI,CAACzB,EAAf;AACA,WAAKC,KAAL,GAAawB,IAAI,CAACxB,KAAlB;AAEA,UAAIrC,CAAC,GAAG,KAAKmC,EAAL,GAAQ,KAAKC,EAAb,GAAgB,KAAKC,KAA7B;AACA,WAAKxB,CAAL,GAASlC,MAAM,CAACoB,KAAP,CAAaC,CAAb,CAAT;AACA,WAAK0C,EAAL,GAAU/D,MAAM,CAACoB,KAAP,CAAaC,CAAb,CAAV,CAPuB,CAQvB;;AACA,WAAI,IAAIK,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACL,CAAd,EAAgBK,CAAC,EAAjB,EAAqB;AACnB,aAAKQ,CAAL,CAAOR,CAAP,IAAYwD,IAAI,CAAChD,CAAL,CAAOR,CAAP,CAAZ;AACD;AACF;AA1Da,GAAhB;AA6DA1B,EAAAA,MAAM,CAACuD,GAAP,GAAaA,GAAb;AACD,CA9GD,EA8GGzD,SA9GH;;AA+GA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;AAEtB;AACA;AACA;AACA;AACA;;AACA,MAAI6B,OAAO,GAAG,UAASP,CAAT,EAAYQ,IAAZ,EAAkBC,EAAlB,EAAsBC,EAAtB,EAA0BC,MAA1B,EAAkC;AAC9C;AACA,QAAG,OAAOA,MAAP,KAAiB,WAApB,EAAiC,IAAIA,MAAM,GAAG,KAAb;AACjC,QAAG,OAAOF,EAAP,KAAa,WAAhB,EAA6B,IAAIA,EAAE,GAAGtF,MAAM,CAACe,KAAP,CAAa,CAAb,EAAgB8D,CAAC,CAACrB,EAAF,GAAO6B,IAAvB,CAAT;AAC7B,QAAG,OAAOE,EAAP,KAAa,WAAhB,EAA6B,IAAIA,EAAE,GAAGvF,MAAM,CAACe,KAAP,CAAa,CAAb,EAAgB8D,CAAC,CAACpB,EAAF,GAAO4B,IAAvB,CAAT,CAJiB,CAM9C;;AACA,QAAII,CAAJ;;AACA,QAAGJ,IAAI,KAAKR,CAAC,CAACrB,EAAX,IAAiB8B,EAAE,KAAG,CAAtB,IAA2BC,EAAE,KAAG,CAAnC,EAAsC;AACpCE,MAAAA,CAAC,GAAG,IAAIlC,GAAJ,CAAQ8B,IAAR,EAAcA,IAAd,EAAoBR,CAAC,CAACnB,KAAtB,EAA6B,GAA7B,CAAJ;;AACA,WAAI,IAAIQ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACmB,IAAd,EAAmBnB,CAAC,EAApB,EAAwB;AACtB,aAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACkB,IAAd,EAAmBlB,CAAC,EAApB,EAAwB;AACtB,cAAGD,CAAC,GAACoB,EAAF,GAAK,CAAL,IAAUpB,CAAC,GAACoB,EAAF,IAAMT,CAAC,CAACrB,EAAlB,IAAwBW,CAAC,GAACoB,EAAF,GAAK,CAA7B,IAAkCpB,CAAC,GAACoB,EAAF,IAAMV,CAAC,CAACpB,EAA7C,EAAiD,SAD3B,CACqC;;AAC3D,eAAI,IAAIW,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACS,CAAC,CAACnB,KAAhB,EAAsBU,CAAC,EAAvB,EAA2B;AAC1BqB,YAAAA,CAAC,CAACnB,GAAF,CAAMJ,CAAN,EAAQC,CAAR,EAAUC,CAAV,EAAYS,CAAC,CAACZ,GAAF,CAAMC,CAAC,GAACoB,EAAR,EAAWnB,CAAC,GAACoB,EAAb,EAAgBnB,CAAhB,CAAZ,EAD0B,CACO;AACjC;AACF;AACF;AACF,KAVD,MAUO;AACLqB,MAAAA,CAAC,GAAGZ,CAAJ;AACD;;AAED,QAAGW,MAAH,EAAW;AACT;AACA,UAAIE,EAAE,GAAGD,CAAC,CAACd,YAAF,EAAT;;AACA,WAAI,IAAIT,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACuB,CAAC,CAACjC,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,aAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACsB,CAAC,CAAChC,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,eAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACqB,CAAC,CAAC/B,KAAhB,EAAsBU,CAAC,EAAvB,EAA2B;AAC1BsB,YAAAA,EAAE,CAACpB,GAAH,CAAOJ,CAAP,EAASC,CAAT,EAAWC,CAAX,EAAaqB,CAAC,CAACxB,GAAF,CAAMwB,CAAC,CAACjC,EAAF,GAAOU,CAAP,GAAW,CAAjB,EAAmBC,CAAnB,EAAqBC,CAArB,CAAb,EAD0B,CACa;AACvC;AACF;AACF;;AACDqB,MAAAA,CAAC,GAAGC,EAAJ,CAVS,CAUD;AACT;;AACD,WAAOD,CAAP;AACD,GAnCD,CATgB,CA8ChB;AACA;;;AACA,MAAIE,UAAU,GAAG,UAASC,GAAT,EAAcC,iBAAd,EAAiC;AAEhD,QAAG,OAAOA,iBAAP,KAA4B,WAA/B,EAA4C,IAAIA,iBAAiB,GAAG,KAAxB;AAE5C,QAAIC,MAAM,GAAGC,QAAQ,CAACC,aAAT,CAAuB,QAAvB,CAAb;AACAF,IAAAA,MAAM,CAACG,KAAP,GAAeL,GAAG,CAACK,KAAnB;AACAH,IAAAA,MAAM,CAACI,MAAP,GAAgBN,GAAG,CAACM,MAApB;AACA,QAAIC,GAAG,GAAGL,MAAM,CAACM,UAAP,CAAkB,IAAlB,CAAV,CAPgD,CAShD;;AACA,QAAI;AACFD,MAAAA,GAAG,CAACE,SAAJ,CAAcT,GAAd,EAAmB,CAAnB,EAAsB,CAAtB;AACD,KAFD,CAEE,OAAOU,CAAP,EAAU;AACV,UAAIA,CAAC,CAACC,IAAF,KAAW,wBAAf,EAAyC;AACvC;AACA,eAAO,KAAP;AACD,OAHD,MAGO;AACL,cAAMD,CAAN;AACD;AACF;;AAED,QAAI;AACF,UAAIE,QAAQ,GAAGL,GAAG,CAACM,YAAJ,CAAiB,CAAjB,EAAoB,CAApB,EAAuBX,MAAM,CAACG,KAA9B,EAAqCH,MAAM,CAACI,MAA5C,CAAf;AACD,KAFD,CAEE,OAAOI,CAAP,EAAU;AACV,UAAGA,CAAC,CAACC,IAAF,KAAW,gBAAd,EAAgC;AAC9B,eAAO,KAAP,CAD8B,CAChB;AACf,OAFD,MAEO;AACL,cAAMD,CAAN;AACD;AACF,KA7B+C,CA+BhD;;;AACA,QAAItD,CAAC,GAAGwD,QAAQ,CAACE,IAAjB;AACA,QAAIjB,CAAC,GAAGG,GAAG,CAACK,KAAZ;AACA,QAAIU,CAAC,GAAGf,GAAG,CAACM,MAAZ;AACA,QAAIU,EAAE,GAAG,EAAT;;AACA,SAAI,IAAIlF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACsB,CAAC,CAAClB,MAAhB,EAAuBJ,CAAC,EAAxB,EAA4B;AAC1BkF,MAAAA,EAAE,CAAC5E,IAAH,CAAQgB,CAAC,CAACtB,CAAD,CAAD,GAAK,KAAL,GAAW,GAAnB,EAD0B,CACD;AAC1B;;AACD,QAAIwC,CAAC,GAAG,IAAIX,GAAJ,CAAQkC,CAAR,EAAWkB,CAAX,EAAc,CAAd,EAAiB,GAAjB,CAAR,CAvCgD,CAuCjB;;AAC/BzC,IAAAA,CAAC,CAAChC,CAAF,GAAM0E,EAAN;;AAEA,QAAGf,iBAAH,EAAsB;AACpB;AACA,UAAIgB,EAAE,GAAG,IAAItD,GAAJ,CAAQkC,CAAR,EAAWkB,CAAX,EAAc,CAAd,EAAiB,GAAjB,CAAT;;AACA,WAAI,IAAIjF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC+D,CAAd,EAAgB/D,CAAC,EAAjB,EAAqB;AACnB,aAAI,IAAIe,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACkE,CAAd,EAAgBlE,CAAC,EAAjB,EAAqB;AACnBoE,UAAAA,EAAE,CAACvC,GAAH,CAAO5C,CAAP,EAASe,CAAT,EAAW,CAAX,EAAayB,CAAC,CAACD,GAAF,CAAMvC,CAAN,EAAQe,CAAR,EAAU,CAAV,CAAb;AACD;AACF;;AACDyB,MAAAA,CAAC,GAAG2C,EAAJ;AACD;;AAED,WAAO3C,CAAP;AACD,GAtDD;;AAwDAlE,EAAAA,MAAM,CAACoF,OAAP,GAAiBA,OAAjB;AACApF,EAAAA,MAAM,CAAC2F,UAAP,GAAoBA,UAApB;AAED,CA3GD,EA2GG7F,SA3GH;;AA4GA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;AAEtB;AACA;AACA;AACA;AACA;AACA;;AACA,MAAIuD,SAAS,GAAG,UAAS1D,GAAT,EAAc;AAC5B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD4B,CAG5B;;AACA,SAAK2D,SAAL,GAAiB3D,GAAG,CAAC4D,OAArB;AACA,SAAKxD,EAAL,GAAUJ,GAAG,CAACI,EAAd,CAL4B,CAKV;;AAClB,SAAKyD,QAAL,GAAgB7D,GAAG,CAAC6D,QAApB;AACA,SAAKC,KAAL,GAAa9D,GAAG,CAAC8D,KAAjB;AACA,SAAKC,KAAL,GAAa/D,GAAG,CAAC+D,KAAjB,CAR4B,CAU5B;;AACA,SAAK1D,EAAL,GAAU,OAAOL,GAAG,CAACK,EAAX,KAAkB,WAAlB,GAAgCL,GAAG,CAACK,EAApC,GAAyC,KAAKD,EAAxD;AACA,SAAK4D,MAAL,GAAc,OAAOhE,GAAG,CAACgE,MAAX,KAAsB,WAAtB,GAAoChE,GAAG,CAACgE,MAAxC,GAAiD,CAA/D,CAZ4B,CAYsC;;AAClE,SAAKC,GAAL,GAAW,OAAOjE,GAAG,CAACiE,GAAX,KAAmB,WAAnB,GAAiCjE,GAAG,CAACiE,GAArC,GAA2C,CAAtD,CAb4B,CAa6B;;AACzD,SAAKC,YAAL,GAAoB,OAAOlE,GAAG,CAACkE,YAAX,KAA4B,WAA5B,GAA0ClE,GAAG,CAACkE,YAA9C,GAA6D,GAAjF;AACA,SAAKC,YAAL,GAAoB,OAAOnE,GAAG,CAACmE,YAAX,KAA4B,WAA5B,GAA0CnE,GAAG,CAACmE,YAA9C,GAA6D,GAAjF,CAf4B,CAiB5B;AACA;AACA;AACA;;AACA,SAAKC,MAAL,GAAcnH,IAAI,CAACW,KAAL,CAAW,CAAC,KAAKkG,KAAL,GAAa,KAAKG,GAAL,GAAW,CAAxB,GAA4B,KAAK7D,EAAlC,IAAwC,KAAK4D,MAA7C,GAAsD,CAAjE,CAAd;AACA,SAAKK,MAAL,GAAcpH,IAAI,CAACW,KAAL,CAAW,CAAC,KAAKmG,KAAL,GAAa,KAAKE,GAAL,GAAW,CAAxB,GAA4B,KAAK5D,EAAlC,IAAwC,KAAK2D,MAA7C,GAAsD,CAAjE,CAAd;AACA,SAAKM,UAAL,GAAkB,MAAlB,CAvB4B,CAyB5B;;AACA,QAAIC,IAAI,GAAG,OAAOvE,GAAG,CAACwE,SAAX,KAAyB,WAAzB,GAAuCxE,GAAG,CAACwE,SAA3C,GAAuD,GAAlE;AACA,SAAKZ,OAAL,GAAe,EAAf;;AACA,SAAI,IAAItF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAAE,WAAKsF,OAAL,CAAahF,IAAb,CAAkB,IAAIuB,GAAJ,CAAQ,KAAKC,EAAb,EAAiB,KAAKC,EAAtB,EAA0B,KAAKwD,QAA/B,CAAlB;AAA8D;;AAClG,SAAKY,MAAL,GAAc,IAAItE,GAAJ,CAAQ,CAAR,EAAW,CAAX,EAAc,KAAKwD,SAAnB,EAA8BY,IAA9B,CAAd;AACD,GA9BD;;AA+BAb,EAAAA,SAAS,CAAClD,SAAV,GAAsB;AACpBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC;AAEA,WAAKC,MAAL,GAAcnD,CAAd;AACA,UAAIoD,CAAC,GAAG,IAAI1E,GAAJ,CAAQ,KAAKiE,MAAL,GAAa,CAArB,EAAwB,KAAKC,MAAL,GAAa,CAArC,EAAwC,KAAKV,SAAL,GAAgB,CAAxD,EAA2D,GAA3D,CAAR;AAEA,UAAImB,IAAI,GAAGrD,CAAC,CAACrB,EAAF,GAAM,CAAjB;AACA,UAAI2E,IAAI,GAAGtD,CAAC,CAACpB,EAAF,GAAM,CAAjB;AACA,UAAI2E,SAAS,GAAG,KAAKhB,MAAL,GAAa,CAA7B;;AAEA,WAAI,IAAIhD,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK2C,SAAnB,EAA6B3C,CAAC,EAA9B,EAAkC;AAChC,YAAIiE,CAAC,GAAG,KAAKrB,OAAL,CAAa5C,CAAb,CAAR;AACA,YAAIF,CAAC,GAAG,CAAC,KAAKmD,GAAN,GAAW,CAAnB;AACA,YAAIlD,CAAC,GAAG,CAAC,KAAKkD,GAAN,GAAW,CAAnB;;AACA,aAAI,IAAIiB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKb,MAAtB,EAA8BtD,CAAC,IAAEiE,SAAH,EAAaE,EAAE,EAA7C,EAAiD;AAAG;AAClDpE,UAAAA,CAAC,GAAG,CAAC,KAAKmD,GAAN,GAAW,CAAf;;AACA,eAAI,IAAIkB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKf,MAAtB,EAA8BtD,CAAC,IAAEkE,SAAH,EAAaG,EAAE,EAA7C,EAAiD;AAAG;AAElD;AACA,gBAAI1H,CAAC,GAAG,GAAR;;AACA,iBAAI,IAAI2H,EAAE,GAAC,CAAX,EAAaA,EAAE,GAACH,CAAC,CAAC5E,EAAlB,EAAqB+E,EAAE,EAAvB,EAA2B;AACzB,kBAAIC,EAAE,GAAGtE,CAAC,GAACqE,EAAX,CADyB,CACV;;AACf,mBAAI,IAAIE,EAAE,GAAC,CAAX,EAAaA,EAAE,GAACL,CAAC,CAAC7E,EAAlB,EAAqBkF,EAAE,EAAvB,EAA2B;AACzB,oBAAIC,EAAE,GAAGzE,CAAC,GAACwE,EAAX;;AACA,oBAAGD,EAAE,IAAE,CAAJ,IAASA,EAAE,GAACN,IAAZ,IAAoBQ,EAAE,IAAE,CAAxB,IAA6BA,EAAE,GAACT,IAAnC,EAAyC;AACvC,uBAAI,IAAIU,EAAE,GAAC,CAAX,EAAaA,EAAE,GAACP,CAAC,CAAC3E,KAAlB,EAAwBkF,EAAE,EAA1B,EAA8B;AAC5B;AACA/H,oBAAAA,CAAC,IAAIwH,CAAC,CAACnG,CAAF,CAAI,CAAEmG,CAAC,CAAC7E,EAAF,GAAOgF,EAAR,GAAYE,EAAb,IAAiBL,CAAC,CAAC3E,KAAnB,GAAyBkF,EAA7B,IAAmC/D,CAAC,CAAC3C,CAAF,CAAI,CAAEgG,IAAI,GAAGO,EAAR,GAAYE,EAAb,IAAiB9D,CAAC,CAACnB,KAAnB,GAAyBkF,EAA7B,CAAxC;AACD;AACF;AACF;AACF;;AACD/H,YAAAA,CAAC,IAAI,KAAKgH,MAAL,CAAY3F,CAAZ,CAAckC,CAAd,CAAL;AACA6D,YAAAA,CAAC,CAAC3D,GAAF,CAAMiE,EAAN,EAAUD,EAAV,EAAclE,CAAd,EAAiBvD,CAAjB;AACD;AACF;AACF;;AACD,WAAKgI,OAAL,GAAeZ,CAAf;AACA,aAAO,KAAKY,OAAZ;AACD,KAxCmB;AAyCpBC,IAAAA,QAAQ,EAAE,YAAW;AAEnB,UAAIjE,CAAC,GAAG,KAAKmD,MAAb;AACAnD,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAayD,CAAC,CAAC3C,CAAF,CAAIJ,MAAjB,CAAP,CAHmB,CAGc;;AAEjC,UAAIoG,IAAI,GAAGrD,CAAC,CAACrB,EAAF,GAAM,CAAjB;AACA,UAAI2E,IAAI,GAAGtD,CAAC,CAACpB,EAAF,GAAM,CAAjB;AACA,UAAI2E,SAAS,GAAG,KAAKhB,MAAL,GAAa,CAA7B;;AAEA,WAAI,IAAIhD,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK2C,SAAnB,EAA6B3C,CAAC,EAA9B,EAAkC;AAChC,YAAIiE,CAAC,GAAG,KAAKrB,OAAL,CAAa5C,CAAb,CAAR;AACA,YAAIF,CAAC,GAAG,CAAC,KAAKmD,GAAN,GAAW,CAAnB;AACA,YAAIlD,CAAC,GAAG,CAAC,KAAKkD,GAAN,GAAW,CAAnB;;AACA,aAAI,IAAIiB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKb,MAAtB,EAA8BtD,CAAC,IAAEiE,SAAH,EAAaE,EAAE,EAA7C,EAAiD;AAAG;AAClDpE,UAAAA,CAAC,GAAG,CAAC,KAAKmD,GAAN,GAAW,CAAf;;AACA,eAAI,IAAIkB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKf,MAAtB,EAA8BtD,CAAC,IAAEkE,SAAH,EAAaG,EAAE,EAA7C,EAAiD;AAAG;AAElD;AACA,gBAAIQ,UAAU,GAAG,KAAKF,OAAL,CAAarE,QAAb,CAAsB+D,EAAtB,EAAyBD,EAAzB,EAA4BlE,CAA5B,CAAjB,CAH+C,CAGE;;AACjD,iBAAI,IAAIoE,EAAE,GAAC,CAAX,EAAaA,EAAE,GAACH,CAAC,CAAC5E,EAAlB,EAAqB+E,EAAE,EAAvB,EAA2B;AACzB,kBAAIC,EAAE,GAAGtE,CAAC,GAACqE,EAAX,CADyB,CACV;;AACf,mBAAI,IAAIE,EAAE,GAAC,CAAX,EAAaA,EAAE,GAACL,CAAC,CAAC7E,EAAlB,EAAqBkF,EAAE,EAAvB,EAA2B;AACzB,oBAAIC,EAAE,GAAGzE,CAAC,GAACwE,EAAX;;AACA,oBAAGD,EAAE,IAAE,CAAJ,IAASA,EAAE,GAACN,IAAZ,IAAoBQ,EAAE,IAAE,CAAxB,IAA6BA,EAAE,GAACT,IAAnC,EAAyC;AACvC,uBAAI,IAAIU,EAAE,GAAC,CAAX,EAAaA,EAAE,GAACP,CAAC,CAAC3E,KAAlB,EAAwBkF,EAAE,EAA1B,EAA8B;AAC5B;AACA,wBAAII,GAAG,GAAG,CAAEd,IAAI,GAAGO,EAAR,GAAYE,EAAb,IAAiB9D,CAAC,CAACnB,KAAnB,GAAyBkF,EAAnC;AACA,wBAAIK,GAAG,GAAG,CAAEZ,CAAC,CAAC7E,EAAF,GAAOgF,EAAR,GAAYE,EAAb,IAAiBL,CAAC,CAAC3E,KAAnB,GAAyBkF,EAAnC;AACAP,oBAAAA,CAAC,CAACtE,EAAF,CAAKkF,GAAL,KAAapE,CAAC,CAAC3C,CAAF,CAAI8G,GAAJ,IAASD,UAAtB;AACAlE,oBAAAA,CAAC,CAACd,EAAF,CAAKiF,GAAL,KAAaX,CAAC,CAACnG,CAAF,CAAI+G,GAAJ,IAASF,UAAtB;AACD;AACF;AACF;AACF;;AACD,iBAAKlB,MAAL,CAAY9D,EAAZ,CAAeK,CAAf,KAAqB2E,UAArB;AACD;AACF;AACF;AACF,KA/EmB;AAgFpBG,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,UAAIC,QAAQ,GAAG,EAAf;;AACA,WAAI,IAAIzH,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChCyH,QAAAA,QAAQ,CAACnH,IAAT,CAAc;AAACoH,UAAAA,MAAM,EAAE,KAAKpC,OAAL,CAAatF,CAAb,EAAgBQ,CAAzB;AAA4BmH,UAAAA,KAAK,EAAE,KAAKrC,OAAL,CAAatF,CAAb,EAAgBqC,EAAnD;AAAuDwD,UAAAA,YAAY,EAAE,KAAKA,YAA1E;AAAwFD,UAAAA,YAAY,EAAE,KAAKA;AAA3G,SAAd;AACD;;AACD6B,MAAAA,QAAQ,CAACnH,IAAT,CAAc;AAACoH,QAAAA,MAAM,EAAE,KAAKvB,MAAL,CAAY3F,CAArB;AAAwBmH,QAAAA,KAAK,EAAE,KAAKxB,MAAL,CAAY9D,EAA3C;AAA+CuD,QAAAA,YAAY,EAAE,GAA7D;AAAkEC,QAAAA,YAAY,EAAE;AAAhF,OAAd;AACA,aAAO4B,QAAP;AACD,KAvFmB;AAwFpBlE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC1B,EAAL,GAAU,KAAKA,EAAf,CAFiB,CAEE;;AACnB0B,MAAAA,IAAI,CAACzB,EAAL,GAAU,KAAKA,EAAf;AACAyB,MAAAA,IAAI,CAACkC,MAAL,GAAc,KAAKA,MAAnB;AACAlC,MAAAA,IAAI,CAAC+B,QAAL,GAAgB,KAAKA,QAArB;AACA/B,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAACoC,YAAL,GAAoB,KAAKA,YAAzB;AACApC,MAAAA,IAAI,CAACqC,YAAL,GAAoB,KAAKA,YAAzB;AACArC,MAAAA,IAAI,CAACmC,GAAL,GAAW,KAAKA,GAAhB;AACAnC,MAAAA,IAAI,CAAC8B,OAAL,GAAe,EAAf;;AACA,WAAI,IAAItF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKsF,OAAL,CAAalF,MAA3B,EAAkCJ,CAAC,EAAnC,EAAuC;AACrCwD,QAAAA,IAAI,CAAC8B,OAAL,CAAahF,IAAb,CAAkB,KAAKgF,OAAL,CAAatF,CAAb,EAAgBuD,MAAhB,EAAlB;AACD;;AACDC,MAAAA,IAAI,CAAC2C,MAAL,GAAc,KAAKA,MAAL,CAAY5C,MAAZ,EAAd;AACA,aAAOC,IAAP;AACD,KA3GmB;AA4GpBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAKlE,EAAL,GAAU0B,IAAI,CAAC1B,EAAf,CALuB,CAKJ;;AACnB,WAAKC,EAAL,GAAUyB,IAAI,CAACzB,EAAf;AACA,WAAK2D,MAAL,GAAclC,IAAI,CAACkC,MAAnB;AACA,WAAKH,QAAL,GAAgB/B,IAAI,CAAC+B,QAArB,CARuB,CAQQ;;AAC/B,WAAKD,OAAL,GAAe,EAAf;AACA,WAAKM,YAAL,GAAoB,OAAOpC,IAAI,CAACoC,YAAZ,KAA6B,WAA7B,GAA2CpC,IAAI,CAACoC,YAAhD,GAA+D,GAAnF;AACA,WAAKC,YAAL,GAAoB,OAAOrC,IAAI,CAACqC,YAAZ,KAA6B,WAA7B,GAA2CrC,IAAI,CAACqC,YAAhD,GAA+D,GAAnF;AACA,WAAKF,GAAL,GAAW,OAAOnC,IAAI,CAACmC,GAAZ,KAAoB,WAApB,GAAkCnC,IAAI,CAACmC,GAAvC,GAA6C,CAAxD;;AACA,WAAI,IAAI3F,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACwD,IAAI,CAAC8B,OAAL,CAAalF,MAA3B,EAAkCJ,CAAC,EAAnC,EAAuC;AACrC,YAAInB,CAAC,GAAG,IAAIgD,GAAJ,CAAQ,CAAR,EAAU,CAAV,EAAY,CAAZ,EAAc,CAAd,CAAR;AACAhD,QAAAA,CAAC,CAAC4E,QAAF,CAAWD,IAAI,CAAC8B,OAAL,CAAatF,CAAb,CAAX;AACA,aAAKsF,OAAL,CAAahF,IAAb,CAAkBzB,CAAlB;AACD;;AACD,WAAKsH,MAAL,GAAc,IAAItE,GAAJ,CAAQ,CAAR,EAAU,CAAV,EAAY,CAAZ,EAAc,CAAd,CAAd;AACA,WAAKsE,MAAL,CAAY1C,QAAZ,CAAqBD,IAAI,CAAC2C,MAA1B;AACD;AAhImB,GAAtB;;AAmIA,MAAIyB,cAAc,GAAG,UAASlG,GAAT,EAAc;AACjC,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CADiC,CAGjC;AACA;;AACA,SAAK2D,SAAL,GAAiB,OAAO3D,GAAG,CAACmG,WAAX,KAA2B,WAA3B,GAAyCnG,GAAG,CAACmG,WAA7C,GAA2DnG,GAAG,CAAC4D,OAAhF,CALiC,CAOjC;;AACA,SAAKM,YAAL,GAAoB,OAAOlE,GAAG,CAACkE,YAAX,KAA4B,WAA5B,GAA0ClE,GAAG,CAACkE,YAA9C,GAA6D,GAAjF;AACA,SAAKC,YAAL,GAAoB,OAAOnE,GAAG,CAACmE,YAAX,KAA4B,WAA5B,GAA0CnE,GAAG,CAACmE,YAA9C,GAA6D,GAAjF,CATiC,CAWjC;;AACA,SAAKiC,UAAL,GAAkBpG,GAAG,CAAC8D,KAAJ,GAAY9D,GAAG,CAAC+D,KAAhB,GAAwB/D,GAAG,CAAC6D,QAA9C;AACA,SAAKO,MAAL,GAAc,CAAd;AACA,SAAKC,MAAL,GAAc,CAAd;AACA,SAAKC,UAAL,GAAkB,IAAlB,CAfiC,CAiBjC;;AACA,QAAIC,IAAI,GAAG,OAAOvE,GAAG,CAACwE,SAAX,KAAyB,WAAzB,GAAuCxE,GAAG,CAACwE,SAA3C,GAAuD,GAAlE;AACA,SAAKZ,OAAL,GAAe,EAAf;;AACA,SAAI,IAAItF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA8BrF,CAAC,EAA/B,EAAmC;AAAE,WAAKsF,OAAL,CAAahF,IAAb,CAAkB,IAAIuB,GAAJ,CAAQ,CAAR,EAAW,CAAX,EAAc,KAAKiG,UAAnB,CAAlB;AAAoD;;AACzF,SAAK3B,MAAL,GAAc,IAAItE,GAAJ,CAAQ,CAAR,EAAW,CAAX,EAAc,KAAKwD,SAAnB,EAA8BY,IAA9B,CAAd;AACD,GAtBD;;AAwBA2B,EAAAA,cAAc,CAAC1F,SAAf,GAA2B;AACzBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,UAAIoD,CAAC,GAAG,IAAI1E,GAAJ,CAAQ,CAAR,EAAW,CAAX,EAAc,KAAKwD,SAAnB,EAA8B,GAA9B,CAAR;AACA,UAAI0C,EAAE,GAAG5E,CAAC,CAAC3C,CAAX;;AACA,WAAI,IAAIR,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC,YAAIb,CAAC,GAAG,GAAR;AACA,YAAI6I,EAAE,GAAG,KAAK1C,OAAL,CAAatF,CAAb,EAAgBQ,CAAzB;;AACA,aAAI,IAAIkC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKoF,UAAnB,EAA8BpF,CAAC,EAA/B,EAAmC;AACjCvD,UAAAA,CAAC,IAAI4I,EAAE,CAACrF,CAAD,CAAF,GAAQsF,EAAE,CAACtF,CAAD,CAAf,CADiC,CACb;AACrB;;AACDvD,QAAAA,CAAC,IAAI,KAAKgH,MAAL,CAAY3F,CAAZ,CAAcR,CAAd,CAAL;AACAuG,QAAAA,CAAC,CAAC/F,CAAF,CAAIR,CAAJ,IAASb,CAAT;AACD;;AACD,WAAKgI,OAAL,GAAeZ,CAAf;AACA,aAAO,KAAKY,OAAZ;AACD,KAhBwB;AAiBzBC,IAAAA,QAAQ,EAAE,YAAW;AACnB,UAAIjE,CAAC,GAAG,KAAKmD,MAAb;AACAnD,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAayD,CAAC,CAAC3C,CAAF,CAAIJ,MAAjB,CAAP,CAFmB,CAEc;AAEjC;;AACA,WAAI,IAAIJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC,YAAIiI,GAAG,GAAG,KAAK3C,OAAL,CAAatF,CAAb,CAAV;AACA,YAAIqH,UAAU,GAAG,KAAKF,OAAL,CAAa9E,EAAb,CAAgBrC,CAAhB,CAAjB;;AACA,aAAI,IAAI0C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKoF,UAAnB,EAA8BpF,CAAC,EAA/B,EAAmC;AACjCS,UAAAA,CAAC,CAACd,EAAF,CAAKK,CAAL,KAAWuF,GAAG,CAACzH,CAAJ,CAAMkC,CAAN,IAAS2E,UAApB,CADiC,CACD;;AAChCY,UAAAA,GAAG,CAAC5F,EAAJ,CAAOK,CAAP,KAAaS,CAAC,CAAC3C,CAAF,CAAIkC,CAAJ,IAAO2E,UAApB,CAFiC,CAED;AACjC;;AACD,aAAKlB,MAAL,CAAY9D,EAAZ,CAAerC,CAAf,KAAqBqH,UAArB;AACD;AACF,KA/BwB;AAgCzBG,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,UAAIC,QAAQ,GAAG,EAAf;;AACA,WAAI,IAAIzH,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChCyH,QAAAA,QAAQ,CAACnH,IAAT,CAAc;AAACoH,UAAAA,MAAM,EAAE,KAAKpC,OAAL,CAAatF,CAAb,EAAgBQ,CAAzB;AAA4BmH,UAAAA,KAAK,EAAE,KAAKrC,OAAL,CAAatF,CAAb,EAAgBqC,EAAnD;AAAuDuD,UAAAA,YAAY,EAAE,KAAKA,YAA1E;AAAwFC,UAAAA,YAAY,EAAE,KAAKA;AAA3G,SAAd;AACD;;AACD4B,MAAAA,QAAQ,CAACnH,IAAT,CAAc;AAACoH,QAAAA,MAAM,EAAE,KAAKvB,MAAL,CAAY3F,CAArB;AAAwBmH,QAAAA,KAAK,EAAE,KAAKxB,MAAL,CAAY9D,EAA3C;AAA+CuD,QAAAA,YAAY,EAAE,GAA7D;AAAkEC,QAAAA,YAAY,EAAE;AAAhF,OAAd;AACA,aAAO4B,QAAP;AACD,KAvCwB;AAwCzBlE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAACsE,UAAL,GAAkB,KAAKA,UAAvB;AACAtE,MAAAA,IAAI,CAACoC,YAAL,GAAoB,KAAKA,YAAzB;AACApC,MAAAA,IAAI,CAACqC,YAAL,GAAoB,KAAKA,YAAzB;AACArC,MAAAA,IAAI,CAAC8B,OAAL,GAAe,EAAf;;AACA,WAAI,IAAItF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKsF,OAAL,CAAalF,MAA3B,EAAkCJ,CAAC,EAAnC,EAAuC;AACrCwD,QAAAA,IAAI,CAAC8B,OAAL,CAAahF,IAAb,CAAkB,KAAKgF,OAAL,CAAatF,CAAb,EAAgBuD,MAAhB,EAAlB;AACD;;AACDC,MAAAA,IAAI,CAAC2C,MAAL,GAAc,KAAKA,MAAL,CAAY5C,MAAZ,EAAd;AACA,aAAOC,IAAP;AACD,KAvDwB;AAwDzBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAK8B,UAAL,GAAkBtE,IAAI,CAACsE,UAAvB;AACA,WAAKlC,YAAL,GAAoB,OAAOpC,IAAI,CAACoC,YAAZ,KAA6B,WAA7B,GAA2CpC,IAAI,CAACoC,YAAhD,GAA+D,GAAnF;AACA,WAAKC,YAAL,GAAoB,OAAOrC,IAAI,CAACqC,YAAZ,KAA6B,WAA7B,GAA2CrC,IAAI,CAACqC,YAAhD,GAA+D,GAAnF;AACA,WAAKP,OAAL,GAAe,EAAf;;AACA,WAAI,IAAItF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACwD,IAAI,CAAC8B,OAAL,CAAalF,MAA3B,EAAkCJ,CAAC,EAAnC,EAAuC;AACrC,YAAInB,CAAC,GAAG,IAAIgD,GAAJ,CAAQ,CAAR,EAAU,CAAV,EAAY,CAAZ,EAAc,CAAd,CAAR;AACAhD,QAAAA,CAAC,CAAC4E,QAAF,CAAWD,IAAI,CAAC8B,OAAL,CAAatF,CAAb,CAAX;AACA,aAAKsF,OAAL,CAAahF,IAAb,CAAkBzB,CAAlB;AACD;;AACD,WAAKsH,MAAL,GAAc,IAAItE,GAAJ,CAAQ,CAAR,EAAU,CAAV,EAAY,CAAZ,EAAc,CAAd,CAAd;AACA,WAAKsE,MAAL,CAAY1C,QAAZ,CAAqBD,IAAI,CAAC2C,MAA1B;AACD;AAxEwB,GAA3B;AA2EA7H,EAAAA,MAAM,CAAC8G,SAAP,GAAmBA,SAAnB;AACA9G,EAAAA,MAAM,CAACsJ,cAAP,GAAwBA,cAAxB;AAED,CAlRD,EAkRGxJ,SAlRH;;AAmRA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;;AAEtB,MAAIqG,SAAS,GAAG,UAASxG,GAAT,EAAc;AAE5B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAF4B,CAI5B;;AACA,SAAKI,EAAL,GAAUJ,GAAG,CAACI,EAAd,CAL4B,CAKV;;AAClB,SAAKyD,QAAL,GAAgB7D,GAAG,CAAC6D,QAApB;AACA,SAAKC,KAAL,GAAa9D,GAAG,CAAC8D,KAAjB;AACA,SAAKC,KAAL,GAAa/D,GAAG,CAAC+D,KAAjB,CAR4B,CAU5B;;AACA,SAAK1D,EAAL,GAAU,OAAOL,GAAG,CAACK,EAAX,KAAkB,WAAlB,GAAgCL,GAAG,CAACK,EAApC,GAAyC,KAAKD,EAAxD;AACA,SAAK4D,MAAL,GAAc,OAAOhE,GAAG,CAACgE,MAAX,KAAsB,WAAtB,GAAoChE,GAAG,CAACgE,MAAxC,GAAiD,CAA/D;AACA,SAAKC,GAAL,GAAW,OAAOjE,GAAG,CAACiE,GAAX,KAAmB,WAAnB,GAAiCjE,GAAG,CAACiE,GAArC,GAA2C,CAAtD,CAb4B,CAa6B;AAEzD;;AACA,SAAKN,SAAL,GAAiB,KAAKE,QAAtB;AACA,SAAKO,MAAL,GAAcnH,IAAI,CAACW,KAAL,CAAW,CAAC,KAAKkG,KAAL,GAAa,KAAKG,GAAL,GAAW,CAAxB,GAA4B,KAAK7D,EAAlC,IAAwC,KAAK4D,MAA7C,GAAsD,CAAjE,CAAd;AACA,SAAKK,MAAL,GAAcpH,IAAI,CAACW,KAAL,CAAW,CAAC,KAAKmG,KAAL,GAAa,KAAKE,GAAL,GAAW,CAAxB,GAA4B,KAAK5D,EAAlC,IAAwC,KAAK2D,MAA7C,GAAsD,CAAjE,CAAd;AACA,SAAKM,UAAL,GAAkB,MAAlB,CAnB4B,CAoB5B;;AACA,SAAKmC,OAAL,GAAe7J,MAAM,CAACoB,KAAP,CAAa,KAAKoG,MAAL,GAAY,KAAKC,MAAjB,GAAwB,KAAKV,SAA1C,CAAf;AACA,SAAK+C,OAAL,GAAe9J,MAAM,CAACoB,KAAP,CAAa,KAAKoG,MAAL,GAAY,KAAKC,MAAjB,GAAwB,KAAKV,SAA1C,CAAf;AACD,GAvBD;;AAyBA6C,EAAAA,SAAS,CAAChG,SAAV,GAAsB;AACpBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AAEA,UAAIoD,CAAC,GAAG,IAAI1E,GAAJ,CAAQ,KAAKiE,MAAb,EAAqB,KAAKC,MAA1B,EAAkC,KAAKV,SAAvC,EAAkD,GAAlD,CAAR;AAEA,UAAI1F,CAAC,GAAC,CAAN,CALgC,CAKvB;;AACT,WAAI,IAAI+C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK2C,SAAnB,EAA6B3C,CAAC,EAA9B,EAAkC;AAChC,YAAIF,CAAC,GAAG,CAAC,KAAKmD,GAAd;AACA,YAAIlD,CAAC,GAAG,CAAC,KAAKkD,GAAd;;AACA,aAAI,IAAIkB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKf,MAAtB,EAA8BtD,CAAC,IAAE,KAAKkD,MAAR,EAAemB,EAAE,EAA/C,EAAmD;AACjDpE,UAAAA,CAAC,GAAG,CAAC,KAAKkD,GAAV;;AACA,eAAI,IAAIiB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKb,MAAtB,EAA8BtD,CAAC,IAAE,KAAKiD,MAAR,EAAekB,EAAE,EAA/C,EAAmD;AAEjD;AACA,gBAAIzH,CAAC,GAAG,CAAC,KAAT,CAHiD,CAGjC;;AAChB,gBAAIkJ,IAAI,GAAC,CAAC,CAAV;AAAA,gBAAYC,IAAI,GAAC,CAAC,CAAlB;;AACA,iBAAI,IAAItB,EAAE,GAAC,CAAX,EAAaA,EAAE,GAAC,KAAKlF,EAArB,EAAwBkF,EAAE,EAA1B,EAA8B;AAC5B,mBAAI,IAAIF,EAAE,GAAC,CAAX,EAAaA,EAAE,GAAC,KAAK/E,EAArB,EAAwB+E,EAAE,EAA1B,EAA8B;AAC5B,oBAAIC,EAAE,GAAGtE,CAAC,GAACqE,EAAX;AACA,oBAAIG,EAAE,GAAGzE,CAAC,GAACwE,EAAX;;AACA,oBAAGD,EAAE,IAAE,CAAJ,IAASA,EAAE,GAAC5D,CAAC,CAACpB,EAAd,IAAoBkF,EAAE,IAAE,CAAxB,IAA6BA,EAAE,GAAC9D,CAAC,CAACrB,EAArC,EAAyC;AACvC,sBAAIjD,CAAC,GAAGsE,CAAC,CAACZ,GAAF,CAAM0E,EAAN,EAAUF,EAAV,EAAcrE,CAAd,CAAR,CADuC,CAEvC;AACA;AACA;;AACA,sBAAG7D,CAAC,GAAGM,CAAP,EAAU;AAAEA,oBAAAA,CAAC,GAAGN,CAAJ;AAAOwJ,oBAAAA,IAAI,GAACpB,EAAL;AAASqB,oBAAAA,IAAI,GAACvB,EAAL;AAAS;AACtC;AACF;AACF;;AACD,iBAAKoB,OAAL,CAAaxI,CAAb,IAAkB0I,IAAlB;AACA,iBAAKD,OAAL,CAAazI,CAAb,IAAkB2I,IAAlB;AACA3I,YAAAA,CAAC;AACD4G,YAAAA,CAAC,CAAC3D,GAAF,CAAMiE,EAAN,EAAUD,EAAV,EAAclE,CAAd,EAAiBvD,CAAjB;AACD;AACF;AACF;;AACD,WAAKgI,OAAL,GAAeZ,CAAf;AACA,aAAO,KAAKY,OAAZ;AACD,KAvCmB;AAwCpBC,IAAAA,QAAQ,EAAE,YAAW;AACnB;AACA;AACA,UAAIjE,CAAC,GAAG,KAAKmD,MAAb;AACAnD,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAayD,CAAC,CAAC3C,CAAF,CAAIJ,MAAjB,CAAP,CAJmB,CAIc;;AACjC,UAAImG,CAAC,GAAG,KAAKY,OAAb,CALmB,CAKG;;AAEtB,UAAIxH,CAAC,GAAG,CAAR;;AACA,WAAI,IAAI+C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK2C,SAAnB,EAA6B3C,CAAC,EAA9B,EAAkC;AAChC,YAAIF,CAAC,GAAG,CAAC,KAAKmD,GAAd;AACA,YAAIlD,CAAC,GAAG,CAAC,KAAKkD,GAAd;;AACA,aAAI,IAAIkB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKf,MAAtB,EAA8BtD,CAAC,IAAE,KAAKkD,MAAR,EAAemB,EAAE,EAA/C,EAAmD;AACjDpE,UAAAA,CAAC,GAAG,CAAC,KAAKkD,GAAV;;AACA,eAAI,IAAIiB,EAAE,GAAC,CAAX,EAAcA,EAAE,GAAC,KAAKb,MAAtB,EAA8BtD,CAAC,IAAE,KAAKiD,MAAR,EAAekB,EAAE,EAA/C,EAAmD;AAEjD,gBAAIS,UAAU,GAAG,KAAKF,OAAL,CAAarE,QAAb,CAAsB+D,EAAtB,EAAyBD,EAAzB,EAA4BlE,CAA5B,CAAjB;AACAS,YAAAA,CAAC,CAACH,QAAF,CAAW,KAAKmF,OAAL,CAAaxI,CAAb,CAAX,EAA4B,KAAKyI,OAAL,CAAazI,CAAb,CAA5B,EAA6C+C,CAA7C,EAAgD2E,UAAhD;AACA1H,YAAAA,CAAC;AAEF;AACF;AACF;AACF,KA9DmB;AA+DpB6H,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAjEmB;AAkEpBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC1B,EAAL,GAAU,KAAKA,EAAf;AACA0B,MAAAA,IAAI,CAACzB,EAAL,GAAU,KAAKA,EAAf;AACAyB,MAAAA,IAAI,CAACkC,MAAL,GAAc,KAAKA,MAAnB;AACAlC,MAAAA,IAAI,CAAC+B,QAAL,GAAgB,KAAKA,QAArB;AACA/B,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAACmC,GAAL,GAAW,KAAKA,GAAhB;AACA,aAAOnC,IAAP;AACD,KA9EmB;AA+EpBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAKlE,EAAL,GAAU0B,IAAI,CAAC1B,EAAf;AACA,WAAKC,EAAL,GAAUyB,IAAI,CAACzB,EAAf;AACA,WAAK2D,MAAL,GAAclC,IAAI,CAACkC,MAAnB;AACA,WAAKH,QAAL,GAAgB/B,IAAI,CAAC+B,QAArB;AACA,WAAKI,GAAL,GAAW,OAAOnC,IAAI,CAACmC,GAAZ,KAAoB,WAApB,GAAkCnC,IAAI,CAACmC,GAAvC,GAA6C,CAAxD,CATuB,CASoC;;AAC3D,WAAKwC,OAAL,GAAe7J,MAAM,CAACoB,KAAP,CAAa,KAAKoG,MAAL,GAAY,KAAKC,MAAjB,GAAwB,KAAKV,SAA1C,CAAf,CAVuB,CAU8C;;AACrE,WAAK+C,OAAL,GAAe9J,MAAM,CAACoB,KAAP,CAAa,KAAKoG,MAAL,GAAY,KAAKC,MAAjB,GAAwB,KAAKV,SAA1C,CAAf;AACD;AA3FmB,GAAtB;AA8FA/G,EAAAA,MAAM,CAAC4J,SAAP,GAAmBA,SAAnB;AAED,CA7HD,EA6HG9J,SA7HH;;AA+HA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;;AAEtB,MAAI0G,UAAU,GAAG,UAAS7G,GAAT,EAAc;AAC7B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD6B,CAG7B;;AACA,SAAKoE,MAAL,GAAc,OAAOpE,GAAG,CAACoE,MAAX,KAAsB,WAAtB,GAAoCpE,GAAG,CAACoE,MAAxC,GAAiDpE,GAAG,CAAC8D,KAAnE;AACA,SAAKO,MAAL,GAAc,OAAOrE,GAAG,CAACqE,MAAX,KAAsB,WAAtB,GAAoCrE,GAAG,CAACqE,MAAxC,GAAiDrE,GAAG,CAAC+D,KAAnE;AACA,SAAKJ,SAAL,GAAiB,OAAO3D,GAAG,CAAC2D,SAAX,KAAyB,WAAzB,GAAuC3D,GAAG,CAAC2D,SAA3C,GAAuD3D,GAAG,CAAC6D,QAA5E;AACA,SAAKS,UAAL,GAAkB,OAAlB;AACD,GARD;;AASAuC,EAAAA,UAAU,CAACrG,SAAX,GAAuB;AACrBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,WAAKgE,OAAL,GAAehE,CAAf;AACA,aAAO,KAAKgE,OAAZ,CAHgC,CAGX;AACtB,KALoB;AAMrBC,IAAAA,QAAQ,EAAE,YAAW,CAAG,CANH;AAOrBI,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAToB;AAUrBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOxC,IAAP;AACD,KAjBoB;AAkBrBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACD;AAvBoB,GAAvB;AA0BA1H,EAAAA,MAAM,CAACiK,UAAP,GAAoBA,UAApB;AACD,CAxCD,EAwCGnK,SAxCH;;AAyCA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;AAEtB;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;;AACA,MAAI2G,YAAY,GAAG,UAAS9G,GAAT,EAAc;AAC/B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD+B,CAG/B;;AACA,SAAKoG,UAAL,GAAkBpG,GAAG,CAAC8D,KAAJ,GAAY9D,GAAG,CAAC+D,KAAhB,GAAwB/D,GAAG,CAAC6D,QAA9C;AACA,SAAKF,SAAL,GAAiB,KAAKyC,UAAtB;AACA,SAAKhC,MAAL,GAAc,CAAd;AACA,SAAKC,MAAL,GAAc,CAAd;AACA,SAAKC,UAAL,GAAkB,SAAlB;AACD,GATD;;AAWAwC,EAAAA,YAAY,CAACtG,SAAb,GAAyB;AACvBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AAEA,UAAIoD,CAAC,GAAG,IAAI1E,GAAJ,CAAQ,CAAR,EAAW,CAAX,EAAc,KAAKwD,SAAnB,EAA8B,GAA9B,CAAR,CAHgC,CAKhC;;AACA,UAAIoD,EAAE,GAAGtF,CAAC,CAAC3C,CAAX;AACA,UAAIkI,IAAI,GAAGvF,CAAC,CAAC3C,CAAF,CAAI,CAAJ,CAAX;;AACA,WAAI,IAAIR,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC,YAAGyI,EAAE,CAACzI,CAAD,CAAF,GAAQ0I,IAAX,EAAiBA,IAAI,GAAGD,EAAE,CAACzI,CAAD,CAAT;AAClB,OAV+B,CAYhC;;;AACA,UAAI2I,EAAE,GAAGrK,MAAM,CAACoB,KAAP,CAAa,KAAK2F,SAAlB,CAAT;AACA,UAAIuD,IAAI,GAAG,GAAX;;AACA,WAAI,IAAI5I,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC,YAAI4E,CAAC,GAAGjG,IAAI,CAACkK,GAAL,CAASJ,EAAE,CAACzI,CAAD,CAAF,GAAQ0I,IAAjB,CAAR;AACAE,QAAAA,IAAI,IAAIhE,CAAR;AACA+D,QAAAA,EAAE,CAAC3I,CAAD,CAAF,GAAQ4E,CAAR;AACD,OAnB+B,CAqBhC;;;AACA,WAAI,IAAI5E,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC2I,QAAAA,EAAE,CAAC3I,CAAD,CAAF,IAAS4I,IAAT;AACArC,QAAAA,CAAC,CAAC/F,CAAF,CAAIR,CAAJ,IAAS2I,EAAE,CAAC3I,CAAD,CAAX;AACD;;AAED,WAAK2I,EAAL,GAAUA,EAAV,CA3BgC,CA2BlB;;AACd,WAAKxB,OAAL,GAAeZ,CAAf;AACA,aAAO,KAAKY,OAAZ;AACD,KA/BsB;AAgCvBC,IAAAA,QAAQ,EAAE,UAAS3E,CAAT,EAAY;AAEpB;AACA,UAAID,CAAC,GAAG,KAAK8D,MAAb;AACA9D,MAAAA,CAAC,CAACH,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAa8C,CAAC,CAAChC,CAAF,CAAIJ,MAAjB,CAAP,CAJoB,CAIa;;AAEjC,WAAI,IAAIJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC,YAAI8I,SAAS,GAAG9I,CAAC,KAAKyC,CAAN,GAAU,GAAV,GAAgB,GAAhC;AACA,YAAIsG,GAAG,GAAG,EAAED,SAAS,GAAG,KAAKH,EAAL,CAAQ3I,CAAR,CAAd,CAAV;AACAwC,QAAAA,CAAC,CAACH,EAAF,CAAKrC,CAAL,IAAU+I,GAAV;AACD,OAVmB,CAYpB;;;AACA,aAAO,CAACpK,IAAI,CAACM,GAAL,CAAS,KAAK0J,EAAL,CAAQlG,CAAR,CAAT,CAAR;AACD,KA9CsB;AA+CvB+E,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAjDsB;AAkDvBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAACsE,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOtE,IAAP;AACD,KA1DsB;AA2DvBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAK8B,UAAL,GAAkBtE,IAAI,CAACsE,UAAvB;AACD;AAjEsB,GAAzB,CAxBgB,CA4FhB;AACA;AACA;;AACA,MAAIkB,eAAe,GAAG,UAAStH,GAAT,EAAc;AAClC,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CADkC,CAGlC;;AACA,SAAKoG,UAAL,GAAkBpG,GAAG,CAAC8D,KAAJ,GAAY9D,GAAG,CAAC+D,KAAhB,GAAwB/D,GAAG,CAAC6D,QAA9C;AACA,SAAKF,SAAL,GAAiB,KAAKyC,UAAtB;AACA,SAAKhC,MAAL,GAAc,CAAd;AACA,SAAKC,MAAL,GAAc,CAAd;AACA,SAAKC,UAAL,GAAkB,YAAlB;AACD,GATD;;AAWAgD,EAAAA,eAAe,CAAC9G,SAAhB,GAA4B;AAC1BkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,WAAKgE,OAAL,GAAehE,CAAf;AACA,aAAOA,CAAP,CAHgC,CAGtB;AACX,KALyB;AAM1B;AACAiE,IAAAA,QAAQ,EAAE,UAAS3E,CAAT,EAAY;AAEpB;AACA,UAAID,CAAC,GAAG,KAAK8D,MAAb;AACA9D,MAAAA,CAAC,CAACH,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAa8C,CAAC,CAAChC,CAAF,CAAIJ,MAAjB,CAAP,CAJoB,CAIa;;AACjC,UAAI6I,IAAI,GAAG,GAAX;;AACA,UAAGxG,CAAC,YAAY1C,KAAb,IAAsB0C,CAAC,YAAYxC,YAAtC,EAAoD;AAClD,aAAI,IAAID,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC,cAAI6D,EAAE,GAAGrB,CAAC,CAAChC,CAAF,CAAIR,CAAJ,IAASyC,CAAC,CAACzC,CAAD,CAAnB;AACAwC,UAAAA,CAAC,CAACH,EAAF,CAAKrC,CAAL,IAAU6D,EAAV;AACAoF,UAAAA,IAAI,IAAI,IAAEpF,EAAF,GAAKA,EAAb;AACD;AACF,OAND,MAMO;AACL;AACA;AACA,YAAI7D,CAAC,GAAGyC,CAAC,CAACyG,GAAV;AACA,YAAIC,EAAE,GAAG1G,CAAC,CAAC2G,GAAX;AACA,YAAIvF,EAAE,GAAGrB,CAAC,CAAChC,CAAF,CAAIR,CAAJ,IAASmJ,EAAlB;AACA3G,QAAAA,CAAC,CAACH,EAAF,CAAKrC,CAAL,IAAU6D,EAAV;AACAoF,QAAAA,IAAI,IAAI,IAAEpF,EAAF,GAAKA,EAAb;AACD;;AACD,aAAOoF,IAAP;AACD,KA7ByB;AA8B1BzB,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAhCyB;AAiC1BjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAACsE,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOtE,IAAP;AACD,KAzCyB;AA0C1BC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAK8B,UAAL,GAAkBtE,IAAI,CAACsE,UAAvB;AACD;AAhDyB,GAA5B;;AAmDA,MAAIuB,QAAQ,GAAG,UAAS3H,GAAT,EAAc;AAC3B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD2B,CAG3B;;AACA,SAAKoG,UAAL,GAAkBpG,GAAG,CAAC8D,KAAJ,GAAY9D,GAAG,CAAC+D,KAAhB,GAAwB/D,GAAG,CAAC6D,QAA9C;AACA,SAAKF,SAAL,GAAiB,KAAKyC,UAAtB;AACA,SAAKhC,MAAL,GAAc,CAAd;AACA,SAAKC,MAAL,GAAc,CAAd;AACA,SAAKC,UAAL,GAAkB,KAAlB;AACD,GATD;;AAWAqD,EAAAA,QAAQ,CAACnH,SAAT,GAAqB;AACnBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,WAAKgE,OAAL,GAAehE,CAAf,CAFgC,CAEd;;AAClB,aAAOA,CAAP;AACD,KALkB;AAMnBiE,IAAAA,QAAQ,EAAE,UAAS3E,CAAT,EAAY;AAEpB;AACA,UAAID,CAAC,GAAG,KAAK8D,MAAb;AACA9D,MAAAA,CAAC,CAACH,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAa8C,CAAC,CAAChC,CAAF,CAAIJ,MAAjB,CAAP,CAJoB,CAIa;;AAEjC,UAAIkJ,MAAM,GAAG9G,CAAC,CAAChC,CAAF,CAAIiC,CAAJ,CAAb,CANoB,CAMC;;AACrB,UAAI8G,MAAM,GAAG,GAAb;AACA,UAAIN,IAAI,GAAG,GAAX;;AACA,WAAI,IAAIjJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqF,SAAnB,EAA6BrF,CAAC,EAA9B,EAAkC;AAChC,YAAG,CAACsJ,MAAD,GAAU9G,CAAC,CAAChC,CAAF,CAAIR,CAAJ,CAAV,GAAmBuJ,MAAnB,GAA4B,CAA/B,EAAkC;AAChC;AACA;AACA;AACA;AACA;AACA/G,UAAAA,CAAC,CAACH,EAAF,CAAKrC,CAAL,KAAW,CAAX;AACAwC,UAAAA,CAAC,CAACH,EAAF,CAAKI,CAAL,KAAW,CAAX;AACAwG,UAAAA,IAAI,IAAI,CAACK,MAAD,GAAU9G,CAAC,CAAChC,CAAF,CAAIR,CAAJ,CAAV,GAAmBuJ,MAA3B;AACD;AACF;;AAED,aAAON,IAAP;AACD,KA7BkB;AA8BnBzB,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAhCkB;AAiCnBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAACsE,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOtE,IAAP;AACD,KAzCkB;AA0CnBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAK8B,UAAL,GAAkBtE,IAAI,CAACsE,UAAvB;AACD;AAhDkB,GAArB;AAmDAxJ,EAAAA,MAAM,CAAC0K,eAAP,GAAyBA,eAAzB;AACA1K,EAAAA,MAAM,CAACkK,YAAP,GAAsBA,YAAtB;AACAlK,EAAAA,MAAM,CAAC+K,QAAP,GAAkBA,QAAlB;AAED,CA/ND,EA+NGjL,SA/NH;;AAiOA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;AAEtB;AACA;AACA;;AACA,MAAI2H,SAAS,GAAG,UAAS9H,GAAT,EAAc;AAC5B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD4B,CAG5B;;AACA,SAAKoE,MAAL,GAAcpE,GAAG,CAAC8D,KAAlB;AACA,SAAKO,MAAL,GAAcrE,GAAG,CAAC+D,KAAlB;AACA,SAAKJ,SAAL,GAAiB3D,GAAG,CAAC6D,QAArB;AACA,SAAKS,UAAL,GAAkB,MAAlB;AACD,GARD;;AASAwD,EAAAA,SAAS,CAACtH,SAAV,GAAsB;AACpBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,UAAIsG,EAAE,GAAGtG,CAAC,CAACD,KAAF,EAAT;AACA,UAAIwG,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;AACA,UAAIuJ,GAAG,GAAGF,EAAE,CAACjJ,CAAb;;AACA,WAAI,IAAIR,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,YAAG2J,GAAG,CAAC3J,CAAD,CAAH,GAAS,CAAZ,EAAe2J,GAAG,CAAC3J,CAAD,CAAH,GAAS,CAAT,CADI,CACQ;AAC5B;;AACD,WAAKmH,OAAL,GAAesC,EAAf;AACA,aAAO,KAAKtC,OAAZ;AACD,KAXmB;AAYpBC,IAAAA,QAAQ,EAAE,YAAW;AACnB,UAAIjE,CAAC,GAAG,KAAKmD,MAAb,CADmB,CACE;;AACrB,UAAImD,EAAE,GAAG,KAAKtC,OAAd;AACA,UAAIuC,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;AACA+C,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAagK,CAAb,CAAP,CAJmB,CAIK;;AACxB,WAAI,IAAI1J,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,YAAGyJ,EAAE,CAACjJ,CAAH,CAAKR,CAAL,KAAW,CAAd,EAAiBmD,CAAC,CAACd,EAAF,CAAKrC,CAAL,IAAU,CAAV,CAAjB,CAA8B;AAA9B,aACKmD,CAAC,CAACd,EAAF,CAAKrC,CAAL,IAAUyJ,EAAE,CAACpH,EAAH,CAAMrC,CAAN,CAAV;AACN;AACF,KArBmB;AAsBpBwH,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAxBmB;AAyBpBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOxC,IAAP;AACD,KAhCmB;AAiCpBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACD;AAtCmB,GAAtB,CAhBgB,CAyDhB;AACA;AACA;;AACA,MAAI4D,YAAY,GAAG,UAASlI,GAAT,EAAc;AAC/B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD+B,CAG/B;;AACA,SAAKoE,MAAL,GAAcpE,GAAG,CAAC8D,KAAlB;AACA,SAAKO,MAAL,GAAcrE,GAAG,CAAC+D,KAAlB;AACA,SAAKJ,SAAL,GAAiB3D,GAAG,CAAC6D,QAArB;AACA,SAAKS,UAAL,GAAkB,SAAlB;AACD,GARD;;AASA4D,EAAAA,YAAY,CAAC1H,SAAb,GAAyB;AACvBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,UAAIsG,EAAE,GAAGtG,CAAC,CAACF,YAAF,EAAT;AACA,UAAIyG,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;AACA,UAAIuJ,GAAG,GAAGF,EAAE,CAACjJ,CAAb;AACA,UAAIuH,EAAE,GAAG5E,CAAC,CAAC3C,CAAX;;AACA,WAAI,IAAIR,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB2J,QAAAA,GAAG,CAAC3J,CAAD,CAAH,GAAS,OAAK,MAAIrB,IAAI,CAACkK,GAAL,CAAS,CAACd,EAAE,CAAC/H,CAAD,CAAZ,CAAT,CAAT;AACD;;AACD,WAAKmH,OAAL,GAAesC,EAAf;AACA,aAAO,KAAKtC,OAAZ;AACD,KAZsB;AAavBC,IAAAA,QAAQ,EAAE,YAAW;AACnB,UAAIjE,CAAC,GAAG,KAAKmD,MAAb,CADmB,CACE;;AACrB,UAAImD,EAAE,GAAG,KAAKtC,OAAd;AACA,UAAIuC,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;AACA+C,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAagK,CAAb,CAAP,CAJmB,CAIK;;AACxB,WAAI,IAAI1J,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,YAAI6J,IAAI,GAAGJ,EAAE,CAACjJ,CAAH,CAAKR,CAAL,CAAX;AACAmD,QAAAA,CAAC,CAACd,EAAF,CAAKrC,CAAL,IAAW6J,IAAI,IAAI,MAAMA,IAAV,CAAJ,GAAsBJ,EAAE,CAACpH,EAAH,CAAMrC,CAAN,CAAjC;AACD;AACF,KAtBsB;AAuBvBwH,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAzBsB;AA0BvBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOxC,IAAP;AACD,KAjCsB;AAkCvBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACD;AAvCsB,GAAzB,CArEgB,CA+GhB;AACA;AACA;AACA;;AACA,MAAI8D,WAAW,GAAG,UAASpI,GAAT,EAAc;AAC9B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD8B,CAG9B;;AACA,SAAKqI,UAAL,GAAkB,OAAOrI,GAAG,CAACqI,UAAX,KAA0B,WAA1B,GAAwCrI,GAAG,CAACqI,UAA5C,GAAyD,CAA3E,CAJ8B,CAM9B;;AACA,SAAKjE,MAAL,GAAcpE,GAAG,CAAC8D,KAAlB;AACA,SAAKO,MAAL,GAAcrE,GAAG,CAAC+D,KAAlB;AACA,SAAKJ,SAAL,GAAiB1G,IAAI,CAACW,KAAL,CAAWoC,GAAG,CAAC6D,QAAJ,GAAe,KAAKwE,UAA/B,CAAjB;AACA,SAAK/D,UAAL,GAAkB,QAAlB;AAEA,SAAKgE,QAAL,GAAgB1L,MAAM,CAACoB,KAAP,CAAa,KAAKoG,MAAL,GAAY,KAAKC,MAAjB,GAAwB,KAAKV,SAA1C,CAAhB,CAZ8B,CAYwC;AACvE,GAbD;;AAcAyE,EAAAA,WAAW,CAAC5H,SAAZ,GAAwB;AACtBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,UAAIuG,CAAC,GAAG,KAAKrE,SAAb;AACA,UAAIoE,EAAE,GAAG,IAAI5H,GAAJ,CAAQ,KAAKiE,MAAb,EAAqB,KAAKC,MAA1B,EAAkC,KAAKV,SAAvC,EAAkD,GAAlD,CAAT,CAHgC,CAKhC;AACA;AACA;;AACA,UAAG,KAAKS,MAAL,KAAgB,CAAhB,IAAqB,KAAKC,MAAL,KAAgB,CAAxC,EAA2C;AACzC,aAAI,IAAI/F,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,cAAI2C,EAAE,GAAG3C,CAAC,GAAG,KAAK+J,UAAlB,CADmB,CACW;;AAC9B,cAAI5K,CAAC,GAAGgE,CAAC,CAAC3C,CAAF,CAAImC,EAAJ,CAAR;AACA,cAAIsH,EAAE,GAAG,CAAT;;AACA,eAAI,IAAIlJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKgJ,UAAnB,EAA8BhJ,CAAC,EAA/B,EAAmC;AACjC,gBAAImJ,EAAE,GAAG/G,CAAC,CAAC3C,CAAF,CAAImC,EAAE,GAAC5B,CAAP,CAAT;;AACA,gBAAGmJ,EAAE,GAAG/K,CAAR,EAAW;AACTA,cAAAA,CAAC,GAAG+K,EAAJ;AACAD,cAAAA,EAAE,GAAGlJ,CAAL;AACD;AACF;;AACD0I,UAAAA,EAAE,CAACjJ,CAAH,CAAKR,CAAL,IAAUb,CAAV;AACA,eAAK6K,QAAL,CAAchK,CAAd,IAAmB2C,EAAE,GAAGsH,EAAxB;AACD;AACF,OAfD,MAeO;AACL,YAAItK,CAAC,GAAC,CAAN,CADK,CACI;;AACT,aAAI,IAAI6C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACW,CAAC,CAACrB,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,eAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACU,CAAC,CAACpB,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,iBAAI,IAAIzC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,kBAAI2C,EAAE,GAAG3C,CAAC,GAAG,KAAK+J,UAAlB;AACA,kBAAI5K,CAAC,GAAGgE,CAAC,CAACZ,GAAF,CAAMC,CAAN,EAASC,CAAT,EAAYE,EAAZ,CAAR;AACA,kBAAIsH,EAAE,GAAG,CAAT;;AACA,mBAAI,IAAIlJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKgJ,UAAnB,EAA8BhJ,CAAC,EAA/B,EAAmC;AACjC,oBAAImJ,EAAE,GAAG/G,CAAC,CAACZ,GAAF,CAAMC,CAAN,EAASC,CAAT,EAAYE,EAAE,GAAC5B,CAAf,CAAT;;AACA,oBAAGmJ,EAAE,GAAG/K,CAAR,EAAW;AACTA,kBAAAA,CAAC,GAAG+K,EAAJ;AACAD,kBAAAA,EAAE,GAAGlJ,CAAL;AACD;AACF;;AACD0I,cAAAA,EAAE,CAAC7G,GAAH,CAAOJ,CAAP,EAASC,CAAT,EAAWzC,CAAX,EAAab,CAAb;AACA,mBAAK6K,QAAL,CAAcrK,CAAd,IAAmBgD,EAAE,GAAGsH,EAAxB;AACAtK,cAAAA,CAAC;AACF;AACF;AACF;AAEF;;AACD,WAAKwH,OAAL,GAAesC,EAAf;AACA,aAAO,KAAKtC,OAAZ;AACD,KAjDqB;AAkDtBC,IAAAA,QAAQ,EAAE,YAAW;AACnB,UAAIjE,CAAC,GAAG,KAAKmD,MAAb,CADmB,CACE;;AACrB,UAAImD,EAAE,GAAG,KAAKtC,OAAd;AACA,UAAIuC,CAAC,GAAG,KAAKrE,SAAb;AACAlC,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAayD,CAAC,CAAC3C,CAAF,CAAIJ,MAAjB,CAAP,CAJmB,CAIc;AAEjC;;AACA,UAAG,KAAK0F,MAAL,KAAgB,CAAhB,IAAqB,KAAKC,MAAL,KAAgB,CAAxC,EAA2C;AACzC,aAAI,IAAI/F,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,cAAIqH,UAAU,GAAGoC,EAAE,CAACpH,EAAH,CAAMrC,CAAN,CAAjB;AACAmD,UAAAA,CAAC,CAACd,EAAF,CAAK,KAAK2H,QAAL,CAAchK,CAAd,CAAL,IAAyBqH,UAAzB;AACD;AACF,OALD,MAKO;AACL;AACA,YAAI1H,CAAC,GAAC,CAAN,CAFK,CAEI;;AACT,aAAI,IAAI6C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACiH,EAAE,CAAC3H,EAAjB,EAAoBU,CAAC,EAArB,EAAyB;AACvB,eAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACgH,EAAE,CAAC1H,EAAjB,EAAoBU,CAAC,EAArB,EAAyB;AACvB,iBAAI,IAAIzC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,kBAAIqH,UAAU,GAAGoC,EAAE,CAAC3G,QAAH,CAAYN,CAAZ,EAAcC,CAAd,EAAgBzC,CAAhB,CAAjB;AACAmD,cAAAA,CAAC,CAACJ,QAAF,CAAWP,CAAX,EAAaC,CAAb,EAAe,KAAKuH,QAAL,CAAcrK,CAAd,CAAf,EAAgC0H,UAAhC;AACA1H,cAAAA,CAAC;AACF;AACF;AACF;AACF;AACF,KA3EqB;AA4EtB6H,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KA9EqB;AA+EtBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAACuG,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOvG,IAAP;AACD,KAvFqB;AAwFtBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAK+D,UAAL,GAAkBvG,IAAI,CAACuG,UAAvB;AACA,WAAKC,QAAL,GAAgB1L,MAAM,CAACoB,KAAP,CAAa,KAAKqK,UAAlB,CAAhB;AACD;AA/FqB,GAAxB,CAjIgB,CAmOhB;;AACA,WAASI,IAAT,CAAc3H,CAAd,EAAiB;AACf,QAAIC,CAAC,GAAG9D,IAAI,CAACkK,GAAL,CAAS,IAAIrG,CAAb,CAAR;AACA,WAAO,CAACC,CAAC,GAAG,CAAL,KAAWA,CAAC,GAAG,CAAf,CAAP;AACD,GAvOe,CAwOhB;AACA;AACA;;;AACA,MAAI2H,SAAS,GAAG,UAAS1I,GAAT,EAAc;AAC5B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD4B,CAG5B;;AACA,SAAKoE,MAAL,GAAcpE,GAAG,CAAC8D,KAAlB;AACA,SAAKO,MAAL,GAAcrE,GAAG,CAAC+D,KAAlB;AACA,SAAKJ,SAAL,GAAiB3D,GAAG,CAAC6D,QAArB;AACA,SAAKS,UAAL,GAAkB,MAAlB;AACD,GARD;;AASAoE,EAAAA,SAAS,CAAClI,SAAV,GAAsB;AACpBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AACA,UAAIsG,EAAE,GAAGtG,CAAC,CAACF,YAAF,EAAT;AACA,UAAIyG,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;;AACA,WAAI,IAAIJ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnByJ,QAAAA,EAAE,CAACjJ,CAAH,CAAKR,CAAL,IAAUmK,IAAI,CAAChH,CAAC,CAAC3C,CAAF,CAAIR,CAAJ,CAAD,CAAd;AACD;;AACD,WAAKmH,OAAL,GAAesC,EAAf;AACA,aAAO,KAAKtC,OAAZ;AACD,KAVmB;AAWpBC,IAAAA,QAAQ,EAAE,YAAW;AACnB,UAAIjE,CAAC,GAAG,KAAKmD,MAAb,CADmB,CACE;;AACrB,UAAImD,EAAE,GAAG,KAAKtC,OAAd;AACA,UAAIuC,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;AACA+C,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAagK,CAAb,CAAP,CAJmB,CAIK;;AACxB,WAAI,IAAI1J,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,YAAI6J,IAAI,GAAGJ,EAAE,CAACjJ,CAAH,CAAKR,CAAL,CAAX;AACAmD,QAAAA,CAAC,CAACd,EAAF,CAAKrC,CAAL,IAAU,CAAC,MAAM6J,IAAI,GAAGA,IAAd,IAAsBJ,EAAE,CAACpH,EAAH,CAAMrC,CAAN,CAAhC;AACD;AACF,KApBmB;AAqBpBwH,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAvBmB;AAwBpBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOxC,IAAP;AACD,KA/BmB;AAgCpBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACD;AArCmB,GAAtB;AAwCA1H,EAAAA,MAAM,CAAC8L,SAAP,GAAmBA,SAAnB;AACA9L,EAAAA,MAAM,CAACwL,WAAP,GAAqBA,WAArB;AACAxL,EAAAA,MAAM,CAACkL,SAAP,GAAmBA,SAAnB;AACAlL,EAAAA,MAAM,CAACsL,YAAP,GAAsBA,YAAtB;AAED,CAjSD,EAiSGxL,SAjSH;;AAmSA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;AAEtB;AACA;AACA;AACA;AACA;AACA;;AACA,MAAIwI,YAAY,GAAG,UAAS3I,GAAT,EAAc;AAC/B,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CAD+B,CAG/B;;AACA,SAAKoE,MAAL,GAAcpE,GAAG,CAAC8D,KAAlB;AACA,SAAKO,MAAL,GAAcrE,GAAG,CAAC+D,KAAlB;AACA,SAAKJ,SAAL,GAAiB3D,GAAG,CAAC6D,QAArB;AACA,SAAKS,UAAL,GAAkB,SAAlB;AACA,SAAKsE,SAAL,GAAiB,OAAO5I,GAAG,CAAC4I,SAAX,KAAyB,WAAzB,GAAuC5I,GAAG,CAAC4I,SAA3C,GAAuD,GAAxE;AACA,SAAKC,OAAL,GAAejM,MAAM,CAACoB,KAAP,CAAa,KAAKoG,MAAL,GAAY,KAAKC,MAAjB,GAAwB,KAAKV,SAA1C,CAAf;AACD,GAVD;;AAWAgF,EAAAA,YAAY,CAACnI,SAAb,GAAyB;AACvBkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;;AACA,UAAG,OAAOkD,WAAP,KAAsB,WAAzB,EAAsC;AAAEA,QAAAA,WAAW,GAAG,KAAd;AAAsB,OAF9B,CAE+B;;;AAC/D,UAAIoD,EAAE,GAAGtG,CAAC,CAACD,KAAF,EAAT;AACA,UAAIwG,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;;AACA,UAAGiG,WAAH,EAAgB;AACd;AACA,aAAI,IAAIrG,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,cAAGrB,IAAI,CAACC,MAAL,KAAc,KAAK0L,SAAtB,EAAiC;AAAEb,YAAAA,EAAE,CAACjJ,CAAH,CAAKR,CAAL,IAAQ,CAAR;AAAW,iBAAKuK,OAAL,CAAavK,CAAb,IAAkB,IAAlB;AAAyB,WAAvE,CAAwE;AAAxE,eACK;AAAC,iBAAKuK,OAAL,CAAavK,CAAb,IAAkB,KAAlB;AAAyB;AAChC;AACF,OAND,MAMO;AACL;AACA,aAAI,IAAIA,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AAAEyJ,UAAAA,EAAE,CAACjJ,CAAH,CAAKR,CAAL,KAAS,KAAKsK,SAAd;AAA0B;AAClD;;AACD,WAAKnD,OAAL,GAAesC,EAAf;AACA,aAAO,KAAKtC,OAAZ,CAhBgC,CAgBX;AACtB,KAlBsB;AAmBvBC,IAAAA,QAAQ,EAAE,YAAW;AACnB,UAAIjE,CAAC,GAAG,KAAKmD,MAAb,CADmB,CACE;;AACrB,UAAIe,UAAU,GAAG,KAAKF,OAAtB;AACA,UAAIuC,CAAC,GAAGvG,CAAC,CAAC3C,CAAF,CAAIJ,MAAZ;AACA+C,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAagK,CAAb,CAAP,CAJmB,CAIK;;AACxB,WAAI,IAAI1J,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0J,CAAd,EAAgB1J,CAAC,EAAjB,EAAqB;AACnB,YAAG,CAAE,KAAKuK,OAAL,CAAavK,CAAb,CAAL,EAAuB;AACrBmD,UAAAA,CAAC,CAACd,EAAF,CAAKrC,CAAL,IAAUqH,UAAU,CAAChF,EAAX,CAAcrC,CAAd,CAAV,CADqB,CACO;AAC7B;AACF;AACF,KA7BsB;AA8BvBwH,IAAAA,iBAAiB,EAAE,YAAW;AAC5B,aAAO,EAAP;AACD,KAhCsB;AAiCvBjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACAxC,MAAAA,IAAI,CAAC8G,SAAL,GAAiB,KAAKA,SAAtB;AACA,aAAO9G,IAAP;AACD,KAzCsB;AA0CvBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6B,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKS,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKC,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACA,WAAKsE,SAAL,GAAiB9G,IAAI,CAAC8G,SAAtB;AACD;AAhDsB,GAAzB;AAoDAhM,EAAAA,MAAM,CAAC+L,YAAP,GAAsBA,YAAtB;AACD,CA1ED,EA0EGjM,SA1EH;;AA2EA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;AAEtB;AACA;AACA;;AACA,MAAI2I,+BAA+B,GAAG,UAAS9I,GAAT,EAAc;AAClD,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CADkD,CAGlD;;AACA,SAAKF,CAAL,GAASE,GAAG,CAACF,CAAb;AACA,SAAK7B,CAAL,GAAS+B,GAAG,CAAC/B,CAAb;AACA,SAAK8K,KAAL,GAAa/I,GAAG,CAAC+I,KAAjB;AACA,SAAKC,IAAL,GAAYhJ,GAAG,CAACgJ,IAAhB,CAPkD,CASlD;;AACA,SAAK5E,MAAL,GAAcpE,GAAG,CAAC8D,KAAlB;AACA,SAAKO,MAAL,GAAcrE,GAAG,CAAC+D,KAAlB;AACA,SAAKJ,SAAL,GAAiB3D,GAAG,CAAC6D,QAArB;AACA,SAAKS,UAAL,GAAkB,KAAlB,CAbkD,CAelD;;AACA,QAAG,KAAKrG,CAAL,GAAO,CAAP,KAAa,CAAhB,EAAmB;AAAEgL,MAAAA,OAAO,CAAC1L,GAAR,CAAY,uCAAZ;AAAuD;AAC7E,GAjBD;;AAkBAuL,EAAAA,+BAA+B,CAACtI,SAAhC,GAA4C;AAC1CkE,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,WAAKC,MAAL,GAAcnD,CAAd;AAEA,UAAIoD,CAAC,GAAGpD,CAAC,CAACF,YAAF,EAAR;AACA,WAAK2H,QAAL,GAAgBzH,CAAC,CAACF,YAAF,EAAhB;AACA,UAAI4H,EAAE,GAAGlM,IAAI,CAACW,KAAL,CAAW,KAAKK,CAAL,GAAO,CAAlB,CAAT;;AACA,WAAI,IAAI6C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACW,CAAC,CAACrB,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,aAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACU,CAAC,CAACpB,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,eAAI,IAAIzC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACmD,CAAC,CAACnB,KAAhB,EAAsBhC,CAAC,EAAvB,EAA2B;AAEzB,gBAAIiK,EAAE,GAAG9G,CAAC,CAACZ,GAAF,CAAMC,CAAN,EAAQC,CAAR,EAAUzC,CAAV,CAAT,CAFyB,CAIzB;;AACA,gBAAI8K,GAAG,GAAG,GAAV;;AACA,iBAAI,IAAI/J,CAAC,GAACpC,IAAI,CAACoM,GAAL,CAAS,CAAT,EAAW/K,CAAC,GAAC6K,EAAb,CAAV,EAA2B9J,CAAC,IAAEpC,IAAI,CAACqM,GAAL,CAAShL,CAAC,GAAC6K,EAAX,EAAc1H,CAAC,CAACnB,KAAF,GAAQ,CAAtB,CAA9B,EAAuDjB,CAAC,EAAxD,EAA4D;AAC1D,kBAAIkK,EAAE,GAAG9H,CAAC,CAACZ,GAAF,CAAMC,CAAN,EAAQC,CAAR,EAAU1B,CAAV,CAAT;AACA+J,cAAAA,GAAG,IAAIG,EAAE,GAACA,EAAV;AACD;;AACDH,YAAAA,GAAG,IAAI,KAAKL,KAAL,GAAa,KAAK9K,CAAzB;AACAmL,YAAAA,GAAG,IAAI,KAAKtJ,CAAZ;AACA,iBAAKoJ,QAAL,CAAchI,GAAd,CAAkBJ,CAAlB,EAAoBC,CAApB,EAAsBzC,CAAtB,EAAwB8K,GAAxB,EAZyB,CAYK;;AAC9BA,YAAAA,GAAG,GAAGnM,IAAI,CAACuM,GAAL,CAASJ,GAAT,EAAc,KAAKJ,IAAnB,CAAN;AACAnE,YAAAA,CAAC,CAAC3D,GAAF,CAAMJ,CAAN,EAAQC,CAAR,EAAUzC,CAAV,EAAYiK,EAAE,GAACa,GAAf;AACD;AACF;AACF;;AAED,WAAK3D,OAAL,GAAeZ,CAAf;AACA,aAAO,KAAKY,OAAZ,CA5BgC,CA4BX;AACtB,KA9ByC;AA+B1CC,IAAAA,QAAQ,EAAE,YAAW;AACnB;AACA,UAAIjE,CAAC,GAAG,KAAKmD,MAAb,CAFmB,CAEE;;AACrBnD,MAAAA,CAAC,CAACd,EAAF,GAAO/D,MAAM,CAACoB,KAAP,CAAayD,CAAC,CAAC3C,CAAF,CAAIJ,MAAjB,CAAP,CAHmB,CAGc;;AACjC,UAAImG,CAAC,GAAG,KAAKY,OAAb,CAJmB,CAIG;;AAEtB,UAAI0D,EAAE,GAAGlM,IAAI,CAACW,KAAL,CAAW,KAAKK,CAAL,GAAO,CAAlB,CAAT;;AACA,WAAI,IAAI6C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACW,CAAC,CAACrB,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,aAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACU,CAAC,CAACpB,EAAhB,EAAmBU,CAAC,EAApB,EAAwB;AACtB,eAAI,IAAIzC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACmD,CAAC,CAACnB,KAAhB,EAAsBhC,CAAC,EAAvB,EAA2B;AAEzB,gBAAIqH,UAAU,GAAG,KAAKF,OAAL,CAAarE,QAAb,CAAsBN,CAAtB,EAAwBC,CAAxB,EAA0BzC,CAA1B,CAAjB;AACA,gBAAImL,CAAC,GAAG,KAAKP,QAAL,CAAcrI,GAAd,CAAkBC,CAAlB,EAAoBC,CAApB,EAAsBzC,CAAtB,CAAR;AACA,gBAAIoL,EAAE,GAAGzM,IAAI,CAACuM,GAAL,CAASC,CAAT,EAAY,KAAKT,IAAjB,CAAT;AACA,gBAAIW,GAAG,GAAGD,EAAE,GAACA,EAAb,CALyB,CAOzB;;AACA,iBAAI,IAAIrK,CAAC,GAACpC,IAAI,CAACoM,GAAL,CAAS,CAAT,EAAW/K,CAAC,GAAC6K,EAAb,CAAV,EAA2B9J,CAAC,IAAEpC,IAAI,CAACqM,GAAL,CAAShL,CAAC,GAAC6K,EAAX,EAAc1H,CAAC,CAACnB,KAAF,GAAQ,CAAtB,CAA9B,EAAuDjB,CAAC,EAAxD,EAA4D;AAC1D,kBAAIuK,EAAE,GAAGnI,CAAC,CAACZ,GAAF,CAAMC,CAAN,EAAQC,CAAR,EAAU1B,CAAV,CAAT;AACA,kBAAIwK,CAAC,GAAG,CAACD,EAAD,GAAI,KAAKZ,IAAT,GAAc/L,IAAI,CAACuM,GAAL,CAASC,CAAT,EAAW,KAAKT,IAAL,GAAU,CAArB,CAAd,GAAsC,KAAKD,KAA3C,GAAiD,KAAK9K,CAAtD,GAAwD,CAAxD,GAA0D2L,EAAlE;AACA,kBAAGvK,CAAC,KAAGf,CAAP,EAAUuL,CAAC,IAAGH,EAAJ;AACVG,cAAAA,CAAC,IAAIF,GAAL;AACAE,cAAAA,CAAC,IAAIlE,UAAL;AACAlE,cAAAA,CAAC,CAACH,QAAF,CAAWR,CAAX,EAAaC,CAAb,EAAe1B,CAAf,EAAiBwK,CAAjB;AACD;AAEF;AACF;AACF;AACF,KA5DyC;AA6D1C/D,IAAAA,iBAAiB,EAAE,YAAW;AAAE,aAAO,EAAP;AAAY,KA7DF;AA8D1CjE,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAAChC,CAAL,GAAS,KAAKA,CAAd;AACAgC,MAAAA,IAAI,CAAC7D,CAAL,GAAS,KAAKA,CAAd;AACA6D,MAAAA,IAAI,CAACiH,KAAL,GAAa,KAAKA,KAAlB,CAJiB,CAIQ;;AACzBjH,MAAAA,IAAI,CAACkH,IAAL,GAAY,KAAKA,IAAjB;AACAlH,MAAAA,IAAI,CAACsC,MAAL,GAAc,KAAKA,MAAnB;AACAtC,MAAAA,IAAI,CAACuC,MAAL,GAAc,KAAKA,MAAnB;AACAvC,MAAAA,IAAI,CAAC6B,SAAL,GAAiB,KAAKA,SAAtB;AACA7B,MAAAA,IAAI,CAACwC,UAAL,GAAkB,KAAKA,UAAvB;AACA,aAAOxC,IAAP;AACD,KAzEyC;AA0E1CC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAKhC,CAAL,GAASgC,IAAI,CAAChC,CAAd;AACA,WAAK7B,CAAL,GAAS6D,IAAI,CAAC7D,CAAd;AACA,WAAK8K,KAAL,GAAajH,IAAI,CAACiH,KAAlB,CAHuB,CAGE;;AACzB,WAAKC,IAAL,GAAYlH,IAAI,CAACkH,IAAjB;AACA,WAAK5E,MAAL,GAActC,IAAI,CAACsC,MAAnB;AACA,WAAKC,MAAL,GAAcvC,IAAI,CAACuC,MAAnB;AACA,WAAKV,SAAL,GAAiB7B,IAAI,CAAC6B,SAAtB;AACA,WAAKW,UAAL,GAAkBxC,IAAI,CAACwC,UAAvB;AACD;AAnFyC,GAA5C;AAuFA1H,EAAAA,MAAM,CAACkM,+BAAP,GAAyCA,+BAAzC;AACD,CAjHD,EAiHGpM,SAjHH;;AAkHA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;AAEtB;AACA;;AACA,MAAI2J,GAAG,GAAG,UAASC,OAAT,EAAkB;AAC1B,SAAKC,MAAL,GAAc,EAAd;AACD,GAFD;;AAIAF,EAAAA,GAAG,CAACtJ,SAAJ,GAAgB;AAEd;AACAyJ,IAAAA,UAAU,EAAE,UAASC,IAAT,EAAe;AAEzB;AACA,UAAGA,IAAI,CAACxL,MAAL,GAAY,CAAf,EAAkB;AAACuK,QAAAA,OAAO,CAAC1L,GAAR,CAAY,wDAAZ;AAAuE;;AAC1F,UAAG2M,IAAI,CAAC,CAAD,CAAJ,CAAQC,IAAR,KAAiB,OAApB,EAA6B;AAAClB,QAAAA,OAAO,CAAC1L,GAAR,CAAY,6CAAZ;AAA4D,OAJjE,CAMzB;;;AACA,UAAI6M,OAAO,GAAG,YAAW;AACvB,YAAIC,QAAQ,GAAG,EAAf;;AACA,aAAI,IAAI/L,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC4L,IAAI,CAACxL,MAAnB,EAA0BJ,CAAC,EAA3B,EAA+B;AAC7B,cAAIgM,GAAG,GAAGJ,IAAI,CAAC5L,CAAD,CAAd;;AAEA,cAAGgM,GAAG,CAACH,IAAJ,KAAW,SAAX,IAAwBG,GAAG,CAACH,IAAJ,KAAW,KAAtC,EAA6C;AAC3C;AACA;AACAE,YAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,cAAAA,IAAI,EAAC,IAAN;AAAYhE,cAAAA,WAAW,EAAEmE,GAAG,CAACC;AAA7B,aAAd;AACD;;AAED,cAAGD,GAAG,CAACH,IAAJ,KAAW,YAAd,EAA4B;AAC1B;AACA;AACAE,YAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,cAAAA,IAAI,EAAC,IAAN;AAAYhE,cAAAA,WAAW,EAAEmE,GAAG,CAACnE;AAA7B,aAAd;AACD;;AAED,cAAG,CAACmE,GAAG,CAACH,IAAJ,KAAW,IAAX,IAAmBG,GAAG,CAACH,IAAJ,KAAW,MAA/B,KACI,OAAOG,GAAG,CAAC9F,SAAX,KAA0B,WADjC,EAC6C;AAC3C8F,YAAAA,GAAG,CAAC9F,SAAJ,GAAgB,GAAhB;;AACA,gBAAG,OAAO8F,GAAG,CAACE,UAAX,KAA0B,WAA1B,IAAyCF,GAAG,CAACE,UAAJ,KAAmB,MAA/D,EAAuE;AACrEF,cAAAA,GAAG,CAAC9F,SAAJ,GAAgB,GAAhB,CADqE,CAChD;AACrB;AACA;AACD;AACF;;AAED,cAAG,OAAO8F,GAAG,CAACG,MAAX,KAAsB,WAAzB,EAAsC;AACpC;AACA;AACA,gBAAGH,GAAG,CAACG,MAAP,EAAe;AACbJ,cAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,gBAAAA,IAAI,EAAE;AAAP,eAAd;AACD;AACF;;AAEDE,UAAAA,QAAQ,CAACzL,IAAT,CAAc0L,GAAd;;AAEA,cAAG,OAAOA,GAAG,CAACE,UAAX,KAA0B,WAA7B,EAA0C;AACxC,gBAAGF,GAAG,CAACE,UAAJ,KAAiB,MAApB,EAA4B;AAAEH,cAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,gBAAAA,IAAI,EAAC;AAAN,eAAd;AAA+B,aAA7D,MACK,IAAIG,GAAG,CAACE,UAAJ,KAAiB,SAArB,EAAgC;AAAEH,cAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,gBAAAA,IAAI,EAAC;AAAN,eAAd;AAAkC,aAApE,MACA,IAAIG,GAAG,CAACE,UAAJ,KAAiB,MAArB,EAA6B;AAAEH,cAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,gBAAAA,IAAI,EAAC;AAAN,eAAd;AAA+B,aAA9D,MACA,IAAIG,GAAG,CAACE,UAAJ,KAAiB,QAArB,EAA+B;AAClC;AACA,kBAAIE,EAAE,GAAGJ,GAAG,CAACjC,UAAJ,KAAmB,WAAnB,GAAiCiC,GAAG,CAACjC,UAArC,GAAkD,CAA3D;AACAgC,cAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,gBAAAA,IAAI,EAAC,QAAN;AAAgB9B,gBAAAA,UAAU,EAACqC;AAA3B,eAAd;AACD,aAJI,MAKA;AAAEzB,cAAAA,OAAO,CAAC1L,GAAR,CAAY,kCAAkC+M,GAAG,CAACE,UAAlD;AAAgE;AACxE;;AACD,cAAG,OAAOF,GAAG,CAAC1B,SAAX,KAAyB,WAAzB,IAAwC0B,GAAG,CAACH,IAAJ,KAAa,SAAxD,EAAmE;AACjEE,YAAAA,QAAQ,CAACzL,IAAT,CAAc;AAACuL,cAAAA,IAAI,EAAC,SAAN;AAAiBvB,cAAAA,SAAS,EAAE0B,GAAG,CAAC1B;AAAhC,aAAd;AACD;AAEF;;AACD,eAAOyB,QAAP;AACD,OAtDD;;AAuDAH,MAAAA,IAAI,GAAGE,OAAO,CAACF,IAAD,CAAd,CA9DyB,CAgEzB;;AACA,WAAKF,MAAL,GAAc,EAAd;;AACA,WAAI,IAAI1L,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC4L,IAAI,CAACxL,MAAnB,EAA0BJ,CAAC,EAA3B,EAA+B;AAC7B,YAAIgM,GAAG,GAAGJ,IAAI,CAAC5L,CAAD,CAAd;;AACA,YAAGA,CAAC,GAAC,CAAL,EAAQ;AACN,cAAIqM,IAAI,GAAG,KAAKX,MAAL,CAAY1L,CAAC,GAAC,CAAd,CAAX;AACAgM,UAAAA,GAAG,CAACxG,KAAJ,GAAY6G,IAAI,CAACvG,MAAjB;AACAkG,UAAAA,GAAG,CAACvG,KAAJ,GAAY4G,IAAI,CAACtG,MAAjB;AACAiG,UAAAA,GAAG,CAACzG,QAAJ,GAAe8G,IAAI,CAAChH,SAApB;AACD;;AAED,gBAAO2G,GAAG,CAACH,IAAX;AACE,eAAK,IAAL;AAAW,iBAAKH,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACsJ,cAAX,CAA0BoE,GAA1B,CAAjB;AAAkD;;AAC7D,eAAK,KAAL;AAAY,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACkM,+BAAX,CAA2CwB,GAA3C,CAAjB;AAAmE;;AAC/E,eAAK,SAAL;AAAgB,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAAC+L,YAAX,CAAwB2B,GAAxB,CAAjB;AAAgD;;AAChE,eAAK,OAAL;AAAc,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACiK,UAAX,CAAsByD,GAAtB,CAAjB;AAA8C;;AAC5D,eAAK,SAAL;AAAgB,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACkK,YAAX,CAAwBwD,GAAxB,CAAjB;AAAgD;;AAChE,eAAK,YAAL;AAAmB,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAAC0K,eAAX,CAA2BgD,GAA3B,CAAjB;AAAmD;;AACtE,eAAK,MAAL;AAAa,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAAC8G,SAAX,CAAqB4G,GAArB,CAAjB;AAA6C;;AAC1D,eAAK,MAAL;AAAa,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAAC4J,SAAX,CAAqB8D,GAArB,CAAjB;AAA6C;;AAC1D,eAAK,MAAL;AAAa,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACkL,SAAX,CAAqBwC,GAArB,CAAjB;AAA6C;;AAC1D,eAAK,SAAL;AAAgB,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACsL,YAAX,CAAwBoC,GAAxB,CAAjB;AAAgD;;AAChE,eAAK,MAAL;AAAa,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAAC8L,SAAX,CAAqB4B,GAArB,CAAjB;AAA6C;;AAC1D,eAAK,QAAL;AAAe,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACwL,WAAX,CAAuBkC,GAAvB,CAAjB;AAA+C;;AAC9D,eAAK,eAAL;AAAsB,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAACgO,kBAAX,CAA8BN,GAA9B,CAAjB;AAAsD;;AAC5E,eAAK,KAAL;AAAY,iBAAKN,MAAL,CAAYpL,IAAZ,CAAiB,IAAIhC,MAAM,CAAC+K,QAAX,CAAoB2C,GAApB,CAAjB;AAA4C;;AACxD;AAASrB,YAAAA,OAAO,CAAC1L,GAAR,CAAY,iCAAZ;AAfX;AAiBD;AACF,KAhGa;AAkGd;AACAmH,IAAAA,OAAO,EAAE,UAASjD,CAAT,EAAYkD,WAAZ,EAAyB;AAChC,UAAG,OAAOA,WAAP,KAAsB,WAAzB,EAAsCA,WAAW,GAAG,KAAd;AACtC,UAAIkG,GAAG,GAAG,KAAKb,MAAL,CAAY,CAAZ,EAAetF,OAAf,CAAuBjD,CAAvB,EAA0BkD,WAA1B,CAAV;;AACA,WAAI,IAAIrG,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK0L,MAAL,CAAYtL,MAA1B,EAAiCJ,CAAC,EAAlC,EAAsC;AACpCuM,QAAAA,GAAG,GAAG,KAAKb,MAAL,CAAY1L,CAAZ,EAAeoG,OAAf,CAAuBmG,GAAvB,EAA4BlG,WAA5B,CAAN;AACD;;AACD,aAAOkG,GAAP;AACD,KA1Ga;AA4GdC,IAAAA,WAAW,EAAE,UAASrJ,CAAT,EAAYV,CAAZ,EAAe;AAC1B,WAAK2D,OAAL,CAAajD,CAAb,EAAgB,KAAhB;AACA,UAAIuG,CAAC,GAAG,KAAKgC,MAAL,CAAYtL,MAApB;AACA,UAAI6I,IAAI,GAAG,KAAKyC,MAAL,CAAYhC,CAAC,GAAC,CAAd,EAAiBtC,QAAjB,CAA0B3E,CAA1B,CAAX;AACA,aAAOwG,IAAP;AACD,KAjHa;AAmHd;AACA7B,IAAAA,QAAQ,EAAE,UAAS3E,CAAT,EAAY;AACpB,UAAIiH,CAAC,GAAG,KAAKgC,MAAL,CAAYtL,MAApB;AACA,UAAI6I,IAAI,GAAG,KAAKyC,MAAL,CAAYhC,CAAC,GAAC,CAAd,EAAiBtC,QAAjB,CAA0B3E,CAA1B,CAAX,CAFoB,CAEqB;;AACzC,WAAI,IAAIzC,CAAC,GAAC0J,CAAC,GAAC,CAAZ,EAAc1J,CAAC,IAAE,CAAjB,EAAmBA,CAAC,EAApB,EAAwB;AAAE;AACxB,aAAK0L,MAAL,CAAY1L,CAAZ,EAAeoH,QAAf;AACD;;AACD,aAAO6B,IAAP;AACD,KA3Ha;AA4HdzB,IAAAA,iBAAiB,EAAE,YAAW;AAC5B;AACA,UAAIC,QAAQ,GAAG,EAAf;;AACA,WAAI,IAAIzH,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK0L,MAAL,CAAYtL,MAA1B,EAAiCJ,CAAC,EAAlC,EAAsC;AACpC,YAAIyM,aAAa,GAAG,KAAKf,MAAL,CAAY1L,CAAZ,EAAewH,iBAAf,EAApB;;AACA,aAAI,IAAIzG,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0L,aAAa,CAACrM,MAA5B,EAAmCW,CAAC,EAApC,EAAwC;AACtC0G,UAAAA,QAAQ,CAACnH,IAAT,CAAcmM,aAAa,CAAC1L,CAAD,CAA3B;AACD;AACF;;AACD,aAAO0G,QAAP;AACD,KAtIa;AAuIdiF,IAAAA,aAAa,EAAE,YAAW;AACxB,UAAIvB,CAAC,GAAG,KAAKO,MAAL,CAAY,KAAKA,MAAL,CAAYtL,MAAZ,GAAmB,CAA/B,CAAR,CADwB,CACmB;;AAC3C,UAAIkB,CAAC,GAAG6J,CAAC,CAAChE,OAAF,CAAU3G,CAAlB;AACA,UAAIC,IAAI,GAAGa,CAAC,CAAC,CAAD,CAAZ;AACA,UAAIX,IAAI,GAAG,CAAX;;AACA,WAAI,IAAIX,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACsB,CAAC,CAAClB,MAAhB,EAAuBJ,CAAC,EAAxB,EAA4B;AAC1B,YAAGsB,CAAC,CAACtB,CAAD,CAAD,GAAOS,IAAV,EAAgB;AAAEA,UAAAA,IAAI,GAAGa,CAAC,CAACtB,CAAD,CAAR;AAAaW,UAAAA,IAAI,GAAGX,CAAP;AAAU;AAC1C;;AACD,aAAOW,IAAP;AACD,KAhJa;AAiJd4C,IAAAA,MAAM,EAAE,YAAW;AACjB,UAAIC,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAACkI,MAAL,GAAc,EAAd;;AACA,WAAI,IAAI1L,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK0L,MAAL,CAAYtL,MAA1B,EAAiCJ,CAAC,EAAlC,EAAsC;AACpCwD,QAAAA,IAAI,CAACkI,MAAL,CAAYpL,IAAZ,CAAiB,KAAKoL,MAAL,CAAY1L,CAAZ,EAAeuD,MAAf,EAAjB;AACD;;AACD,aAAOC,IAAP;AACD,KAxJa;AAyJdC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAKkI,MAAL,GAAc,EAAd;;AACA,WAAI,IAAI1L,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACwD,IAAI,CAACkI,MAAL,CAAYtL,MAA1B,EAAiCJ,CAAC,EAAlC,EAAsC;AACpC,YAAI2M,EAAE,GAAGnJ,IAAI,CAACkI,MAAL,CAAY1L,CAAZ,CAAT;AACA,YAAI4M,CAAC,GAAGD,EAAE,CAAC3G,UAAX;AACA,YAAI6G,CAAJ;;AACA,YAAGD,CAAC,KAAG,OAAP,EAAgB;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACiK,UAAX,EAAJ;AAA8B;;AAChD,YAAGqE,CAAC,KAAG,MAAP,EAAe;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACkL,SAAX,EAAJ;AAA6B;;AAC9C,YAAGoD,CAAC,KAAG,SAAP,EAAkB;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACsL,YAAX,EAAJ;AAAgC;;AACpD,YAAGgD,CAAC,KAAG,MAAP,EAAe;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAAC8L,SAAX,EAAJ;AAA6B;;AAC9C,YAAGwC,CAAC,KAAG,SAAP,EAAkB;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAAC+L,YAAX,EAAJ;AAAgC;;AACpD,YAAGuC,CAAC,KAAG,MAAP,EAAe;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAAC8G,SAAX,EAAJ;AAA6B;;AAC9C,YAAGwH,CAAC,KAAG,MAAP,EAAe;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAAC4J,SAAX,EAAJ;AAA6B;;AAC9C,YAAG0E,CAAC,KAAG,KAAP,EAAc;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACkM,+BAAX,EAAJ;AAAmD;;AACnE,YAAGoC,CAAC,KAAG,SAAP,EAAkB;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACkK,YAAX,EAAJ;AAAgC;;AACpD,YAAGoE,CAAC,KAAG,YAAP,EAAqB;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAAC0K,eAAX,EAAJ;AAAmC;;AAC1D,YAAG4D,CAAC,KAAG,IAAP,EAAa;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACsJ,cAAX,EAAJ;AAAkC;;AACjD,YAAGgF,CAAC,KAAG,QAAP,EAAiB;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACwL,WAAX,EAAJ;AAA+B;;AAClD,YAAG8C,CAAC,KAAG,eAAP,EAAwB;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAACgO,kBAAX,EAAJ;AAAsC;;AAChE,YAAGM,CAAC,KAAG,KAAP,EAAc;AAAEC,UAAAA,CAAC,GAAG,IAAIvO,MAAM,CAAC+K,QAAX,EAAJ;AAA4B;;AAC5CwD,QAAAA,CAAC,CAACpJ,QAAF,CAAWkJ,EAAX;AACA,aAAKjB,MAAL,CAAYpL,IAAZ,CAAiBuM,CAAjB;AACD;AACF;AAhLa,GAAhB;AAoLAvO,EAAAA,MAAM,CAACkN,GAAP,GAAaA,GAAb;AACD,CA/LD,EA+LGpN,SA/LH;;AAgMA,CAAC,UAASE,MAAT,EAAiB;AAChB;;AACA,MAAIuD,GAAG,GAAGvD,MAAM,CAACuD,GAAjB,CAFgB,CAEM;;AAEtB,MAAIiL,OAAO,GAAG,UAASC,GAAT,EAActB,OAAd,EAAuB;AAEnC,SAAKsB,GAAL,GAAWA,GAAX;AAEA,QAAItB,OAAO,GAAGA,OAAO,IAAI,EAAzB;AACA,SAAKuB,aAAL,GAAqB,OAAOvB,OAAO,CAACuB,aAAf,KAAiC,WAAjC,GAA+CvB,OAAO,CAACuB,aAAvD,GAAuE,IAA5F;AACA,SAAKC,QAAL,GAAgB,OAAOxB,OAAO,CAACwB,QAAf,KAA4B,WAA5B,GAA0CxB,OAAO,CAACwB,QAAlD,GAA6D,GAA7E;AACA,SAAKC,QAAL,GAAgB,OAAOzB,OAAO,CAACyB,QAAf,KAA4B,WAA5B,GAA0CzB,OAAO,CAACyB,QAAlD,GAA6D,GAA7E;AACA,SAAKC,UAAL,GAAkB,OAAO1B,OAAO,CAAC0B,UAAf,KAA8B,WAA9B,GAA4C1B,OAAO,CAAC0B,UAApD,GAAiE,CAAnF;AACA,SAAKC,MAAL,GAAc,OAAO3B,OAAO,CAAC2B,MAAf,KAA0B,WAA1B,GAAwC3B,OAAO,CAAC2B,MAAhD,GAAyD,KAAvE,CATmC,CAS2C;;AAE9E,SAAKC,QAAL,GAAgB,OAAO5B,OAAO,CAAC4B,QAAf,KAA4B,WAA5B,GAA0C5B,OAAO,CAAC4B,QAAlD,GAA6D,GAA7E;AACA,SAAKC,EAAL,GAAU,OAAO7B,OAAO,CAAC6B,EAAf,KAAsB,WAAtB,GAAoC7B,OAAO,CAAC6B,EAA5C,GAAiD,IAA3D,CAZmC,CAY8B;;AACjE,SAAKC,GAAL,GAAW,OAAO9B,OAAO,CAAC8B,GAAf,KAAuB,WAAvB,GAAqC9B,OAAO,CAAC8B,GAA7C,GAAmD,IAA9D,CAbmC,CAaiC;;AAEpE,SAAK/L,CAAL,GAAS,CAAT,CAfmC,CAevB;;AACZ,SAAKgM,IAAL,GAAY,EAAZ,CAhBmC,CAgBnB;;AAChB,SAAKC,IAAL,GAAY,EAAZ,CAjBmC,CAiBnB;AACjB,GAlBD;;AAoBAX,EAAAA,OAAO,CAAC5K,SAAR,GAAoB;AAClBwL,IAAAA,KAAK,EAAE,UAASlL,CAAT,EAAYC,CAAZ,EAAe;AAEpB,UAAIkL,KAAK,GAAG,IAAIC,IAAJ,GAAWC,OAAX,EAAZ;AACA,WAAKd,GAAL,CAAS3G,OAAT,CAAiB5D,CAAjB,EAAoB,IAApB,EAHoB,CAGO;;AAC3B,UAAIsL,GAAG,GAAG,IAAIF,IAAJ,GAAWC,OAAX,EAAV;AACA,UAAIE,QAAQ,GAAGD,GAAG,GAAGH,KAArB;AAEA,UAAIA,KAAK,GAAG,IAAIC,IAAJ,GAAWC,OAAX,EAAZ;AACA,UAAIG,SAAS,GAAG,KAAKjB,GAAL,CAAS3F,QAAT,CAAkB3E,CAAlB,CAAhB;AACA,UAAIwL,aAAa,GAAG,GAApB;AACA,UAAIC,aAAa,GAAG,GAApB;AACA,UAAIJ,GAAG,GAAG,IAAIF,IAAJ,GAAWC,OAAX,EAAV;AACA,UAAIM,QAAQ,GAAGL,GAAG,GAAGH,KAArB;AAEA,WAAKnM,CAAL;;AACA,UAAG,KAAKA,CAAL,GAAS,KAAK2L,UAAd,KAA6B,CAAhC,EAAmC;AAEjC,YAAIiB,MAAM,GAAG,KAAKrB,GAAL,CAASvF,iBAAT,EAAb,CAFiC,CAIjC;;AACA,YAAG,KAAKgG,IAAL,CAAUpN,MAAV,KAAqB,CAArB,KAA2B,KAAKgN,MAAL,KAAgB,KAAhB,IAAyB,KAAKC,QAAL,GAAgB,GAApE,CAAH,EAA6E;AAC3E;AACA;AACA;AACA;AACA,eAAI,IAAIrN,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACoO,MAAM,CAAChO,MAArB,EAA4BJ,CAAC,EAA7B,EAAiC;AAC/B,iBAAKwN,IAAL,CAAUlN,IAAV,CAAehC,MAAM,CAACoB,KAAP,CAAa0O,MAAM,CAACpO,CAAD,CAAN,CAAU0H,MAAV,CAAiBtH,MAA9B,CAAf;;AACA,gBAAG,KAAKgN,MAAL,KAAgB,UAAnB,EAA+B;AAC7B,mBAAKK,IAAL,CAAUnN,IAAV,CAAehC,MAAM,CAACoB,KAAP,CAAa0O,MAAM,CAACpO,CAAD,CAAN,CAAU0H,MAAV,CAAiBtH,MAA9B,CAAf;AACD,aAFD,MAEO;AACL,mBAAKqN,IAAL,CAAUnN,IAAV,CAAe,EAAf,EADK,CACe;AACrB;AACF;AACF,SAlBgC,CAoBjC;;;AACA,aAAI,IAAIN,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACoO,MAAM,CAAChO,MAArB,EAA4BJ,CAAC,EAA7B,EAAiC;AAC/B,cAAIqO,EAAE,GAAGD,MAAM,CAACpO,CAAD,CAAf,CAD+B,CACX;;AACpB,cAAIsB,CAAC,GAAG+M,EAAE,CAAC3G,MAAX;AACA,cAAI6D,CAAC,GAAG8C,EAAE,CAAC1G,KAAX,CAH+B,CAK/B;;AACA,cAAI9B,YAAY,GAAG,OAAOwI,EAAE,CAACxI,YAAV,KAA2B,WAA3B,GAAyCwI,EAAE,CAACxI,YAA5C,GAA2D,GAA9E;AACA,cAAID,YAAY,GAAG,OAAOyI,EAAE,CAACzI,YAAV,KAA2B,WAA3B,GAAyCyI,EAAE,CAACzI,YAA5C,GAA2D,GAA9E;AACA,cAAIsH,QAAQ,GAAG,KAAKA,QAAL,GAAgBrH,YAA/B;AACA,cAAIoH,QAAQ,GAAG,KAAKA,QAAL,GAAgBrH,YAA/B;AAEA,cAAI0I,IAAI,GAAGhN,CAAC,CAAClB,MAAb;;AACA,eAAI,IAAIW,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACuN,IAAd,EAAmBvN,CAAC,EAApB,EAAwB;AACtBkN,YAAAA,aAAa,IAAIf,QAAQ,GAAC5L,CAAC,CAACP,CAAD,CAAV,GAAcO,CAAC,CAACP,CAAD,CAAf,GAAmB,CAApC,CADsB,CACiB;;AACvCmN,YAAAA,aAAa,IAAIjB,QAAQ,GAACtO,IAAI,CAAC4P,GAAL,CAASjN,CAAC,CAACP,CAAD,CAAV,CAA1B;AACA,gBAAIyN,MAAM,GAAGvB,QAAQ,IAAI3L,CAAC,CAACP,CAAD,CAAD,GAAO,CAAP,GAAW,CAAX,GAAe,CAAC,CAApB,CAArB;AACA,gBAAI0N,MAAM,GAAGvB,QAAQ,GAAI5L,CAAC,CAACP,CAAD,CAA1B;AAEA,gBAAI2N,GAAG,GAAG,CAACD,MAAM,GAAGD,MAAT,GAAkBjD,CAAC,CAACxK,CAAD,CAApB,IAA2B,KAAKoM,UAA1C,CANsB,CAMgC;;AAEtD,gBAAIwB,KAAK,GAAG,KAAKnB,IAAL,CAAUxN,CAAV,CAAZ;AACA,gBAAI4O,KAAK,GAAG,KAAKnB,IAAL,CAAUzN,CAAV,CAAZ;;AACA,gBAAG,KAAKoN,MAAL,KAAgB,SAAnB,EAA8B;AAC5B;AACAuB,cAAAA,KAAK,CAAC5N,CAAD,CAAL,GAAW4N,KAAK,CAAC5N,CAAD,CAAL,GAAW2N,GAAG,GAAGA,GAA5B;AACA,kBAAI9K,EAAE,GAAG,CAAE,KAAKoJ,aAAP,GAAuBrO,IAAI,CAACK,IAAL,CAAU2P,KAAK,CAAC5N,CAAD,CAAL,GAAW,KAAKwM,GAA1B,CAAvB,GAAwDmB,GAAjE;AACApN,cAAAA,CAAC,CAACP,CAAD,CAAD,IAAQ6C,EAAR;AACD,aALD,MAKO,IAAG,KAAKwJ,MAAL,KAAgB,YAAnB,EAAiC;AACtC;AACA;AACA;AACAuB,cAAAA,KAAK,CAAC5N,CAAD,CAAL,GAAW,KAAKuM,EAAL,GAAUqB,KAAK,CAAC5N,CAAD,CAAf,GAAqB,CAAC,IAAE,KAAKuM,EAAR,IAAcoB,GAAd,GAAoBA,GAApD;AACA,kBAAI9K,EAAE,GAAG,CAAE,KAAKoJ,aAAP,GAAuBrO,IAAI,CAACK,IAAL,CAAU2P,KAAK,CAAC5N,CAAD,CAAL,GAAW,KAAKwM,GAA1B,CAAvB,GAAwDmB,GAAjE,CALsC,CAKgC;;AACtEpN,cAAAA,CAAC,CAACP,CAAD,CAAD,IAAQ6C,EAAR;AACD,aAPM,MAOA,IAAG,KAAKwJ,MAAL,KAAgB,UAAnB,EAA+B;AACpC;AACAuB,cAAAA,KAAK,CAAC5N,CAAD,CAAL,GAAW,KAAKuM,EAAL,GAAUqB,KAAK,CAAC5N,CAAD,CAAf,GAAqB,CAAC,IAAE,KAAKuM,EAAR,IAAcoB,GAAd,GAAoBA,GAApD;AACA,kBAAI9K,EAAE,GAAG,CAAEjF,IAAI,CAACK,IAAL,CAAU,CAAC4P,KAAK,CAAC7N,CAAD,CAAL,GAAW,KAAKwM,GAAjB,KAAuBoB,KAAK,CAAC5N,CAAD,CAAL,GAAW,KAAKwM,GAAvC,CAAV,CAAF,GAA2DmB,GAApE;AACAE,cAAAA,KAAK,CAAC7N,CAAD,CAAL,GAAW,KAAKuM,EAAL,GAAUsB,KAAK,CAAC7N,CAAD,CAAf,GAAqB,CAAC,IAAE,KAAKuM,EAAR,IAAc1J,EAAd,GAAmBA,EAAnD,CAJoC,CAImB;;AACvDtC,cAAAA,CAAC,CAACP,CAAD,CAAD,IAAQ6C,EAAR;AACD,aANM,MAMA;AACL;AACA,kBAAG,KAAKyJ,QAAL,GAAgB,GAAnB,EAAwB;AACtB;AACA,oBAAIzJ,EAAE,GAAG,KAAKyJ,QAAL,GAAgBsB,KAAK,CAAC5N,CAAD,CAArB,GAA2B,KAAKiM,aAAL,GAAqB0B,GAAzD,CAFsB,CAEwC;;AAC9DC,gBAAAA,KAAK,CAAC5N,CAAD,CAAL,GAAW6C,EAAX,CAHsB,CAGP;;AACftC,gBAAAA,CAAC,CAACP,CAAD,CAAD,IAAQ6C,EAAR,CAJsB,CAIV;AACb,eALD,MAKO;AACL;AACAtC,gBAAAA,CAAC,CAACP,CAAD,CAAD,IAAS,CAAE,KAAKiM,aAAP,GAAuB0B,GAAhC;AACD;AACF;;AACDnD,YAAAA,CAAC,CAACxK,CAAD,CAAD,GAAO,GAAP,CAxCsB,CAwCV;AACb;AACF;AACF,OA3FmB,CA6FpB;AACA;AACA;AACA;;;AACA,aAAO;AAACgN,QAAAA,QAAQ,EAAEA,QAAX;AAAqBI,QAAAA,QAAQ,EAAEA,QAA/B;AACCF,QAAAA,aAAa,EAAEA,aADhB;AAC+BC,QAAAA,aAAa,EAAEA,aAD9C;AAECF,QAAAA,SAAS,EAAEA,SAFZ;AAEuBa,QAAAA,YAAY,EAAEb,SAFrC;AAGC/E,QAAAA,IAAI,EAAE+E,SAAS,GAAGE,aAAZ,GAA4BD;AAHnC,OAAP;AAID;AAtGiB,GAApB;AAyGA3P,EAAAA,MAAM,CAACwO,OAAP,GAAiBA,OAAjB;AACAxO,EAAAA,MAAM,CAACwQ,UAAP,GAAoBhC,OAApB,CAlIgB,CAkIa;AAC9B,CAnID,EAmIG1O,SAnIH;;AAqIA,CAAC,UAASE,MAAT,EAAiB;AAChB,eADgB,CAGhB;;AACA,MAAIY,KAAK,GAAGZ,MAAM,CAACY,KAAnB;AACA,MAAIG,KAAK,GAAGf,MAAM,CAACe,KAAnB;AACA,MAAImM,GAAG,GAAGlN,MAAM,CAACkN,GAAjB;AACA,MAAIsB,OAAO,GAAGxO,MAAM,CAACwO,OAArB;AACA,MAAIvM,MAAM,GAAGjC,MAAM,CAACiC,MAApB;AACA,MAAIO,QAAQ,GAAGxC,MAAM,CAACwC,QAAtB;AACA,MAAIK,cAAc,GAAG7C,MAAM,CAAC6C,cAA5B;AACA,MAAIM,MAAM,GAAGnD,MAAM,CAACmD,MAApB;AACA,MAAIpB,SAAS,GAAG/B,MAAM,CAAC+B,SAAvB;AAEA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;;AACE,MAAI0O,QAAQ,GAAG,UAAS/J,IAAT,EAAegK,MAAf,EAAuBtN,GAAvB,EAA4B;AACzC,QAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB;;AACA,QAAG,OAAOsD,IAAP,KAAgB,WAAnB,EAAgC;AAAEA,MAAAA,IAAI,GAAG,EAAP;AAAY;;AAC9C,QAAG,OAAOgK,MAAP,KAAkB,WAArB,EAAkC;AAAEA,MAAAA,MAAM,GAAG,EAAT;AAAc,KAHT,CAKzC;;;AACA,SAAKhK,IAAL,GAAYA,IAAZ,CANyC,CAMvB;;AAClB,SAAKgK,MAAL,GAAcA,MAAd,CAPyC,CASzC;;AACA,SAAKC,WAAL,GAAmBxN,MAAM,CAACC,GAAD,EAAM,aAAN,EAAqB,GAArB,CAAzB;AACA,SAAKwN,SAAL,GAAiBzN,MAAM,CAACC,GAAD,EAAM,WAAN,EAAmB,EAAnB,CAAvB;AACA,SAAKyN,cAAL,GAAsB1N,MAAM,CAACC,GAAD,EAAM,gBAAN,EAAwB,EAAxB,CAA5B,CAZyC,CAYgB;AACzD;AACA;;AACA,SAAK0N,UAAL,GAAkB3N,MAAM,CAACC,GAAD,EAAM,YAAN,EAAoB,EAApB,CAAxB,CAfyC,CAgBzC;;AACA,SAAK2N,aAAL,GAAqB5N,MAAM,CAACC,GAAD,EAAM,eAAN,EAAuB,EAAvB,CAA3B,CAjByC,CAmBzC;;AACA,SAAK4N,cAAL,GAAsB7N,MAAM,CAACC,GAAD,EAAM,gBAAN,EAAwB,EAAxB,CAA5B;AACA,SAAK6N,cAAL,GAAsB9N,MAAM,CAACC,GAAD,EAAM,gBAAN,EAAwB,GAAxB,CAA5B;AACA,SAAK8N,YAAL,GAAoB/N,MAAM,CAACC,GAAD,EAAM,cAAN,EAAsB,CAAC,CAAvB,CAA1B;AACA,SAAK+N,YAAL,GAAoBhO,MAAM,CAACC,GAAD,EAAM,cAAN,EAAsB,CAAtB,CAA1B;AACA,SAAKgO,iBAAL,GAAyBjO,MAAM,CAACC,GAAD,EAAM,mBAAN,EAA2B,CAAC,CAA5B,CAA/B;AACA,SAAKiO,iBAAL,GAAyBlO,MAAM,CAACC,GAAD,EAAM,mBAAN,EAA2B,CAA3B,CAA/B;AACA,SAAKkO,YAAL,GAAoBnO,MAAM,CAACC,GAAD,EAAM,cAAN,EAAsB,GAAtB,CAA1B;AACA,SAAKmO,YAAL,GAAoBpO,MAAM,CAACC,GAAD,EAAM,cAAN,EAAsB,GAAtB,CAA1B;AACA,SAAKoO,WAAL,GAAmBrO,MAAM,CAACC,GAAD,EAAM,aAAN,EAAqB,CAArB,CAAzB;AACA,SAAKqO,WAAL,GAAmBtO,MAAM,CAACC,GAAD,EAAM,aAAN,EAAqB,EAArB,CAAzB,CA7ByC,CA+BzC;;AACA,SAAKsO,KAAL,GAAa,EAAb,CAhCyC,CAgCxB;;AACjB,SAAKC,UAAL,GAAkB,EAAlB,CAjCyC,CAiCnB;;AACtB,SAAKC,oBAAL,GAA4B,EAA5B,CAlCyC,CAkCT;;AAChC,SAAKC,aAAL,GAAqB9P,SAAS,CAAC2O,MAAD,CAA9B;AACA,SAAKoB,IAAL,GAAY,CAAZ,CApCyC,CAoC1B;;AACf,SAAKC,MAAL,GAAc,CAAd,CArCyC,CAqCxB;AAEjB;;AACA,SAAKC,oBAAL,GAA4B,IAA5B;AACA,SAAKC,qBAAL,GAA6B,IAA7B,CAzCyC,CA2CzC;;AACA,QAAG,KAAKvL,IAAL,CAAU5E,MAAV,GAAmB,CAAtB,EAAyB;AACvB,WAAKoQ,WAAL;AACA,WAAKC,gBAAL;AACD;AACF,GAhDD;;AAkDA1B,EAAAA,QAAQ,CAAC7M,SAAT,GAAqB;AAEnB;AACAsO,IAAAA,WAAW,EAAE,YAAW;AACtB,UAAI9G,CAAC,GAAG,KAAK1E,IAAL,CAAU5E,MAAlB;AACA,UAAIsQ,SAAS,GAAG/R,IAAI,CAACW,KAAL,CAAW,KAAK2P,WAAL,GAAmBvF,CAA9B,CAAhB;AACA,WAAKsG,KAAL,GAAa,EAAb,CAHsB,CAGL;;AACjB,WAAI,IAAIhQ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKkP,SAAnB,EAA6BlP,CAAC,EAA9B,EAAkC;AAChC,YAAIsB,CAAC,GAAGR,QAAQ,CAAC4I,CAAD,CAAhB;AACA,aAAKsG,KAAL,CAAW1P,IAAX,CAAgB;AAACqQ,UAAAA,QAAQ,EAAErP,CAAC,CAACsP,KAAF,CAAQ,CAAR,EAAWF,SAAX,CAAX;AAAkCG,UAAAA,OAAO,EAAEvP,CAAC,CAACsP,KAAF,CAAQF,SAAR,EAAmBhH,CAAnB;AAA3C,SAAhB;AACD;AACF,KAXkB;AAanB;AACAoH,IAAAA,eAAe,EAAE,YAAW;AAC1B,UAAIC,WAAW,GAAG,KAAK/L,IAAL,CAAU,CAAV,EAAaxE,CAAb,CAAeJ,MAAjC;AACA,UAAI6L,WAAW,GAAG,KAAKkE,aAAL,CAAmB/P,MAArC,CAF0B,CAI1B;;AACA,UAAI4Q,UAAU,GAAG,EAAjB;AACAA,MAAAA,UAAU,CAAC1Q,IAAX,CAAgB;AAACuL,QAAAA,IAAI,EAAC,OAAN;AAAe/F,QAAAA,MAAM,EAAC,CAAtB;AAAyBC,QAAAA,MAAM,EAAC,CAAhC;AAAmCV,QAAAA,SAAS,EAAE0L;AAA9C,OAAhB;AACA,UAAIE,EAAE,GAAG9P,cAAc,CAAC,CAAC,CAAD,EAAG,CAAH,EAAK,CAAL,EAAO,CAAP,CAAD,EAAY,CAAC,GAAD,EAAM,GAAN,EAAW,GAAX,EAAgB,GAAhB,CAAZ,CAAvB,CAP0B,CAOgC;;AAC1D,WAAI,IAAID,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC+P,EAAd,EAAiB/P,CAAC,EAAlB,EAAsB;AACpB,YAAIgQ,EAAE,GAAG7R,KAAK,CAAC,KAAKyQ,WAAN,EAAmB,KAAKC,WAAxB,CAAd;AACA,YAAIxD,GAAG,GAAG,CAAC,MAAD,EAAQ,QAAR,EAAiB,MAAjB,EAAyBlN,KAAK,CAAC,CAAD,EAAG,CAAH,CAA9B,CAAV;;AACA,YAAGH,KAAK,CAAC,CAAD,EAAG,CAAH,CAAL,GAAW,GAAd,EAAmB;AACjB,cAAIiS,EAAE,GAAGxS,IAAI,CAACC,MAAL,EAAT;AACAoS,UAAAA,UAAU,CAAC1Q,IAAX,CAAgB;AAACuL,YAAAA,IAAI,EAAC,IAAN;AAAYhE,YAAAA,WAAW,EAAEqJ,EAAzB;AAA6BhF,YAAAA,UAAU,EAAEK,GAAzC;AAA8CjC,YAAAA,SAAS,EAAE6G;AAAzD,WAAhB;AACD,SAHD,MAGO;AACLH,UAAAA,UAAU,CAAC1Q,IAAX,CAAgB;AAACuL,YAAAA,IAAI,EAAC,IAAN;AAAYhE,YAAAA,WAAW,EAAEqJ,EAAzB;AAA6BhF,YAAAA,UAAU,EAAEK;AAAzC,WAAhB;AACD;AACF;;AACDyE,MAAAA,UAAU,CAAC1Q,IAAX,CAAgB;AAACuL,QAAAA,IAAI,EAAC,SAAN;AAAiBI,QAAAA,WAAW,EAAEA;AAA9B,OAAhB;AACA,UAAIc,GAAG,GAAG,IAAIvB,GAAJ,EAAV;AACAuB,MAAAA,GAAG,CAACpB,UAAJ,CAAeqF,UAAf,EApB0B,CAsB1B;;AACA,UAAII,EAAE,GAAG/R,KAAK,CAAC,KAAKiQ,cAAN,EAAsB,KAAKC,cAA3B,CAAd,CAvB0B,CAuBgC;;AAC1D,UAAI8B,EAAE,GAAG1S,IAAI,CAACuM,GAAL,CAAS,EAAT,EAAahM,KAAK,CAAC,KAAKsQ,YAAN,EAAoB,KAAKC,YAAzB,CAAlB,CAAT,CAxB0B,CAwB0C;;AACpE,UAAI6B,EAAE,GAAG3S,IAAI,CAACuM,GAAL,CAAS,EAAT,EAAahM,KAAK,CAAC,KAAKwQ,iBAAN,EAAyB,KAAKC,iBAA9B,CAAlB,CAAT,CAzB0B,CAyBoD;;AAC9E,UAAI4B,GAAG,GAAGrS,KAAK,CAAC,KAAK0Q,YAAN,EAAoB,KAAKC,YAAzB,CAAf,CA1B0B,CA0B6B;;AACvD,UAAI2B,EAAE,GAAGtS,KAAK,CAAC,CAAD,EAAG,CAAH,CAAd,CA3B0B,CA2BL;;AACrB,UAAIuS,WAAJ;;AACA,UAAGD,EAAE,GAAC,IAAN,EAAY;AACVC,QAAAA,WAAW,GAAG;AAACrE,UAAAA,MAAM,EAAC,UAAR;AAAoBD,UAAAA,UAAU,EAACiE,EAA/B;AAAmClE,UAAAA,QAAQ,EAACmE;AAA5C,SAAd;AACD,OAFD,MAEO,IAAGG,EAAE,GAAC,IAAN,EAAY;AACjBC,QAAAA,WAAW,GAAG;AAACrE,UAAAA,MAAM,EAAC,SAAR;AAAmBJ,UAAAA,aAAa,EAAEsE,EAAlC;AAAsCnE,UAAAA,UAAU,EAACiE,EAAjD;AAAqDlE,UAAAA,QAAQ,EAACmE;AAA9D,SAAd;AACD,OAFM,MAEA;AACLI,QAAAA,WAAW,GAAG;AAACrE,UAAAA,MAAM,EAAC,KAAR;AAAeJ,UAAAA,aAAa,EAAEsE,EAA9B;AAAkCjE,UAAAA,QAAQ,EAAEkE,GAA5C;AAAiDpE,UAAAA,UAAU,EAACiE,EAA5D;AAAgElE,UAAAA,QAAQ,EAACmE;AAAzE,SAAd;AACD;;AAED,UAAIK,OAAO,GAAG,IAAI5E,OAAJ,CAAYC,GAAZ,EAAiB0E,WAAjB,CAAd;AAEA,UAAIE,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAACC,GAAL,GAAW,EAAX;AACAD,MAAAA,IAAI,CAACE,IAAL,GAAY,CAAZ,CAzC0B,CAyCX;;AACfF,MAAAA,IAAI,CAACX,UAAL,GAAkBA,UAAlB;AACAW,MAAAA,IAAI,CAACF,WAAL,GAAmBA,WAAnB;AACAE,MAAAA,IAAI,CAAC5E,GAAL,GAAWA,GAAX;AACA4E,MAAAA,IAAI,CAACD,OAAL,GAAeA,OAAf;AACA,aAAOC,IAAP;AACD,KA7DkB;AA+DnB;AACAlB,IAAAA,gBAAgB,EAAE,YAAW;AAC3B,WAAKR,UAAL,GAAkB,EAAlB,CAD2B,CACL;;AACtB,WAAI,IAAIjQ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKmP,cAAnB,EAAkCnP,CAAC,EAAnC,EAAuC;AACrC,YAAI2R,IAAI,GAAG,KAAKb,eAAL,EAAX;AACA,aAAKb,UAAL,CAAgB3P,IAAhB,CAAqBqR,IAArB;AACD;AACF,KAtEkB;AAwEnBG,IAAAA,IAAI,EAAE,YAAW;AAEf;AACA,WAAK1B,IAAL,GAHe,CAKf;;AACA,UAAI2B,IAAI,GAAG,KAAK/B,KAAL,CAAW,KAAKK,MAAhB,CAAX,CANe,CAMqB;;AACpC,UAAI2B,MAAM,GAAGD,IAAI,CAACpB,QAAL,CAActR,KAAK,CAAC,CAAD,EAAI0S,IAAI,CAACpB,QAAL,CAAcvQ,MAAlB,CAAnB,CAAb;;AACA,WAAI,IAAIoB,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKyO,UAAL,CAAgB7P,MAA9B,EAAqCoB,CAAC,EAAtC,EAA0C;AACxC,YAAIgB,CAAC,GAAG,KAAKwC,IAAL,CAAUgN,MAAV,CAAR;AACA,YAAIC,CAAC,GAAG,KAAKjD,MAAL,CAAYgD,MAAZ,CAAR;AACA,aAAK/B,UAAL,CAAgBzO,CAAhB,EAAmBkQ,OAAnB,CAA2BhE,KAA3B,CAAiClL,CAAjC,EAAoCyP,CAApC;AACD,OAZc,CAcf;;;AACA,UAAIC,QAAQ,GAAG,KAAK9C,UAAL,GAAkB2C,IAAI,CAACpB,QAAL,CAAcvQ,MAA/C;;AACA,UAAG,KAAKgQ,IAAL,IAAa8B,QAAhB,EAA0B;AACxB;AACA;AACA,YAAIC,OAAO,GAAG,KAAKC,aAAL,EAAd;;AACA,aAAI,IAAI5Q,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKyO,UAAL,CAAgB7P,MAA9B,EAAqCoB,CAAC,EAAtC,EAA0C;AACxC,cAAIzC,CAAC,GAAG,KAAKkR,UAAL,CAAgBzO,CAAhB,CAAR;AACAzC,UAAAA,CAAC,CAAC6S,GAAF,CAAMtR,IAAN,CAAW6R,OAAO,CAAC3Q,CAAD,CAAlB;AACAzC,UAAAA,CAAC,CAAC8S,IAAF,IAAUM,OAAO,CAAC3Q,CAAD,CAAjB;AACD;;AACD,aAAK4O,IAAL,GAAY,CAAZ,CATwB,CAST;;AACf,aAAKC,MAAL,GAVwB,CAUT;;AAEf,YAAG,KAAKC,oBAAL,KAA8B,IAAjC,EAAuC;AACrC,eAAKA,oBAAL;AACD;;AAED,YAAG,KAAKD,MAAL,IAAe,KAAKL,KAAL,CAAW5P,MAA7B,EAAqC;AACnC;AACA;AACA,eAAI,IAAIoB,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKyO,UAAL,CAAgB7P,MAA9B,EAAqCoB,CAAC,EAAtC,EAA0C;AACxC,iBAAK0O,oBAAL,CAA0B5P,IAA1B,CAA+B,KAAK2P,UAAL,CAAgBzO,CAAhB,CAA/B;AACD,WALkC,CAMnC;;;AACA,eAAK0O,oBAAL,CAA0BmC,IAA1B,CAA+B,UAASlT,CAAT,EAAYC,CAAZ,EAAe;AAC5C,mBAAQD,CAAC,CAAC0S,IAAF,GAAS1S,CAAC,CAACyS,GAAF,CAAMxR,MAAhB,GACChB,CAAC,CAACyS,IAAF,GAASzS,CAAC,CAACwS,GAAF,CAAMxR,MADhB,GAEA,CAAC,CAFD,GAEK,CAFZ;AAGD,WAJD,EAPmC,CAYnC;AACA;AACA;;AACA,cAAG,KAAK8P,oBAAL,CAA0B9P,MAA1B,GAAmC,IAAI,KAAKiP,aAA/C,EAA8D;AAC5D,iBAAKa,oBAAL,GAA4B,KAAKA,oBAAL,CAA0BU,KAA1B,CAAgC,CAAhC,EAAmC,IAAI,KAAKvB,aAA5C,CAA5B;AACD;;AACD,cAAG,KAAKkB,qBAAL,KAA+B,IAAlC,EAAwC;AACtC,iBAAKA,qBAAL;AACD;;AACD,eAAKE,gBAAL,GArBmC,CAqBV;;AACzB,eAAKJ,MAAL,GAAc,CAAd,CAtBmC,CAsBlB;AAClB,SAvBD,MAuBO;AACL;AACA,eAAI,IAAI7O,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKyO,UAAL,CAAgB7P,MAA9B,EAAqCoB,CAAC,EAAtC,EAA0C;AACxC,gBAAIzC,CAAC,GAAG,KAAKkR,UAAL,CAAgBzO,CAAhB,CAAR;AACA,gBAAIuL,GAAG,GAAG,IAAIvB,GAAJ,EAAV;AACAuB,YAAAA,GAAG,CAACpB,UAAJ,CAAe5M,CAAC,CAACiS,UAAjB;AACA,gBAAIU,OAAO,GAAG,IAAI5E,OAAJ,CAAYC,GAAZ,EAAiBhO,CAAC,CAAC0S,WAAnB,CAAd;AACA1S,YAAAA,CAAC,CAACgO,GAAF,GAAQA,GAAR;AACAhO,YAAAA,CAAC,CAAC2S,OAAF,GAAYA,OAAZ;AACD;AACF;AACF;AACF,KA3IkB;AA6InBU,IAAAA,aAAa,EAAE,YAAW;AACxB;AACA;AACA,UAAIE,IAAI,GAAG,EAAX;AACA,UAAIP,IAAI,GAAG,KAAK/B,KAAL,CAAW,KAAKK,MAAhB,CAAX,CAJwB,CAIY;;AACpC,WAAI,IAAI7O,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKyO,UAAL,CAAgB7P,MAA9B,EAAqCoB,CAAC,EAAtC,EAA0C;AACxC,YAAIuL,GAAG,GAAG,KAAKkD,UAAL,CAAgBzO,CAAhB,EAAmBuL,GAA7B;AACA,YAAIlO,CAAC,GAAG,GAAR;;AACA,aAAI,IAAIqC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC6Q,IAAI,CAAClB,OAAL,CAAazQ,MAA3B,EAAkCc,CAAC,EAAnC,EAAuC;AACrC,cAAIsB,CAAC,GAAG,KAAKwC,IAAL,CAAU+M,IAAI,CAAClB,OAAL,CAAa3P,CAAb,CAAV,CAAR;AACA,cAAI+Q,CAAC,GAAG,KAAKjD,MAAL,CAAY+C,IAAI,CAAClB,OAAL,CAAa3P,CAAb,CAAZ,CAAR;AACA6L,UAAAA,GAAG,CAAC3G,OAAJ,CAAY5D,CAAZ;AACA,cAAI+P,IAAI,GAAGxF,GAAG,CAACL,aAAJ,EAAX;AACA7N,UAAAA,CAAC,IAAK0T,IAAI,KAAKN,CAAT,GAAa,GAAb,GAAmB,GAAzB,CALqC,CAKN;AAChC;;AACDpT,QAAAA,CAAC,IAAIkT,IAAI,CAAClB,OAAL,CAAazQ,MAAlB,CAVwC,CAUd;;AAC1BkS,QAAAA,IAAI,CAAChS,IAAL,CAAUzB,CAAV;AACD;;AACD,aAAOyT,IAAP;AACD,KAhKkB;AAkKnB;AACA;AACA;AACAE,IAAAA,YAAY,EAAE,UAASxN,IAAT,EAAe;AAC3B;AACA;AACA,UAAIyN,EAAE,GAAG9T,IAAI,CAACqM,GAAL,CAAS,KAAKqE,aAAd,EAA6B,KAAKa,oBAAL,CAA0B9P,MAAvD,CAAT;;AACA,UAAGqS,EAAE,KAAK,CAAV,EAAa;AAAE,eAAO,IAAIrU,SAAS,CAACyD,GAAd,CAAkB,CAAlB,EAAoB,CAApB,EAAsB,CAAtB,CAAP;AAAkC,OAJtB,CAIuB;;;AAClD,UAAI6Q,IAAJ,EAAU/S,CAAV;;AACA,WAAI,IAAIoB,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC0R,EAAd,EAAiB1R,CAAC,EAAlB,EAAsB;AACpB,YAAIgM,GAAG,GAAG,KAAKmD,oBAAL,CAA0BnP,CAA1B,EAA6BgM,GAAvC;AACA,YAAIvK,CAAC,GAAGuK,GAAG,CAAC3G,OAAJ,CAAYpB,IAAZ,CAAR;;AACA,YAAGjE,CAAC,KAAG,CAAP,EAAU;AACR2R,UAAAA,IAAI,GAAGlQ,CAAP;AACA7C,UAAAA,CAAC,GAAG6C,CAAC,CAAChC,CAAF,CAAIJ,MAAR;AACD,SAHD,MAGO;AACL;AACA,eAAI,IAAIsC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC/C,CAAd,EAAgB+C,CAAC,EAAjB,EAAqB;AACnBgQ,YAAAA,IAAI,CAAClS,CAAL,CAAOkC,CAAP,KAAaF,CAAC,CAAChC,CAAF,CAAIkC,CAAJ,CAAb;AACD;AACF;AACF,OAlB0B,CAmB3B;;;AACA,WAAI,IAAIA,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC/C,CAAd,EAAgB+C,CAAC,EAAjB,EAAqB;AACnBgQ,QAAAA,IAAI,CAAClS,CAAL,CAAOkC,CAAP,KAAa/C,CAAb;AACD;;AACD,aAAO+S,IAAP;AACD,KA7LkB;AA+LnBC,IAAAA,OAAO,EAAE,UAAS3N,IAAT,EAAe;AACtB,UAAI0N,IAAI,GAAG,KAAKF,YAAL,CAAkBxN,IAAlB,CAAX;;AACA,UAAG0N,IAAI,CAAClS,CAAL,CAAOJ,MAAP,KAAkB,CAArB,EAAwB;AACtB,YAAIwS,KAAK,GAAGrS,MAAM,CAACmS,IAAI,CAAClS,CAAN,CAAlB;AACA,YAAIqS,eAAe,GAAGD,KAAK,CAACjS,IAA5B;AACD,OAHD,MAGO;AACL,YAAIkS,eAAe,GAAG,CAAC,CAAvB,CADK,CACqB;AAC3B;;AACD,aAAOA,eAAP;AAED,KAzMkB;AA2MnBtP,IAAAA,MAAM,EAAE,YAAW;AACjB;AACA,UAAIkP,EAAE,GAAG9T,IAAI,CAACqM,GAAL,CAAS,KAAKqE,aAAd,EAA6B,KAAKa,oBAAL,CAA0B9P,MAAvD,CAAT;AACA,UAAIoD,IAAI,GAAG,EAAX;AACAA,MAAAA,IAAI,CAACsP,IAAL,GAAY,EAAZ;;AACA,WAAI,IAAI9S,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACyS,EAAd,EAAiBzS,CAAC,EAAlB,EAAsB;AACpBwD,QAAAA,IAAI,CAACsP,IAAL,CAAUxS,IAAV,CAAe,KAAK4P,oBAAL,CAA0BlQ,CAA1B,EAA6B+M,GAA7B,CAAiCxJ,MAAjC,EAAf;AACD;;AACD,aAAOC,IAAP;AACD,KApNkB;AAsNnBC,IAAAA,QAAQ,EAAE,UAASD,IAAT,EAAe;AACvB,WAAK6L,aAAL,GAAqB7L,IAAI,CAACsP,IAAL,CAAU1S,MAA/B;AACA,WAAK8P,oBAAL,GAA4B,EAA5B;;AACA,WAAI,IAAIlQ,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKqP,aAAnB,EAAiCrP,CAAC,EAAlC,EAAsC;AACpC,YAAI+M,GAAG,GAAG,IAAIvB,GAAJ,EAAV;AACAuB,QAAAA,GAAG,CAACtJ,QAAJ,CAAaD,IAAI,CAACsP,IAAL,CAAU9S,CAAV,CAAb;AACA,YAAI+S,eAAe,GAAG,EAAtB;AACAA,QAAAA,eAAe,CAAChG,GAAhB,GAAsBA,GAAtB;AACA,aAAKmD,oBAAL,CAA0B5P,IAA1B,CAA+ByS,eAA/B;AACD;AACF,KAhOkB;AAkOnB;AACA;AACAC,IAAAA,YAAY,EAAE,UAASrM,CAAT,EAAY;AAAE,WAAK2J,oBAAL,GAA4B3J,CAA5B;AAAgC,KApOzC;AAqOnB;AACAsM,IAAAA,aAAa,EAAE,UAAStM,CAAT,EAAY;AAAE,WAAK4J,qBAAL,GAA6B5J,CAA7B;AAAiC;AAtO3C,GAArB;AA0OArI,EAAAA,MAAM,CAACyQ,QAAP,GAAkBA,QAAlB;AACD,CAnTD,EAmTG3Q,SAnTH;;AAoTA,KAAKA,SAAL,GAAiBA,SAAjB","sourcesContent":["var convnetjs = convnetjs || { REVISION: 'ALPHA' };\n(function(global) {\n  \"use strict\";\n\n  // Random number utilities\n  var return_v = false;\n  var v_val = 0.0;\n  var gaussRandom = function() {\n    if(return_v) { \n      return_v = false;\n      return v_val; \n    }\n    var u = 2*Math.random()-1;\n    var v = 2*Math.random()-1;\n    var r = u*u + v*v;\n    if(r == 0 || r > 1) return gaussRandom();\n    var c = Math.sqrt(-2*Math.log(r)/r);\n    v_val = v*c; // cache this\n    return_v = true;\n    return u*c;\n  }\n  var randf = function(a, b) { return Math.random()*(b-a)+a; }\n  var randi = function(a, b) { return Math.floor(Math.random()*(b-a)+a); }\n  var randn = function(mu, std){ return mu+gaussRandom()*std; }\n\n  // Array utilities\n  var zeros = function(n) {\n    if(typeof(n)==='undefined' || isNaN(n)) { return []; }\n    if(typeof ArrayBuffer === 'undefined') {\n      // lacking browser support\n      var arr = new Array(n);\n      for(var i=0;i<n;i++) { arr[i]= 0; }\n      return arr;\n    } else {\n      return new Float64Array(n);\n    }\n  }\n\n  var arrContains = function(arr, elt) {\n    for(var i=0,n=arr.length;i<n;i++) {\n      if(arr[i]===elt) return true;\n    }\n    return false;\n  }\n\n  var arrUnique = function(arr) {\n    var b = [];\n    for(var i=0,n=arr.length;i<n;i++) {\n      if(!arrContains(b, arr[i])) {\n        b.push(arr[i]);\n      }\n    }\n    return b;\n  }\n\n  // return max and min of a given non-empty array.\n  var maxmin = function(w) {\n    if(w.length === 0) { return {}; } // ... ;s\n    var maxv = w[0];\n    var minv = w[0];\n    var maxi = 0;\n    var mini = 0;\n    var n = w.length;\n    for(var i=1;i<n;i++) {\n      if(w[i] > maxv) { maxv = w[i]; maxi = i; } \n      if(w[i] < minv) { minv = w[i]; mini = i; } \n    }\n    return {maxi: maxi, maxv: maxv, mini: mini, minv: minv, dv:maxv-minv};\n  }\n\n  // create random permutation of numbers, in range [0...n-1]\n  var randperm = function(n) {\n    var i = n,\n        j = 0,\n        temp;\n    var array = [];\n    for(var q=0;q<n;q++)array[q]=q;\n    while (i--) {\n        j = Math.floor(Math.random() * (i+1));\n        temp = array[i];\n        array[i] = array[j];\n        array[j] = temp;\n    }\n    return array;\n  }\n\n  // sample from list lst according to probabilities in list probs\n  // the two lists are of same size, and probs adds up to 1\n  var weightedSample = function(lst, probs) {\n    var p = randf(0, 1.0);\n    var cumprob = 0.0;\n    for(var k=0,n=lst.length;k<n;k++) {\n      cumprob += probs[k];\n      if(p < cumprob) { return lst[k]; }\n    }\n  }\n\n  // syntactic sugar function for getting default parameter values\n  var getopt = function(opt, field_name, default_value) {\n    return typeof opt[field_name] !== 'undefined' ? opt[field_name] : default_value;\n  }\n\n  global.randf = randf;\n  global.randi = randi;\n  global.randn = randn;\n  global.zeros = zeros;\n  global.maxmin = maxmin;\n  global.randperm = randperm;\n  global.weightedSample = weightedSample;\n  global.arrUnique = arrUnique;\n  global.arrContains = arrContains;\n  global.getopt = getopt;\n  \n})(convnetjs);\n(function(global) {\n  \"use strict\";\n\n  // Vol is the basic building block of all data in a net.\n  // it is essentially just a 3D volume of numbers, with a\n  // width (sx), height (sy), and depth (depth).\n  // it is used to hold data for all filters, all volumes,\n  // all weights, and also stores all gradients w.r.t. \n  // the data. c is optionally a value to initialize the volume\n  // with. If c is missing, fills the Vol with random numbers.\n  var Vol = function(sx, sy, depth, c) {\n    // this is how you check if a variable is an array. Oh, Javascript :)\n    if(Object.prototype.toString.call(sx) === '[object Array]') {\n      // we were given a list in sx, assume 1D volume and fill it up\n      this.sx = 1;\n      this.sy = 1;\n      this.depth = sx.length;\n      // we have to do the following copy because we want to use\n      // fast typed arrays, not an ordinary javascript array\n      this.w = global.zeros(this.depth);\n      this.dw = global.zeros(this.depth);\n      for(var i=0;i<this.depth;i++) {\n        this.w[i] = sx[i];\n      }\n    } else {\n      // we were given dimensions of the vol\n      this.sx = sx;\n      this.sy = sy;\n      this.depth = depth;\n      var n = sx*sy*depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n);\n      if(typeof c === 'undefined') {\n        // weight normalization is done to equalize the output\n        // variance of every neuron, otherwise neurons with a lot\n        // of incoming connections have outputs of larger variance\n        var scale = Math.sqrt(1.0/(sx*sy*depth));\n        for(var i=0;i<n;i++) { \n          this.w[i] = global.randn(0.0, scale);\n        }\n      } else {\n        for(var i=0;i<n;i++) { \n          this.w[i] = c;\n        }\n      }\n    }\n  }\n\n  Vol.prototype = {\n    get: function(x, y, d) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      return this.w[ix];\n    },\n    set: function(x, y, d, v) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      this.w[ix] = v; \n    },\n    add: function(x, y, d, v) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      this.w[ix] += v; \n    },\n    get_grad: function(x, y, d) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      return this.dw[ix]; \n    },\n    set_grad: function(x, y, d, v) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      this.dw[ix] = v; \n    },\n    add_grad: function(x, y, d, v) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      this.dw[ix] += v; \n    },\n    cloneAndZero: function() { return new Vol(this.sx, this.sy, this.depth, 0.0)},\n    clone: function() {\n      var V = new Vol(this.sx, this.sy, this.depth, 0.0);\n      var n = this.w.length;\n      for(var i=0;i<n;i++) { V.w[i] = this.w[i]; }\n      return V;\n    },\n    addFrom: function(V) { for(var k=0;k<this.w.length;k++) { this.w[k] += V.w[k]; }},\n    addFromScaled: function(V, a) { for(var k=0;k<this.w.length;k++) { this.w[k] += a*V.w[k]; }},\n    setConst: function(a) { for(var k=0;k<this.w.length;k++) { this.w[k] = a; }},\n\n    toJSON: function() {\n      // todo: we may want to only save d most significant digits to save space\n      var json = {}\n      json.sx = this.sx; \n      json.sy = this.sy;\n      json.depth = this.depth;\n      json.w = this.w;\n      return json;\n      // we wont back up gradients to save space\n    },\n    fromJSON: function(json) {\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.depth = json.depth;\n\n      var n = this.sx*this.sy*this.depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n);\n      // copy over the elements.\n      for(var i=0;i<n;i++) {\n        this.w[i] = json.w[i];\n      }\n    }\n  }\n\n  global.Vol = Vol;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // Volume utilities\n  // intended for use with data augmentation\n  // crop is the size of output\n  // dx,dy are offset wrt incoming volume, of the shift\n  // fliplr is boolean on whether we also want to flip left<->right\n  var augment = function(V, crop, dx, dy, fliplr) {\n    // note assumes square outputs of size crop x crop\n    if(typeof(fliplr)==='undefined') var fliplr = false;\n    if(typeof(dx)==='undefined') var dx = global.randi(0, V.sx - crop);\n    if(typeof(dy)==='undefined') var dy = global.randi(0, V.sy - crop);\n    \n    // randomly sample a crop in the input volume\n    var W;\n    if(crop !== V.sx || dx!==0 || dy!==0) {\n      W = new Vol(crop, crop, V.depth, 0.0);\n      for(var x=0;x<crop;x++) {\n        for(var y=0;y<crop;y++) {\n          if(x+dx<0 || x+dx>=V.sx || y+dy<0 || y+dy>=V.sy) continue; // oob\n          for(var d=0;d<V.depth;d++) {\n           W.set(x,y,d,V.get(x+dx,y+dy,d)); // copy data over\n          }\n        }\n      }\n    } else {\n      W = V;\n    }\n\n    if(fliplr) {\n      // flip volume horziontally\n      var W2 = W.cloneAndZero();\n      for(var x=0;x<W.sx;x++) {\n        for(var y=0;y<W.sy;y++) {\n          for(var d=0;d<W.depth;d++) {\n           W2.set(x,y,d,W.get(W.sx - x - 1,y,d)); // copy data over\n          }\n        }\n      }\n      W = W2; //swap\n    }\n    return W;\n  }\n\n  // img is a DOM element that contains a loaded image\n  // returns a Vol of size (W, H, 4). 4 is for RGBA\n  var img_to_vol = function(img, convert_grayscale) {\n\n    if(typeof(convert_grayscale)==='undefined') var convert_grayscale = false;\n\n    var canvas = document.createElement('canvas');\n    canvas.width = img.width;\n    canvas.height = img.height;\n    var ctx = canvas.getContext(\"2d\");\n\n    // due to a Firefox bug\n    try {\n      ctx.drawImage(img, 0, 0);\n    } catch (e) {\n      if (e.name === \"NS_ERROR_NOT_AVAILABLE\") {\n        // sometimes happens, lets just abort\n        return false;\n      } else {\n        throw e;\n      }\n    }\n\n    try {\n      var img_data = ctx.getImageData(0, 0, canvas.width, canvas.height);\n    } catch (e) {\n      if(e.name === 'IndexSizeError') {\n        return false; // not sure what causes this sometimes but okay abort\n      } else {\n        throw e;\n      }\n    }\n\n    // prepare the input: get pixels and normalize them\n    var p = img_data.data;\n    var W = img.width;\n    var H = img.height;\n    var pv = []\n    for(var i=0;i<p.length;i++) {\n      pv.push(p[i]/255.0-0.5); // normalize image pixels to [-0.5, 0.5]\n    }\n    var x = new Vol(W, H, 4, 0.0); //input volume (image)\n    x.w = pv;\n\n    if(convert_grayscale) {\n      // flatten into depth=1 array\n      var x1 = new Vol(W, H, 1, 0.0);\n      for(var i=0;i<W;i++) {\n        for(var j=0;j<H;j++) {\n          x1.set(i,j,0,x.get(i,j,0));\n        }\n      }\n      x = x1;\n    }\n\n    return x;\n  }\n  \n  global.augment = augment;\n  global.img_to_vol = img_to_vol;\n\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // This file contains all layers that do dot products with input,\n  // but usually in a different connectivity pattern and weight sharing\n  // schemes: \n  // - FullyConn is fully connected dot products \n  // - ConvLayer does convolutions (so weight sharing spatially)\n  // putting them together in one file because they are very similar\n  var ConvLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.out_depth = opt.filters;\n    this.sx = opt.sx; // filter size. Should be odd if possible, it's cleaner.\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy;\n    \n    // optional\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 1; // stride at which we apply filters to input volume\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;\n\n    // computed\n    // note we are doing floor, so if the strided convolution of the filter doesnt fit into the input\n    // volume exactly, the output volume will be trimmed and not contain the (incomplete) computed\n    // final application.\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'conv';\n\n    // initializations\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n    for(var i=0;i<this.out_depth;i++) { this.filters.push(new Vol(this.sx, this.sy, this.in_depth)); }\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  }\n  ConvLayer.prototype = {\n    forward: function(V, is_training) {\n      // optimized code by @mdda that achieves 2x speedup over previous version\n\n      this.in_act = V;\n      var A = new Vol(this.out_sx |0, this.out_sy |0, this.out_depth |0, 0.0);\n      \n      var V_sx = V.sx |0;\n      var V_sy = V.sy |0;\n      var xy_stride = this.stride |0;\n\n      for(var d=0;d<this.out_depth;d++) {\n        var f = this.filters[d];\n        var x = -this.pad |0;\n        var y = -this.pad |0;\n        for(var ay=0; ay<this.out_sy; y+=xy_stride,ay++) {  // xy_stride\n          x = -this.pad |0;\n          for(var ax=0; ax<this.out_sx; x+=xy_stride,ax++) {  // xy_stride\n\n            // convolve centered at this particular location\n            var a = 0.0;\n            for(var fy=0;fy<f.sy;fy++) {\n              var oy = y+fy; // coordinates in the original input array coordinates\n              for(var fx=0;fx<f.sx;fx++) {\n                var ox = x+fx;\n                if(oy>=0 && oy<V_sy && ox>=0 && ox<V_sx) {\n                  for(var fd=0;fd<f.depth;fd++) {\n                    // avoid function call overhead (x2) for efficiency, compromise modularity :(\n                    a += f.w[((f.sx * fy)+fx)*f.depth+fd] * V.w[((V_sx * oy)+ox)*V.depth+fd];\n                  }\n                }\n              }\n            }\n            a += this.biases.w[d];\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() {\n\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt bottom data, we're about to fill it\n\n      var V_sx = V.sx |0;\n      var V_sy = V.sy |0;\n      var xy_stride = this.stride |0;\n\n      for(var d=0;d<this.out_depth;d++) {\n        var f = this.filters[d];\n        var x = -this.pad |0;\n        var y = -this.pad |0;\n        for(var ay=0; ay<this.out_sy; y+=xy_stride,ay++) {  // xy_stride\n          x = -this.pad |0;\n          for(var ax=0; ax<this.out_sx; x+=xy_stride,ax++) {  // xy_stride\n\n            // convolve centered at this particular location\n            var chain_grad = this.out_act.get_grad(ax,ay,d); // gradient from above, from chain rule\n            for(var fy=0;fy<f.sy;fy++) {\n              var oy = y+fy; // coordinates in the original input array coordinates\n              for(var fx=0;fx<f.sx;fx++) {\n                var ox = x+fx;\n                if(oy>=0 && oy<V_sy && ox>=0 && ox<V_sx) {\n                  for(var fd=0;fd<f.depth;fd++) {\n                    // avoid function call overhead (x2) for efficiency, compromise modularity :(\n                    var ix1 = ((V_sx * oy)+ox)*V.depth+fd;\n                    var ix2 = ((f.sx * fy)+fx)*f.depth+fd;\n                    f.dw[ix2] += V.w[ix1]*chain_grad;\n                    V.dw[ix1] += f.w[ix2]*chain_grad;\n                  }\n                }\n              }\n            }\n            this.biases.dw[d] += chain_grad;\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      var response = [];\n      for(var i=0;i<this.out_depth;i++) {\n        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l2_decay_mul: this.l2_decay_mul, l1_decay_mul: this.l1_decay_mul});\n      }\n      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});\n      return response;\n    },\n    toJSON: function() {\n      var json = {};\n      json.sx = this.sx; // filter size in x, y dims\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.pad = this.pad;\n      json.filters = [];\n      for(var i=0;i<this.filters.length;i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx; // filter size in x, y dims\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth; // depth of input volume\n      this.filters = [];\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0;\n      for(var i=0;i<json.filters.length;i++) {\n        var v = new Vol(0,0,0,0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n      this.biases = new Vol(0,0,0,0);\n      this.biases.fromJSON(json.biases);\n    }\n  }\n\n  var FullyConnLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    // ok fine we will allow 'filters' as the word as well\n    this.out_depth = typeof opt.num_neurons !== 'undefined' ? opt.num_neurons : opt.filters;\n\n    // optional \n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'fc';\n\n    // initializations\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n    for(var i=0;i<this.out_depth ;i++) { this.filters.push(new Vol(1, 1, this.num_inputs)); }\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  }\n\n  FullyConnLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var A = new Vol(1, 1, this.out_depth, 0.0);\n      var Vw = V.w;\n      for(var i=0;i<this.out_depth;i++) {\n        var a = 0.0;\n        var wi = this.filters[i].w;\n        for(var d=0;d<this.num_inputs;d++) {\n          a += Vw[d] * wi[d]; // for efficiency use Vols directly for now\n        }\n        a += this.biases.w[i];\n        A.w[i] = a;\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out the gradient in input Vol\n      \n      // compute gradient wrt weights and data\n      for(var i=0;i<this.out_depth;i++) {\n        var tfi = this.filters[i];\n        var chain_grad = this.out_act.dw[i];\n        for(var d=0;d<this.num_inputs;d++) {\n          V.dw[d] += tfi.w[d]*chain_grad; // grad wrt input data\n          tfi.dw[d] += V.w[d]*chain_grad; // grad wrt params\n        }\n        this.biases.dw[i] += chain_grad;\n      }\n    },\n    getParamsAndGrads: function() {\n      var response = [];\n      for(var i=0;i<this.out_depth;i++) {\n        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l1_decay_mul: this.l1_decay_mul, l2_decay_mul: this.l2_decay_mul});\n      }\n      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});\n      return response;\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.filters = [];\n      for(var i=0;i<this.filters.length;i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.filters = [];\n      for(var i=0;i<json.filters.length;i++) {\n        var v = new Vol(0,0,0,0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n      this.biases = new Vol(0,0,0,0);\n      this.biases.fromJSON(json.biases);\n    }\n  }\n\n  global.ConvLayer = ConvLayer;\n  global.FullyConnLayer = FullyConnLayer;\n  \n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  var PoolLayer = function(opt) {\n\n    var opt = opt || {};\n\n    // required\n    this.sx = opt.sx; // filter size\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy;\n\n    // optional\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 2;\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n\n    // computed\n    this.out_depth = this.in_depth;\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'pool';\n    // store switches for x,y coordinates for where the max comes from, for each output neuron\n    this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n    this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n  }\n\n  PoolLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      \n      var n=0; // a counter for switches\n      for(var d=0;d<this.out_depth;d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            // convolve centered at this particular location\n            var a = -99999; // hopefully small enough ;\\\n            var winx=-1,winy=-1;\n            for(var fx=0;fx<this.sx;fx++) {\n              for(var fy=0;fy<this.sy;fy++) {\n                var oy = y+fy;\n                var ox = x+fx;\n                if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {\n                  var v = V.get(ox, oy, d);\n                  // perform max pooling and store pointers to where\n                  // the max came from. This will speed up backprop \n                  // and can help make nice visualizations in future\n                  if(v > a) { a = v; winx=ox; winy=oy;}\n                }\n              }\n            }\n            this.switchx[n] = winx;\n            this.switchy[n] = winy;\n            n++;\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() { \n      // pooling layers have no parameters, so simply compute \n      // gradient wrt data here\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var A = this.out_act; // computed in forward pass \n\n      var n = 0;\n      for(var d=0;d<this.out_depth;d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            var chain_grad = this.out_act.get_grad(ax,ay,d);\n            V.add_grad(this.switchx[n], this.switchy[n], d, chain_grad);\n            n++;\n\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.sx = this.sx;\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.pad = this.pad;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0; // backwards compatibility\n      this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth); // need to re-init these appropriately\n      this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n    }\n  }\n\n  global.PoolLayer = PoolLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  var InputLayer = function(opt) {\n    var opt = opt || {};\n\n    // this is a bit silly but lets allow people to specify either ins or outs\n    this.out_sx = typeof opt.out_sx !== 'undefined' ? opt.out_sx : opt.in_sx;\n    this.out_sy = typeof opt.out_sy !== 'undefined' ? opt.out_sy : opt.in_sy;\n    this.out_depth = typeof opt.out_depth !== 'undefined' ? opt.out_depth : opt.in_depth;\n    this.layer_type = 'input';\n  }\n  InputLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() { },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  global.InputLayer = InputLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Layers that implement a loss. Currently these are the layers that \n  // can initiate a backward() pass. In future we probably want a more \n  // flexible system that can accomodate multiple losses to do multi-task\n  // learning, and stuff like that. But for now, one of the layers in this\n  // file must be the final layer in a Net.\n\n  // This is a classifier, with N discrete classes from 0 to N-1\n  // it gets a stream of N incoming numbers and computes the softmax\n  // function (exponentiate and normalize to sum to 1 as probabilities should)\n  var SoftmaxLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'softmax';\n  }\n\n  SoftmaxLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(1, 1, this.out_depth, 0.0);\n\n      // compute max activation\n      var as = V.w;\n      var amax = V.w[0];\n      for(var i=1;i<this.out_depth;i++) {\n        if(as[i] > amax) amax = as[i];\n      }\n\n      // compute exponentials (carefully to not blow up)\n      var es = global.zeros(this.out_depth);\n      var esum = 0.0;\n      for(var i=0;i<this.out_depth;i++) {\n        var e = Math.exp(as[i] - amax);\n        esum += e;\n        es[i] = e;\n      }\n\n      // normalize and output to sum to one\n      for(var i=0;i<this.out_depth;i++) {\n        es[i] /= esum;\n        A.w[i] = es[i];\n      }\n\n      this.es = es; // save these for backprop\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function(y) {\n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      for(var i=0;i<this.out_depth;i++) {\n        var indicator = i === y ? 1.0 : 0.0;\n        var mul = -(indicator - this.es[i]);\n        x.dw[i] = mul;\n      }\n\n      // loss is the class negative log likelihood\n      return -Math.log(this.es[y]);\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n\n  // implements an L2 regression cost layer,\n  // so penalizes \\sum_i(||x_i - y_i||^2), where x is its input\n  // and y is the user-provided array of \"correct\" values.\n  var RegressionLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'regression';\n  }\n\n  RegressionLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return V; // identity function\n    },\n    // y is a list here of size num_inputs\n    backward: function(y) { \n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n      var loss = 0.0;\n      if(y instanceof Array || y instanceof Float64Array) {\n        for(var i=0;i<this.out_depth;i++) {\n          var dy = x.w[i] - y[i];\n          x.dw[i] = dy;\n          loss += 2*dy*dy;\n        }\n      } else {\n        // assume it is a struct with entries .dim and .val\n        // and we pass gradient only along dimension dim to be equal to val\n        var i = y.dim;\n        var yi = y.val;\n        var dy = x.w[i] - yi;\n        x.dw[i] = dy;\n        loss += 2*dy*dy;\n      }\n      return loss;\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n\n  var SVMLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'svm';\n  }\n\n  SVMLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V; // nothing to do, output raw scores\n      return V;\n    },\n    backward: function(y) {\n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      var yscore = x.w[y]; // score of ground truth\n      var margin = 1.0;\n      var loss = 0.0;\n      for(var i=0;i<this.out_depth;i++) {\n        if(-yscore + x.w[i] + margin > 0) {\n          // violating example, apply loss\n          // I love hinge loss, by the way. Truly.\n          // Seriously, compare this SVM code with Softmax forward AND backprop code above\n          // it's clear which one is superior, not only in code, simplicity\n          // and beauty, but also in practice.\n          x.dw[i] += 1;\n          x.dw[y] -= 1;\n          loss += -yscore + x.w[i] + margin;\n        }\n      }\n\n      return loss;\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n  \n  global.RegressionLayer = RegressionLayer;\n  global.SoftmaxLayer = SoftmaxLayer;\n  global.SVMLayer = SVMLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Implements ReLU nonlinearity elementwise\n  // x -> max(0, x)\n  // the output is in [0, inf)\n  var ReluLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'relu';\n  }\n  ReluLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.clone();\n      var N = V.w.length;\n      var V2w = V2.w;\n      for(var i=0;i<N;i++) { \n        if(V2w[i] < 0) V2w[i] = 0; // threshold at 0\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        if(V2.w[i] <= 0) V.dw[i] = 0; // threshold\n        else V.dw[i] = V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  // Implements Sigmoid nnonlinearity elementwise\n  // x -> 1/(1+e^(-x))\n  // so the output is between 0 and 1.\n  var SigmoidLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'sigmoid';\n  }\n  SigmoidLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n      var V2w = V2.w;\n      var Vw = V.w;\n      for(var i=0;i<N;i++) { \n        V2w[i] = 1.0/(1.0+Math.exp(-Vw[i]));\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] =  v2wi * (1.0 - v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  // Implements Maxout nnonlinearity that computes\n  // x -> max(x)\n  // where x is a vector of size group_size. Ideally of course,\n  // the input size should be exactly divisible by group_size\n  var MaxoutLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.group_size = typeof opt.group_size !== 'undefined' ? opt.group_size : 2;\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = Math.floor(opt.in_depth / this.group_size);\n    this.layer_type = 'maxout';\n\n    this.switches = global.zeros(this.out_sx*this.out_sy*this.out_depth); // useful for backprop\n  }\n  MaxoutLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var N = this.out_depth; \n      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n\n      // optimization branch. If we're operating on 1D arrays we dont have\n      // to worry about keeping track of x,y,d coordinates inside\n      // input volumes. In convnets we do :(\n      if(this.out_sx === 1 && this.out_sy === 1) {\n        for(var i=0;i<N;i++) {\n          var ix = i * this.group_size; // base index offset\n          var a = V.w[ix];\n          var ai = 0;\n          for(var j=1;j<this.group_size;j++) {\n            var a2 = V.w[ix+j];\n            if(a2 > a) {\n              a = a2;\n              ai = j;\n            }\n          }\n          V2.w[i] = a;\n          this.switches[i] = ix + ai;\n        }\n      } else {\n        var n=0; // counter for switches\n        for(var x=0;x<V.sx;x++) {\n          for(var y=0;y<V.sy;y++) {\n            for(var i=0;i<N;i++) {\n              var ix = i * this.group_size;\n              var a = V.get(x, y, ix);\n              var ai = 0;\n              for(var j=1;j<this.group_size;j++) {\n                var a2 = V.get(x, y, ix+j);\n                if(a2 > a) {\n                  a = a2;\n                  ai = j;\n                }\n              }\n              V2.set(x,y,i,a);\n              this.switches[n] = ix + ai;\n              n++;\n            }\n          }\n        }\n\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = this.out_depth;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n\n      // pass the gradient through the appropriate switch\n      if(this.out_sx === 1 && this.out_sy === 1) {\n        for(var i=0;i<N;i++) {\n          var chain_grad = V2.dw[i];\n          V.dw[this.switches[i]] = chain_grad;\n        }\n      } else {\n        // bleh okay, lets do this the hard way\n        var n=0; // counter for switches\n        for(var x=0;x<V2.sx;x++) {\n          for(var y=0;y<V2.sy;y++) {\n            for(var i=0;i<N;i++) {\n              var chain_grad = V2.get_grad(x,y,i);\n              V.set_grad(x,y,this.switches[n],chain_grad);\n              n++;\n            }\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.group_size = this.group_size;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n      this.group_size = json.group_size;\n      this.switches = global.zeros(this.group_size);\n    }\n  }\n\n  // a helper function, since tanh is not yet part of ECMAScript. Will be in v6.\n  function tanh(x) {\n    var y = Math.exp(2 * x);\n    return (y - 1) / (y + 1);\n  }\n  // Implements Tanh nnonlinearity elementwise\n  // x -> tanh(x) \n  // so the output is between -1 and 1.\n  var TanhLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'tanh';\n  }\n  TanhLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n      for(var i=0;i<N;i++) { \n        V2.w[i] = tanh(V.w[i]);\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] = (1.0 - v2wi * v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n  \n  global.TanhLayer = TanhLayer;\n  global.MaxoutLayer = MaxoutLayer;\n  global.ReluLayer = ReluLayer;\n  global.SigmoidLayer = SigmoidLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // An inefficient dropout layer\n  // Note this is not most efficient implementation since the layer before\n  // computed all these activations and now we're just going to drop them :(\n  // same goes for backward pass. Also, if we wanted to be efficient at test time\n  // we could equivalently be clever and upscale during train and copy pointers during test\n  // todo: make more efficient.\n  var DropoutLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'dropout';\n    this.drop_prob = typeof opt.drop_prob !== 'undefined' ? opt.drop_prob : 0.5;\n    this.dropped = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n  }\n  DropoutLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      if(typeof(is_training)==='undefined') { is_training = false; } // default is prediction mode\n      var V2 = V.clone();\n      var N = V.w.length;\n      if(is_training) {\n        // do dropout\n        for(var i=0;i<N;i++) {\n          if(Math.random()<this.drop_prob) { V2.w[i]=0; this.dropped[i] = true; } // drop!\n          else {this.dropped[i] = false;}\n        }\n      } else {\n        // scale the activations during prediction\n        for(var i=0;i<N;i++) { V2.w[i]*=this.drop_prob; }\n      }\n      this.out_act = V2;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var chain_grad = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        if(!(this.dropped[i])) { \n          V.dw[i] = chain_grad.dw[i]; // copy over the gradient\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.drop_prob = this.drop_prob;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n      this.drop_prob = json.drop_prob;\n    }\n  }\n  \n\n  global.DropoutLayer = DropoutLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // a bit experimental layer for now. I think it works but I'm not 100%\n  // the gradient check is a bit funky. I'll look into this a bit later.\n  // Local Response Normalization in window, along depths of volumes\n  var LocalResponseNormalizationLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.k = opt.k;\n    this.n = opt.n;\n    this.alpha = opt.alpha;\n    this.beta = opt.beta;\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'lrn';\n\n    // checks\n    if(this.n%2 === 0) { console.log('WARNING n should be odd for LRN layer'); }\n  }\n  LocalResponseNormalizationLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = V.cloneAndZero();\n      this.S_cache_ = V.cloneAndZero();\n      var n2 = Math.floor(this.n/2);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<V.depth;i++) {\n\n            var ai = V.get(x,y,i);\n\n            // normalize in a window of size n\n            var den = 0.0;\n            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {\n              var aa = V.get(x,y,j);\n              den += aa*aa;\n            }\n            den *= this.alpha / this.n;\n            den += this.k;\n            this.S_cache_.set(x,y,i,den); // will be useful for backprop\n            den = Math.pow(den, this.beta);\n            A.set(x,y,i,ai/den);\n          }\n        }\n      }\n\n      this.out_act = A;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() { \n      // evaluate gradient wrt data\n      var V = this.in_act; // we need to set dw of this\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var A = this.out_act; // computed in forward pass \n\n      var n2 = Math.floor(this.n/2);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<V.depth;i++) {\n\n            var chain_grad = this.out_act.get_grad(x,y,i);\n            var S = this.S_cache_.get(x,y,i);\n            var SB = Math.pow(S, this.beta);\n            var SB2 = SB*SB;\n\n            // normalize in a window of size n\n            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {              \n              var aj = V.get(x,y,j); \n              var g = -aj*this.beta*Math.pow(S,this.beta-1)*this.alpha/this.n*2*aj;\n              if(j===i) g+= SB;\n              g /= SB2;\n              g *= chain_grad;\n              V.add_grad(x,y,j,g);\n            }\n\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() { return []; },\n    toJSON: function() {\n      var json = {};\n      json.k = this.k;\n      json.n = this.n;\n      json.alpha = this.alpha; // normalize by size\n      json.beta = this.beta;\n      json.out_sx = this.out_sx; \n      json.out_sy = this.out_sy;\n      json.out_depth = this.out_depth;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.k = json.k;\n      this.n = json.n;\n      this.alpha = json.alpha; // normalize by size\n      this.beta = json.beta;\n      this.out_sx = json.out_sx; \n      this.out_sy = json.out_sy;\n      this.out_depth = json.out_depth;\n      this.layer_type = json.layer_type;\n    }\n  }\n  \n\n  global.LocalResponseNormalizationLayer = LocalResponseNormalizationLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Net manages a set of layers\n  // For now constraints: Simple linear order of layers, first layer input last layer a cost layer\n  var Net = function(options) {\n    this.layers = [];\n  }\n\n  Net.prototype = {\n    \n    // takes a list of layer definitions and creates the network layer objects\n    makeLayers: function(defs) {\n\n      // few checks for now\n      if(defs.length<2) {console.log('ERROR! For now at least have input and softmax layers.');}\n      if(defs[0].type !== 'input') {console.log('ERROR! For now first layer should be input.');}\n\n      // desugar syntactic for adding activations and dropouts\n      var desugar = function() {\n        var new_defs = [];\n        for(var i=0;i<defs.length;i++) {\n          var def = defs[i];\n          \n          if(def.type==='softmax' || def.type==='svm') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({type:'fc', num_neurons: def.num_classes});\n          }\n\n          if(def.type==='regression') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({type:'fc', num_neurons: def.num_neurons});\n          }\n\n          if((def.type==='fc' || def.type==='conv') \n              && typeof(def.bias_pref) === 'undefined'){\n            def.bias_pref = 0.0;\n            if(typeof def.activation !== 'undefined' && def.activation === 'relu') {\n              def.bias_pref = 0.1; // relus like a bit of positive bias to get gradients early\n              // otherwise it's technically possible that a relu unit will never turn on (by chance)\n              // and will never get any gradient and never contribute any computation. Dead relu.\n            }\n          }\n          \n          if(typeof def.tensor !== 'undefined') {\n            // apply quadratic transform so that the upcoming multiply will include\n            // quadratic terms, equivalent to doing a tensor product\n            if(def.tensor) {\n              new_defs.push({type: 'quadtransform'});\n            }\n          }\n\n          new_defs.push(def);\n\n          if(typeof def.activation !== 'undefined') {\n            if(def.activation==='relu') { new_defs.push({type:'relu'}); }\n            else if (def.activation==='sigmoid') { new_defs.push({type:'sigmoid'}); }\n            else if (def.activation==='tanh') { new_defs.push({type:'tanh'}); }\n            else if (def.activation==='maxout') {\n              // create maxout activation, and pass along group size, if provided\n              var gs = def.group_size !== 'undefined' ? def.group_size : 2;\n              new_defs.push({type:'maxout', group_size:gs});\n            }\n            else { console.log('ERROR unsupported activation ' + def.activation); }\n          }\n          if(typeof def.drop_prob !== 'undefined' && def.type !== 'dropout') {\n            new_defs.push({type:'dropout', drop_prob: def.drop_prob});\n          }\n\n        }\n        return new_defs;\n      }\n      defs = desugar(defs);\n\n      // create the layers\n      this.layers = [];\n      for(var i=0;i<defs.length;i++) {\n        var def = defs[i];\n        if(i>0) {\n          var prev = this.layers[i-1];\n          def.in_sx = prev.out_sx;\n          def.in_sy = prev.out_sy;\n          def.in_depth = prev.out_depth;\n        }\n\n        switch(def.type) {\n          case 'fc': this.layers.push(new global.FullyConnLayer(def)); break;\n          case 'lrn': this.layers.push(new global.LocalResponseNormalizationLayer(def)); break;\n          case 'dropout': this.layers.push(new global.DropoutLayer(def)); break;\n          case 'input': this.layers.push(new global.InputLayer(def)); break;\n          case 'softmax': this.layers.push(new global.SoftmaxLayer(def)); break;\n          case 'regression': this.layers.push(new global.RegressionLayer(def)); break;\n          case 'conv': this.layers.push(new global.ConvLayer(def)); break;\n          case 'pool': this.layers.push(new global.PoolLayer(def)); break;\n          case 'relu': this.layers.push(new global.ReluLayer(def)); break;\n          case 'sigmoid': this.layers.push(new global.SigmoidLayer(def)); break;\n          case 'tanh': this.layers.push(new global.TanhLayer(def)); break;\n          case 'maxout': this.layers.push(new global.MaxoutLayer(def)); break;\n          case 'quadtransform': this.layers.push(new global.QuadTransformLayer(def)); break;\n          case 'svm': this.layers.push(new global.SVMLayer(def)); break;\n          default: console.log('ERROR: UNRECOGNIZED LAYER TYPE!');\n        }\n      }\n    },\n\n    // forward prop the network. A trainer will pass in is_training = true\n    forward: function(V, is_training) {\n      if(typeof(is_training)==='undefined') is_training = false;\n      var act = this.layers[0].forward(V, is_training);\n      for(var i=1;i<this.layers.length;i++) {\n        act = this.layers[i].forward(act, is_training);\n      }\n      return act;\n    },\n\n    getCostLoss: function(V, y) {\n      this.forward(V, false);\n      var N = this.layers.length;\n      var loss = this.layers[N-1].backward(y);\n      return loss;\n    },\n    \n    // backprop: compute gradients wrt all parameters\n    backward: function(y) {\n      var N = this.layers.length;\n      var loss = this.layers[N-1].backward(y); // last layer assumed softmax\n      for(var i=N-2;i>=0;i--) { // first layer assumed input\n        this.layers[i].backward();\n      }\n      return loss;\n    },\n    getParamsAndGrads: function() {\n      // accumulate parameters and gradients for the entire network\n      var response = [];\n      for(var i=0;i<this.layers.length;i++) {\n        var layer_reponse = this.layers[i].getParamsAndGrads();\n        for(var j=0;j<layer_reponse.length;j++) {\n          response.push(layer_reponse[j]);\n        }\n      }\n      return response;\n    },\n    getPrediction: function() {\n      var S = this.layers[this.layers.length-1]; // softmax layer\n      var p = S.out_act.w;\n      var maxv = p[0];\n      var maxi = 0;\n      for(var i=1;i<p.length;i++) {\n        if(p[i] > maxv) { maxv = p[i]; maxi = i;}\n      }\n      return maxi;\n    },\n    toJSON: function() {\n      var json = {};\n      json.layers = [];\n      for(var i=0;i<this.layers.length;i++) {\n        json.layers.push(this.layers[i].toJSON());\n      }\n      return json;\n    },\n    fromJSON: function(json) {\n      this.layers = [];\n      for(var i=0;i<json.layers.length;i++) {\n        var Lj = json.layers[i]\n        var t = Lj.layer_type;\n        var L;\n        if(t==='input') { L = new global.InputLayer(); }\n        if(t==='relu') { L = new global.ReluLayer(); }\n        if(t==='sigmoid') { L = new global.SigmoidLayer(); }\n        if(t==='tanh') { L = new global.TanhLayer(); }\n        if(t==='dropout') { L = new global.DropoutLayer(); }\n        if(t==='conv') { L = new global.ConvLayer(); }\n        if(t==='pool') { L = new global.PoolLayer(); }\n        if(t==='lrn') { L = new global.LocalResponseNormalizationLayer(); }\n        if(t==='softmax') { L = new global.SoftmaxLayer(); }\n        if(t==='regression') { L = new global.RegressionLayer(); }\n        if(t==='fc') { L = new global.FullyConnLayer(); }\n        if(t==='maxout') { L = new global.MaxoutLayer(); }\n        if(t==='quadtransform') { L = new global.QuadTransformLayer(); }\n        if(t==='svm') { L = new global.SVMLayer(); }\n        L.fromJSON(Lj);\n        this.layers.push(L);\n      }\n    }\n  }\n  \n\n  global.Net = Net;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  var Trainer = function(net, options) {\n\n    this.net = net;\n\n    var options = options || {};\n    this.learning_rate = typeof options.learning_rate !== 'undefined' ? options.learning_rate : 0.01;\n    this.l1_decay = typeof options.l1_decay !== 'undefined' ? options.l1_decay : 0.0;\n    this.l2_decay = typeof options.l2_decay !== 'undefined' ? options.l2_decay : 0.0;\n    this.batch_size = typeof options.batch_size !== 'undefined' ? options.batch_size : 1;\n    this.method = typeof options.method !== 'undefined' ? options.method : 'sgd'; // sgd/adagrad/adadelta/windowgrad\n\n    this.momentum = typeof options.momentum !== 'undefined' ? options.momentum : 0.9;\n    this.ro = typeof options.ro !== 'undefined' ? options.ro : 0.95; // used in adadelta\n    this.eps = typeof options.eps !== 'undefined' ? options.eps : 1e-6; // used in adadelta\n\n    this.k = 0; // iteration counter\n    this.gsum = []; // last iteration gradients (used for momentum calculations)\n    this.xsum = []; // used in adadelta\n  }\n\n  Trainer.prototype = {\n    train: function(x, y) {\n\n      var start = new Date().getTime();\n      this.net.forward(x, true); // also set the flag that lets the net know we're just training\n      var end = new Date().getTime();\n      var fwd_time = end - start;\n\n      var start = new Date().getTime();\n      var cost_loss = this.net.backward(y);\n      var l2_decay_loss = 0.0;\n      var l1_decay_loss = 0.0;\n      var end = new Date().getTime();\n      var bwd_time = end - start;\n      \n      this.k++;\n      if(this.k % this.batch_size === 0) {\n\n        var pglist = this.net.getParamsAndGrads();\n\n        // initialize lists for accumulators. Will only be done once on first iteration\n        if(this.gsum.length === 0 && (this.method !== 'sgd' || this.momentum > 0.0)) {\n          // only vanilla sgd doesnt need either lists\n          // momentum needs gsum\n          // adagrad needs gsum\n          // adadelta needs gsum and xsum\n          for(var i=0;i<pglist.length;i++) {\n            this.gsum.push(global.zeros(pglist[i].params.length));\n            if(this.method === 'adadelta') {\n              this.xsum.push(global.zeros(pglist[i].params.length));\n            } else {\n              this.xsum.push([]); // conserve memory\n            }\n          }\n        }\n\n        // perform an update for all sets of weights\n        for(var i=0;i<pglist.length;i++) {\n          var pg = pglist[i]; // param, gradient, other options in future (custom learning rate etc)\n          var p = pg.params;\n          var g = pg.grads;\n\n          // learning rate for some parameters.\n          var l2_decay_mul = typeof pg.l2_decay_mul !== 'undefined' ? pg.l2_decay_mul : 1.0;\n          var l1_decay_mul = typeof pg.l1_decay_mul !== 'undefined' ? pg.l1_decay_mul : 1.0;\n          var l2_decay = this.l2_decay * l2_decay_mul;\n          var l1_decay = this.l1_decay * l1_decay_mul;\n\n          var plen = p.length;\n          for(var j=0;j<plen;j++) {\n            l2_decay_loss += l2_decay*p[j]*p[j]/2; // accumulate weight decay loss\n            l1_decay_loss += l1_decay*Math.abs(p[j]);\n            var l1grad = l1_decay * (p[j] > 0 ? 1 : -1);\n            var l2grad = l2_decay * (p[j]);\n\n            var gij = (l2grad + l1grad + g[j]) / this.batch_size; // raw batch gradient\n\n            var gsumi = this.gsum[i];\n            var xsumi = this.xsum[i];\n            if(this.method === 'adagrad') {\n              // adagrad update\n              gsumi[j] = gsumi[j] + gij * gij;\n              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij;\n              p[j] += dx;\n            } else if(this.method === 'windowgrad') {\n              // this is adagrad but with a moving window weighted average\n              // so the gradient is not accumulated over the entire history of the run. \n              // it's also referred to as Idea #1 in Zeiler paper on Adadelta. Seems reasonable to me!\n              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;\n              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij; // eps added for better conditioning\n              p[j] += dx;\n            } else if(this.method === 'adadelta') {\n              // assume adadelta if not sgd or adagrad\n              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;\n              var dx = - Math.sqrt((xsumi[j] + this.eps)/(gsumi[j] + this.eps)) * gij;\n              xsumi[j] = this.ro * xsumi[j] + (1-this.ro) * dx * dx; // yes, xsum lags behind gsum by 1.\n              p[j] += dx;\n            } else {\n              // assume SGD\n              if(this.momentum > 0.0) {\n                // momentum update\n                var dx = this.momentum * gsumi[j] - this.learning_rate * gij; // step\n                gsumi[j] = dx; // back this up for next iteration of momentum\n                p[j] += dx; // apply corrected gradient\n              } else {\n                // vanilla sgd\n                p[j] +=  - this.learning_rate * gij;\n              }\n            }\n            g[j] = 0.0; // zero out gradient so that we can begin accumulating anew\n          }\n        }\n      }\n\n      // appending softmax_loss for backwards compatibility, but from now on we will always use cost_loss\n      // in future, TODO: have to completely redo the way loss is done around the network as currently \n      // loss is a bit of a hack. Ideally, user should specify arbitrary number of loss functions on any layer\n      // and it should all be computed correctly and automatically. \n      return {fwd_time: fwd_time, bwd_time: bwd_time, \n              l2_decay_loss: l2_decay_loss, l1_decay_loss: l1_decay_loss,\n              cost_loss: cost_loss, softmax_loss: cost_loss, \n              loss: cost_loss + l1_decay_loss + l2_decay_loss}\n    }\n  }\n  \n  global.Trainer = Trainer;\n  global.SGDTrainer = Trainer; // backwards compatibility\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n\n  // used utilities, make explicit local references\n  var randf = global.randf;\n  var randi = global.randi;\n  var Net = global.Net;\n  var Trainer = global.Trainer;\n  var maxmin = global.maxmin;\n  var randperm = global.randperm;\n  var weightedSample = global.weightedSample;\n  var getopt = global.getopt;\n  var arrUnique = global.arrUnique;\n\n  /*\n  A MagicNet takes data: a list of convnetjs.Vol(), and labels\n  which for now are assumed to be class indeces 0..K. MagicNet then:\n  - creates data folds for cross-validation\n  - samples candidate networks\n  - evaluates candidate networks on all data folds\n  - produces predictions by model-averaging the best networks\n  */\n  var MagicNet = function(data, labels, opt) {\n    var opt = opt || {};\n    if(typeof data === 'undefined') { data = []; }\n    if(typeof labels === 'undefined') { labels = []; }\n\n    // required inputs\n    this.data = data; // store these pointers to data\n    this.labels = labels;\n\n    // optional inputs\n    this.train_ratio = getopt(opt, 'train_ratio', 0.7);\n    this.num_folds = getopt(opt, 'num_folds', 10);\n    this.num_candidates = getopt(opt, 'num_candidates', 50); // we evaluate several in parallel\n    // how many epochs of data to train every network? for every fold?\n    // higher values mean higher accuracy in final results, but more expensive\n    this.num_epochs = getopt(opt, 'num_epochs', 50); \n    // number of best models to average during prediction. Usually higher = better\n    this.ensemble_size = getopt(opt, 'ensemble_size', 10);\n\n    // candidate parameters\n    this.batch_size_min = getopt(opt, 'batch_size_min', 10);\n    this.batch_size_max = getopt(opt, 'batch_size_max', 300);\n    this.l2_decay_min = getopt(opt, 'l2_decay_min', -4);\n    this.l2_decay_max = getopt(opt, 'l2_decay_max', 2);\n    this.learning_rate_min = getopt(opt, 'learning_rate_min', -4);\n    this.learning_rate_max = getopt(opt, 'learning_rate_max', 0);\n    this.momentum_min = getopt(opt, 'momentum_min', 0.9);\n    this.momentum_max = getopt(opt, 'momentum_max', 0.9);\n    this.neurons_min = getopt(opt, 'neurons_min', 5);\n    this.neurons_max = getopt(opt, 'neurons_max', 30);\n\n    // computed\n    this.folds = []; // data fold indices, gets filled by sampleFolds()\n    this.candidates = []; // candidate networks that are being currently evaluated\n    this.evaluated_candidates = []; // history of all candidates that were fully evaluated on all folds\n    this.unique_labels = arrUnique(labels);\n    this.iter = 0; // iteration counter, goes from 0 -> num_epochs * num_training_data\n    this.foldix = 0; // index of active fold\n\n    // callbacks\n    this.finish_fold_callback = null;\n    this.finish_batch_callback = null;\n\n    // initializations\n    if(this.data.length > 0) {\n      this.sampleFolds();\n      this.sampleCandidates();\n    }\n  };\n\n  MagicNet.prototype = {\n\n    // sets this.folds to a sampling of this.num_folds folds\n    sampleFolds: function() {\n      var N = this.data.length;\n      var num_train = Math.floor(this.train_ratio * N);\n      this.folds = []; // flush folds, if any\n      for(var i=0;i<this.num_folds;i++) {\n        var p = randperm(N);\n        this.folds.push({train_ix: p.slice(0, num_train), test_ix: p.slice(num_train, N)});\n      }\n    },\n\n    // returns a random candidate network\n    sampleCandidate: function() {\n      var input_depth = this.data[0].w.length;\n      var num_classes = this.unique_labels.length;\n\n      // sample network topology and hyperparameters\n      var layer_defs = [];\n      layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth: input_depth});\n      var nl = weightedSample([0,1,2,3], [0.2, 0.3, 0.3, 0.2]); // prefer nets with 1,2 hidden layers\n      for(var q=0;q<nl;q++) {\n        var ni = randi(this.neurons_min, this.neurons_max);\n        var act = ['tanh','maxout','relu'][randi(0,3)];\n        if(randf(0,1)<0.5) {\n          var dp = Math.random();\n          layer_defs.push({type:'fc', num_neurons: ni, activation: act, drop_prob: dp});\n        } else {\n          layer_defs.push({type:'fc', num_neurons: ni, activation: act});\n        }\n      }\n      layer_defs.push({type:'softmax', num_classes: num_classes});\n      var net = new Net();\n      net.makeLayers(layer_defs);\n\n      // sample training hyperparameters\n      var bs = randi(this.batch_size_min, this.batch_size_max); // batch size\n      var l2 = Math.pow(10, randf(this.l2_decay_min, this.l2_decay_max)); // l2 weight decay\n      var lr = Math.pow(10, randf(this.learning_rate_min, this.learning_rate_max)); // learning rate\n      var mom = randf(this.momentum_min, this.momentum_max); // momentum. Lets just use 0.9, works okay usually ;p\n      var tp = randf(0,1); // trainer type\n      var trainer_def;\n      if(tp<0.33) {\n        trainer_def = {method:'adadelta', batch_size:bs, l2_decay:l2};\n      } else if(tp<0.66) {\n        trainer_def = {method:'adagrad', learning_rate: lr, batch_size:bs, l2_decay:l2};\n      } else {\n        trainer_def = {method:'sgd', learning_rate: lr, momentum: mom, batch_size:bs, l2_decay:l2};\n      }\n      \n      var trainer = new Trainer(net, trainer_def);\n\n      var cand = {};\n      cand.acc = [];\n      cand.accv = 0; // this will maintained as sum(acc) for convenience\n      cand.layer_defs = layer_defs;\n      cand.trainer_def = trainer_def;\n      cand.net = net;\n      cand.trainer = trainer;\n      return cand;\n    },\n\n    // sets this.candidates with this.num_candidates candidate nets\n    sampleCandidates: function() {\n      this.candidates = []; // flush, if any\n      for(var i=0;i<this.num_candidates;i++) {\n        var cand = this.sampleCandidate();\n        this.candidates.push(cand);\n      }\n    },\n\n    step: function() {\n      \n      // run an example through current candidate\n      this.iter++;\n\n      // step all candidates on a random data point\n      var fold = this.folds[this.foldix]; // active fold\n      var dataix = fold.train_ix[randi(0, fold.train_ix.length)];\n      for(var k=0;k<this.candidates.length;k++) {\n        var x = this.data[dataix];\n        var l = this.labels[dataix];\n        this.candidates[k].trainer.train(x, l);\n      }\n\n      // process consequences: sample new folds, or candidates\n      var lastiter = this.num_epochs * fold.train_ix.length;\n      if(this.iter >= lastiter) {\n        // finished evaluation of this fold. Get final validation\n        // accuracies, record them, and go on to next fold.\n        var val_acc = this.evalValErrors();\n        for(var k=0;k<this.candidates.length;k++) {\n          var c = this.candidates[k];\n          c.acc.push(val_acc[k]);\n          c.accv += val_acc[k];\n        }\n        this.iter = 0; // reset step number\n        this.foldix++; // increment fold\n\n        if(this.finish_fold_callback !== null) {\n          this.finish_fold_callback();\n        }\n\n        if(this.foldix >= this.folds.length) {\n          // we finished all folds as well! Record these candidates\n          // and sample new ones to evaluate.\n          for(var k=0;k<this.candidates.length;k++) {\n            this.evaluated_candidates.push(this.candidates[k]);\n          }\n          // sort evaluated candidates according to accuracy achieved\n          this.evaluated_candidates.sort(function(a, b) { \n            return (a.accv / a.acc.length) \n                 > (b.accv / b.acc.length) \n                 ? -1 : 1;\n          });\n          // and clip only to the top few ones (lets place limit at 3*ensemble_size)\n          // otherwise there are concerns with keeping these all in memory \n          // if MagicNet is being evaluated for a very long time\n          if(this.evaluated_candidates.length > 3 * this.ensemble_size) {\n            this.evaluated_candidates = this.evaluated_candidates.slice(0, 3 * this.ensemble_size);\n          }\n          if(this.finish_batch_callback !== null) {\n            this.finish_batch_callback();\n          }\n          this.sampleCandidates(); // begin with new candidates\n          this.foldix = 0; // reset this\n        } else {\n          // we will go on to another fold. reset all candidates nets\n          for(var k=0;k<this.candidates.length;k++) {\n            var c = this.candidates[k];\n            var net = new Net();\n            net.makeLayers(c.layer_defs);\n            var trainer = new Trainer(net, c.trainer_def);\n            c.net = net;\n            c.trainer = trainer;\n          }\n        }\n      }\n    },\n\n    evalValErrors: function() {\n      // evaluate candidates on validation data and return performance of current networks\n      // as simple list\n      var vals = [];\n      var fold = this.folds[this.foldix]; // active fold\n      for(var k=0;k<this.candidates.length;k++) {\n        var net = this.candidates[k].net;\n        var v = 0.0;\n        for(var q=0;q<fold.test_ix.length;q++) {\n          var x = this.data[fold.test_ix[q]];\n          var l = this.labels[fold.test_ix[q]];\n          net.forward(x);\n          var yhat = net.getPrediction();\n          v += (yhat === l ? 1.0 : 0.0); // 0 1 loss\n        }\n        v /= fold.test_ix.length; // normalize\n        vals.push(v);\n      }\n      return vals;\n    },\n\n    // returns prediction scores for given test data point, as Vol\n    // uses an averaged prediction from the best ensemble_size models\n    // x is a Vol.\n    predict_soft: function(data) {\n      // forward prop the best networks\n      // and accumulate probabilities at last layer into a an output Vol\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n      if(nv === 0) { return new convnetjs.Vol(0,0,0); } // not sure what to do here? we're not ready yet\n      var xout, n;\n      for(var j=0;j<nv;j++) {\n        var net = this.evaluated_candidates[j].net;\n        var x = net.forward(data);\n        if(j===0) { \n          xout = x; \n          n = x.w.length; \n        } else {\n          // add it on\n          for(var d=0;d<n;d++) {\n            xout.w[d] += x.w[d];\n          }\n        }\n      }\n      // produce average\n      for(var d=0;d<n;d++) {\n        xout.w[d] /= n;\n      }\n      return xout;\n    },\n\n    predict: function(data) {\n      var xout = this.predict_soft(data);\n      if(xout.w.length !== 0) {\n        var stats = maxmin(xout.w);\n        var predicted_label = stats.maxi; \n      } else {\n        var predicted_label = -1; // error out\n      }\n      return predicted_label;\n\n    },\n\n    toJSON: function() {\n      // dump the top ensemble_size networks as a list\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n      var json = {};\n      json.nets = [];\n      for(var i=0;i<nv;i++) {\n        json.nets.push(this.evaluated_candidates[i].net.toJSON());\n      }\n      return json;\n    },\n\n    fromJSON: function(json) {\n      this.ensemble_size = json.nets.length;\n      this.evaluated_candidates = [];\n      for(var i=0;i<this.ensemble_size;i++) {\n        var net = new Net();\n        net.fromJSON(json.nets[i]);\n        var dummy_candidate = {};\n        dummy_candidate.net = net;\n        this.evaluated_candidates.push(dummy_candidate);\n      }\n    },\n\n    // callback functions\n    // called when a fold is finished, while evaluating a batch\n    onFinishFold: function(f) { this.finish_fold_callback = f; },\n    // called when a batch of candidates has finished evaluating\n    onFinishBatch: function(f) { this.finish_batch_callback = f; }\n    \n  };\n\n  global.MagicNet = MagicNet;\n})(convnetjs);\nthis.convnetjs = convnetjs;\n"]},"sourceType":"module","externalDependencies":{},"hash":"f92d4c602365b48fd2270745733b84a1f98601ce"}
